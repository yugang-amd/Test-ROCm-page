
<!DOCTYPE html>

<html data-content_root="./" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="Start building for HPC and AI with the performance-first AMD ROCm software stack. Explore how-to guides and reference docs." name="description"/>
<meta content="Radeon, open, compute, platform, install, how, conceptual, reference, home, docs" name="keywords"/>
<meta content="What is ROCm" name="description"/>
<meta content="ROCm components, ROCm projects, introduction, ROCm, AMD, runtimes, compilers, tools, libraries, API" name="keywords"/>
<meta content="ROCm compatibility matrix" name="description"/>
<meta content="GPU, architecture, hardware, compatibility, system, requirements, components, libraries" name="keywords"/>
<meta content="How to install deep learning frameworks for ROCm" name="description"/>
<meta content="deep learning, frameworks, ROCm, install, PyTorch, TensorFlow, JAX, MAGMA, DeepSpeed, ML, AI" name="keywords"/>
<meta content="Build ROCm from source" name="description"/>
<meta content="build ROCm, source, ROCm source, ROCm, repo, make, makefile" name="keywords"/>
<meta content="Learn how to use ROCm for AI." name="description"/>
<meta content="ROCm, AI, machine learning, LLM, usage, tutorial" name="keywords"/>
<meta content="How to install ROCm and popular machine learning frameworks." name="description"/>
<meta content="ROCm, AI, LLM, train, fine-tune, FSDP, DeepSpeed, LLaMA, tutorial" name="keywords"/>
<meta content="System health checks with RVS, RCCL tests, BabelStream, and TransferBench to validate AMD hardware performance running AI workloads." name="description"/>
<meta content="gpu, accelerator, system, health, validation, bench, perf, performance, rvs, rccl, babel, mi300x, mi325x, flops, bandwidth, rbt, training, inference" name="keywords"/>
<meta content="How to use ROCm for training models" name="description"/>
<meta content="ROCm, LLM, training, GPUs, training model, scaling model, usage, tutorial" name="keywords"/>
<meta content="How to train a model using Megatron-LM for ROCm." name="description"/>
<meta content="ROCm, AI, LLM, train, Megatron-LM, megatron, Llama, tutorial, docker, torch" name="keywords"/>
<meta content="How to train a model using PyTorch for ROCm." name="description"/>
<meta content="ROCm, AI, LLM, train, PyTorch, torch, Llama, flux, tutorial, docker" name="keywords"/>
<meta content="How to train a model using JAX MaxText for ROCm." name="description"/>
<meta content="ROCm, AI, LLM, train, jax, torch, Llama, flux, tutorial, docker" name="keywords"/>
<meta content="How to train a model using LLM Foundry for ROCm." name="description"/>
<meta content="ROCm, AI, LLM, train, PyTorch, torch, Llama, flux, tutorial, docker" name="keywords"/>
<meta content="How to scale and accelerate model training" name="description"/>
<meta content="ROCm, AI, LLM, train, fine-tune, deploy, FSDP, DeepSpeed, LLaMA, tutorial" name="keywords"/>
<meta content="How to fine-tune LLMs with ROCm" name="description"/>
<meta content="ROCm, LLM, fine-tuning, usage, tutorial, GPUs, Llama, accelerators" name="keywords"/>
<meta content="Conceptual overview of fine-tuning LLMs" name="description"/>
<meta content="ROCm, LLM, Llama, fine-tuning, usage, tutorial, optimzation, LoRA, walkthrough, PEFT, Reinforcement" name="keywords"/>
<meta content="How to fine-tune models with ROCm" name="description"/>
<meta content="ROCm, LLM, fine-tuning, inference, usage, tutorial, deep learning, PyTorch, TensorFlow, JAX" name="keywords"/>
<meta content="Model fine-tuning and inference on a single-GPU system" name="description"/>
<meta content="ROCm, LLM, fine-tuning, usage, tutorial, single-GPU, LoRA, PEFT, inference, SFTTrainer" name="keywords"/>
<meta content="Model fine-tuning and inference on a multi-GPU system" name="description"/>
<meta content="ROCm, LLM, fine-tuning, usage, tutorial, multi-GPU, distributed, inference, accelerators, PyTorch, HuggingFace, torchtune" name="keywords"/>
<meta content="How to use ROCm for AI inference workloads." name="description"/>
<meta content="ROCm, AI, machine learning, LLM, AI inference, NLP, GPUs, usage, tutorial" name="keywords"/>
<meta content="How to run models from Hugging Face on AMD GPUs." name="description"/>
<meta content="ROCm, AI, LLM, Hugging Face, Optimum, Flash Attention, GPTQ, ONNX, tutorial" name="keywords"/>
<meta content="How to implement the LLM inference frameworks with ROCm acceleration." name="description"/>
<meta content="ROCm, LLM, fine-tuning, usage, tutorial, inference, vLLM, TGI, text generation inference" name="keywords"/>
<meta content="Learn how to validate LLM inference performance on MI300X accelerators using AMD MAD and the ROCm vLLM Docker image." name="description"/>
<meta content="model, MAD, automation, dashboarding, validate" name="keywords"/>
<meta content="Learn how to validate LLM inference performance on MI300X accelerators using AMD MAD and the ROCm PyTorch Docker image." name="description"/>
<meta content="model, MAD, automation, dashboarding, validate, pytorch" name="keywords"/>
<meta content="Learn how to validate LLM inference performance on MI300X accelerators using AMD MAD and SGLang" name="description"/>
<meta content="model, MAD, automation, dashboarding, validate" name="keywords"/>
<meta content="How to deploy your model for AI inference using vLLM and Hugging Face TGI." name="description"/>
<meta content="ROCm, AI, LLM, train, fine-tune, deploy, FSDP, DeepSpeed, LLaMA, tutorial" name="keywords"/>
<meta content="How to Use ROCm for AI inference optimization" name="description"/>
<meta content="ROCm, LLM, AI inference, Optimization, GPUs, usage, tutorial" name="keywords"/>
<meta content="How to use model quantization techniques to speed up inference." name="description"/>
<meta content="ROCm, LLM, fine-tuning, usage, tutorial, quantization, Quark, GPTQ, transformers, bitsandbytes" name="keywords"/>
<meta content="How to use model acceleration techniques and libraries to improve memory efficiency and performance." name="description"/>
<meta content="ROCm, LLM, fine-tuning, usage, tutorial, Flash Attention, Hugging Face, xFormers, vLLM, PyTorch" name="keywords"/>
<meta content="How to optimize machine learning workloads with Composable Kernel (CK)." name="description"/>
<meta content="mixed, precision, kernel, inference, linear, algebra, ck, GEMM" name="keywords"/>
<meta content="How to optimize Triton kernels for ROCm." name="description"/>
<meta content="ROCm, LLM, fine-tuning, usage, MI300X, tutorial, Triton, kernel, performance, optimization" name="keywords"/>
<meta content="How to use ROCm profiling and debugging tools." name="description"/>
<meta content="ROCm, LLM, fine-tuning, usage, MI300X, tutorial, profiling, debugging, performance, Triton" name="keywords"/>
<meta content="Learn about workload tuning on AMD Instinct MI300X accelerators for optimal performance." name="description"/>
<meta content="AMD, Instinct, MI300X, HPC, tuning, BIOS settings, NBIO, ROCm, environment variable, performance, HIP, Triton, PyTorch TunableOp, vLLM, RCCL, MIOpen, accelerator, GPU, resource utilization" name="keywords"/>
<meta content="How to use ROCm for high-performance computing (HPC)." name="description"/>
<meta content="ROCm, AI, high performance computing, HPC, science, scientific" name="keywords"/>
<meta content="Learn about AMD hardware optimization for HPC-specific and workstation workloads." name="description"/>
<meta content="high-performance computing, HPC, Instinct accelerators, Radeon, tuning, tuning guide, AMD, ROCm" name="keywords"/>
<meta content="How to configure MI300X accelerators to fully leverage their capabilities and achieve optimal performance." name="description"/>
<meta content="ROCm, AI, machine learning, MI300X, LLM, usage, tutorial, optimization, tuning" name="keywords"/>
<meta content="Learn more about common system-level debugging measures for ROCm." name="description"/>
<meta content="env, var, sys, PCIe, troubleshooting, admin, error" name="keywords"/>
<meta content="Setting the number of CUs" name="description"/>
<meta content="CU, CUs, number of CUs, compute units" name="keywords"/>
<meta content="Learn about BAR configuration in AMD GPUs and ways to troubleshoot physical addressing limit" name="description"/>
<meta content="BAR memory, MMIO, GPU memory, Physical Addressing Limit, AMD, ROCm" name="keywords"/>
<meta content="Learn about the AMD Instinct MI300 series architecture." lang="en" name="description" xml:lang="en"/>
<meta content="Instinct, MI300X, MI300A, microarchitecture, AMD, ROCm" name="keywords"/>
<meta content="MI300 and MI200 series performance counters and metrics" name="description"/>
<meta content="MI300, MI200, performance counters, command processor counters" name="keywords"/>
<meta content="Learn about the AMD Instinct MI250 series architecture." lang="en" name="description" xml:lang="en"/>
<meta content="Instinct, MI250, microarchitecture, AMD, ROCm" name="keywords"/>
<meta content="Learn about the AMD Instinct MI100 series architecture." lang="en" name="description" xml:lang="en"/>
<meta content="Instinct, MI100, microarchitecture, AMD, ROCm" name="keywords"/>
<meta content="Using CMake" name="description"/>
<meta content="CMake, dependencies, HIP, C++, AMD, ROCm" name="keywords"/>
<meta content="AMD Instinct™ accelerator, AMD Radeon PRO™, and AMD Radeon™ GPU architecture information" name="description"/>
<meta content="Instinct, Radeon, accelerator, GCN, CDNA, RDNA, GPU, architecture, VRAM, Compute Units, Cache, Registers, LDS, Register File" name="keywords"/>
<meta content="AMD Instinct accelerator, AMD Radeon PRO, and AMD Radeon GPU atomics operations information" name="description"/>
<meta content="Atomics operations, atomic bitwise functions, atomics add, atomics subtraction, atomics exchange, atomics min, atomics max" name="keywords"/>
<meta content="Supported data types of AMD GPUs and libraries in ROCm." name="description"/>
<meta content="precision, data types, HIP types, int8, float8, float8 (E4M3), float8 (E5M2), bfloat8, float16, half, bfloat16, tensorfloat32, float, float32, float64, double, AMD data types, HIP data types, ROCm precision, ROCm data types" name="keywords"/>
<meta content="This page lists supported graph safe ROCm libraries." name="description"/>
<meta content="AMD, ROCm, HIP, hipGRAPH" name="keywords"/>
<title>ROCm Documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!--
    this give us a css class that will be invisible only if js is disabled
  -->
<noscript>
<style>
      .pst-js-only { display: none !important; }

    </style>
</noscript>
<!-- Loaded before other Sphinx assets -->
<link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet"/>
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet"/>
<link href="_static/pygments.css?v=8f2a1f02" rel="stylesheet" type="text/css"/>
<link href="_static/styles/sphinx-book-theme.css?v=a3416100" rel="stylesheet" type="text/css"/>
<link href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" rel="stylesheet" type="text/css"/>
<link href="_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="_static/custom.css?v=643846e8" rel="stylesheet" type="text/css"/>
<link href="_static/rocm_header.css?v=9557e3d1" rel="stylesheet" type="text/css"/>
<link href="_static/rocm_footer.css?v=7095035a" rel="stylesheet" type="text/css"/>
<link href="_static/fonts.css?v=fcff5274" rel="stylesheet" type="text/css"/>
<link href="_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="_static/rocm_custom.css?v=ace7df76" rel="stylesheet" type="text/css"/>
<link href="_static/rocm_rn.css?v=0e8af9ba" rel="stylesheet" type="text/css"/>
<link href="_static/vllm-benchmark.css?v=aeeec7cd" rel="stylesheet" type="text/css"/>
<!-- So that users can add custom icons -->
<script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" rel="preload"/>
<link as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" rel="preload"/>
<script src="_static/documentation_options.js?v=27202511"></script>
<script src="_static/doctools.js?v=9bcbadda"></script>
<script src="_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="_static/clipboard.min.js?v=a7894cd8"></script>
<script src="_static/copybutton.js?v=f281be69"></script>
<script async="async" src="_static/code_word_breaks.js?v=327952c4"></script>
<script async="async" src="_static/renameVersionLinks.js?v=929fe5e4"></script>
<script async="async" src="_static/rdcMisc.js?v=01f88d96"></script>
<script async="async" src="_static/theme_mode_captions.js?v=15f4ec5d"></script>
<script defer="defer" src="_static/search.js?v=90a4452c"></script>
<script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
<script src="_static/design-tabs.js?v=f930bc37"></script>
<script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
<script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'index';</script>
<script src="_static/vllm-benchmark.js?v=6b88e2d9"></script>
<script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
<link href="https://rocm.docs.amd.com/en/latest/#document-index" rel="canonical"/>
<link href="https://www.amd.com/content/dam/code/images/favicon/favicon.ico" rel="icon"/>
<link href="genindex.html" rel="index" title="Index"/>
<link href="search.html" rel="search" title="Search"/>
<meta content="vo35SZt_GASsTHAEmdww7AYKPCvZyzLvOXBl8guBME4" name="google-site-verification"/>
</head>
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<dialog id="pst-search-dialog">
<form action="search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" name="q" placeholder="Search..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</dialog>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="common-header">
<nav class="navbar navbar-expand-xl">
<div class="container-fluid main-nav rocm-header">
<button aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler collapsed" data-bs-target="#navbarSupportedContent" data-bs-toggle="collapse" data-tracking-information="mainMenuToggle" id="nav-icon" type="button">
<span></span>
<span></span>
<span></span>
</button>
<div class="header-logo">
<a class="navbar-brand" href="https://www.amd.com/">
<img alt="AMD Logo" class="d-inline-block align-text-top hover-opacity" src="_static/images/amd-header-logo.svg" title="AMD Logo" width="90"/>
</a>
<div class="vr vr mx-40 my-25"></div>
<a class="klavika-font hover-opacity" href="https://rocm.docs.amd.com/en/latest">ROCm™ Software 6.4.3</a>
<a class="header-all-versions" href="https://rocm.docs.amd.com/en/latest/release/versions.html">Version List</a>
</div>
<div class="icon-nav text-center d-flex ms-auto">
</div>
</div>
</nav>
<nav class="navbar navbar-expand-xl second-level-nav">
<div class="container-fluid main-nav">
<div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
<ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://github.com/ROCm/ROCm" id="navgithub" role="button" target="_blank">
                                GitHub
                            </a>
</li>
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://github.com/ROCm/ROCm/discussions" id="navcommunity" role="button" target="_blank">
                                Community
                            </a>
</li>
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://rocm.blogs.amd.com/" id="navblogs" role="button" target="_blank">
                                Blogs
                            </a>
</li>
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://www.amd.com/en/developer/resources/rocm-hub.html" id="navrocm-developer-hub" role="button" target="_blank">
                                ROCm Developer Hub
                            </a>
</li>
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://instinct.docs.amd.com" id="navinstinct™-docs" role="button" target="_blank">
                                Instinct™ Docs
                            </a>
</li>
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://www.amd.com/en/developer/resources/infinity-hub.html" id="navinfinity-hub" role="button" target="_blank">
                                Infinity Hub
                            </a>
</li>
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://github.com/ROCm/ROCm/issues/new/choose" id="navsupport" role="button" target="_blank">
                                Support
                            </a>
</li>
</ul>
</div>
</div>
</nav>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<dialog id="pst-primary-sidebar-modal"></dialog>
<div class="bd-sidebar-primary bd-sidebar" id="pst-primary-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<a class="navbar-brand logo" href="#">
<p class="title logo__title">None</p>
</a></div>
<div class="sidebar-primary-item">
<button aria-label="Search" class="btn search-button-field search-button__button pst-js-only" data-bs-placement="bottom" data-bs-toggle="tooltip" title="Search">
<i class="fa-solid fa-magnifying-glass"></i>
<span class="search-button__default-text">Search</span>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
<div class="sidebar-primary-item"><nav aria-label="Main" class="bd-links bd-docs-nav">
<div class="bd-toc-item navbar-nav active">
<ul class="nav bd-sidenav">
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Install</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/">ROCm on Linux</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-windows/en/latest/">HIP SDK on Windows</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/radeon/en/latest/index.html">ROCm on Radeon GPUs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">How to</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://github.com/amd/rocm-examples">ROCm examples</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Conceptual</span></p>
<ul class="nav bd-sidenav">
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Contribute</span></p>
<ul class="nav bd-sidenav">
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
<div class="sidebar-primary-item">
<div class="flat" data-ea-manual="true" data-ea-publisher="readthedocs" data-ea-type="readthedocs-sidebar" id="ethical-ad-placement">
</div></div>
</div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="sbt-scroll-pixel-helper"></div>
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" data-bs-placement="bottom" data-bs-toggle="tooltip" for="__primary" title="Toggle primary sidebar">
<span class="fa-solid fa-angle-right"></span>
</label></div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="article-header-buttons">
<button aria-label="Color mode" class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" data-bs-placement="bottom" data-bs-title="Color mode" data-bs-toggle="tooltip">
<i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light" title="Light"></i>
<i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark" title="Dark"></i>
<i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto" title="System Settings"></i>
</button>
<button aria-label="Search" class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" data-bs-placement="bottom" data-bs-toggle="tooltip" title="Search">
<i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" data-bs-placement="bottom" data-bs-toggle="tooltip" title="Toggle secondary sidebar">
<span class="fa-solid fa-list"></span>
</button>
</div></div>
</div>
</div>
</div>
<div class="onlyprint" id="jb-print-docs-body">
<h1>AMD ROCm documentation</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-what-is-rocm">What is ROCm?</a><ul class="visible nav section-nav flex-column">
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-components">ROCm components</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#libraries">Libraries</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-computer-vision">Machine Learning &amp; Computer Vision</a></li>
<li class="toctree-l4 toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#communication">Communication</a></li>
<li class="toctree-l4 toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#math">Math</a></li>
<li class="toctree-l4 toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#primitives">Primitives</a></li>
</ul>
</li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tools">Tools</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#system-management">System Management</a></li>
<li class="toctree-l4 toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#performance">Performance</a></li>
<li class="toctree-l4 toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#development">Development</a></li>
</ul>
</li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compilers">Compilers</a></li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#runtimes">Runtimes</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-about/release-notes">Release notes</a><ul class="visible nav section-nav flex-column">
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-highlights">Release highlights</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#amdgpu-driver-updates">AMDGPU driver updates</a></li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-smi-update">ROCm SMI update</a></li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-documentation-updates">ROCm documentation updates</a></li>
</ul>
</li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#operating-system-and-hardware-support-changes">Operating system and hardware support changes</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-components">ROCm components</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#detailed-component-changes">Detailed component changes</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-smi-7-7-0"><strong>ROCm SMI</strong> (7.7.0)</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#added">Added</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-known-issues">ROCm known issues</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-upcoming-changes">ROCm upcoming changes</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#amd-smi-migration-to-amdgpu-driver-repository">AMD SMI migration to AMDGPU driver repository</a></li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-smi-deprecation">ROCm SMI deprecation</a></li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#roctracer-rocprofiler-rocprof-and-rocprofv2-deprecation">ROCTracer, ROCProfiler, rocprof, and rocprofv2 deprecation</a></li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#amdgpu-wavefront-size-compiler-macro-deprecation">AMDGPU wavefront size compiler macro deprecation</a></li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hipcc-perl-scripts-deprecation">HIPCC Perl scripts deprecation</a></li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#changes-to-rocm-object-tooling">Changes to ROCm Object Tooling</a></li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hip-runtime-api-changes">HIP runtime API changes</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-compatibility/compatibility-matrix">Compatibility matrix</a><ul class="visible nav section-nav flex-column">
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#operating-systems-kernel-and-glibc-versions">Operating systems, kernel and Glibc versions</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#past-versions-of-rocm-compatibility-matrix">Past versions of ROCm compatibility matrix</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference external nav-link" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html">Linux system requirements</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference external nav-link" href="https://rocm.docs.amd.com/projects/install-on-windows/en/latest/reference/system-requirements.html">Windows system requirements</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Install</span></p>
<ul class="nav section-nav flex-column">
<li class="toctree-l1 nav-item toc-entry"><a class="reference external nav-link" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/">ROCm on Linux</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference external nav-link" href="https://rocm.docs.amd.com/projects/install-on-windows/en/latest/">HIP SDK on Windows</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference external nav-link" href="https://rocm.docs.amd.com/projects/radeon/en/latest/index.html">ROCm on Radeon GPUs</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/deep-learning-rocm">Deep learning frameworks</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/build-rocm">Build ROCm from source</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">How to</span></p>
<ul class="nav section-nav flex-column">
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/index">Use ROCm for AI</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/install">Installation</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-frameworks">Machine learning frameworks</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">Next steps</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/system-health-check">System health benchmarks</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-validation-suite-rvs-tests">ROCm Validation Suite (RVS) tests</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#install-rocm-validation-suite">Install ROCm Validation Suite</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmark-stress-and-qualification-tests">Benchmark, stress, and qualification tests</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#babelstream-test">BabelStream test</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#rccl-tests">RCCL tests</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#transferbench-test">TransferBench test</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/training/index">Training</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/training/benchmark-docker/megatron-lm">Train a model with Megatron-LM</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-models">Supported models</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-measurements">Performance measurements</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#system-validation">System validation</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-setup">Environment setup</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#download-the-docker-image">Download the Docker image</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#configuration">Configuration</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#network-interface">Network interface</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenizer">Tokenizer</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-options">Dataset options</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#download-the-dataset">Download the dataset</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-configuration">Multi-node configuration</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#run-training">Run training</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#single-node-training">Single node training</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-training-examples">Multi-node training examples</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#key-options">Key options</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#previous-versions">Previous versions</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/training/benchmark-docker/pytorch-training">Train a model with PyTorch</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-models">Supported models</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-measurements">Performance measurements</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#system-validation">System validation</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking">Benchmarking</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#previous-versions">Previous versions</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/training/benchmark-docker/jax-maxtext">Train a model with JAX MaxText</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-features-and-models">Supported features and models</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#unsupported-features">Unsupported features</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#system-validation">System validation</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-setup">Environment setup</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-setup">Multi-node setup</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#pull-the-docker-image">Pull the Docker image</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-started">Getting started</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#single-node-training-benchmarking-examples">Single node training benchmarking examples</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-training-benchmarking-examples">Multi-node training benchmarking examples</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#previous-versions">Previous versions</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/training/benchmark-docker/mpt-llm-foundry">Train a model with LLM Foundry</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#system-validation">System validation</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-started">Getting started</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-output">Interpreting the output</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/training/scale-model-training">Scale model training</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-distributed">PyTorch distributed</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-fsdp">PyTorch FSDP</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#deepspeed">DeepSpeed</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-mixed-precision-amp">Automatic mixed precision (AMP)</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-your-model">Fine-tuning your model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/fine-tuning/index">Fine-tuning LLMs</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/fine-tuning/overview">Conceptual overview</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-challenge-of-fine-tuning-models">The challenge of fine-tuning models</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizations-for-model-fine-tuning">Optimizations for model fine-tuning</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#walkthrough">Walkthrough</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/fine-tuning/fine-tuning-and-inference">Fine-tuning</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/fine-tuning/single-gpu-fine-tuning-and-inference">Use a single accelerator</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-setup">Environment setup</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-the-base-implementation-environment">Setting up the base implementation environment</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#download-the-base-model-and-fine-tuning-dataset">Download the base model and fine-tuning dataset</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#configure-fine-tuning-parameters">Configure fine-tuning parameters</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning">Fine-tuning</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-adapters-or-fully-fine-tuned-models">Saving adapters or fully fine-tuned models</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-model-inference">Basic model inference</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/fine-tuning/multi-gpu-fine-tuning-and-inference">Use multiple accelerators</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-setup">Environment setup</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-the-base-implementation-environment">Setting up the base implementation environment</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#hugging-face-accelerate-for-fine-tuning-and-inference">Hugging Face Accelerate for fine-tuning and inference</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#torchtune-for-fine-tuning-and-inference">torchtune for fine-tuning and inference</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference/index">Inference</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference/hugging-face-models">Run models from Hugging Face</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-hugging-face-transformers">Using Hugging Face Transformers</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-hugging-face-with-optimum-amd">Using Hugging Face with Optimum-AMD</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#installation">Installation</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#flash-attention">Flash Attention</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#gptq">GPTQ</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#onnx">ONNX</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference/llm-inference-frameworks">LLM inference frameworks</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#vllm-inference">vLLM inference</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-vllm">Installing vLLM</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-llms-tgi">Hugging Face TGI</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#install-tgi">Install TGI</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference/benchmark-docker/vllm">vLLM inference performance testing</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#what-s-new">What’s new</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-models">Supported models</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-measurements">Performance measurements</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#system-validation">System validation</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#pull-the-docker-image">Pull the Docker image</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking">Benchmarking</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-usage">Advanced usage</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#reproducing-the-docker-image">Reproducing the Docker image</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#known-issues-and-workarounds">Known issues and workarounds</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#previous-versions">Previous versions</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference/benchmark-docker/pytorch-inference">PyTorch inference performance testing</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-models">Supported models</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#system-validation">System validation</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#pull-the-docker-image">Pull the Docker image</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking">Benchmarking</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference/benchmark-docker/sglang">SGLang inference performance testing</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#system-validation">System validation</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#pull-the-docker-image">Pull the Docker image</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking">Benchmarking</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#previous-versions">Previous versions</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference/deploy-your-model">Deploy your model</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#serving-using-vllm">Serving using vLLM</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#vllm-installation">vLLM installation</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#vllm-walkthrough">vLLM walkthrough</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#validating-vllm-performance">Validating vLLM performance</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#serving-using-hugging-face-tgi">Serving using Hugging Face TGI</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#tgi-installation">TGI installation</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#tgi-walkthrough">TGI walkthrough</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference-optimization/index">Inference optimization</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference-optimization/model-quantization">Model quantization techniques</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#amd-quark">AMD Quark</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-quark">Installing Quark</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#using-quark-for-quantization">Using Quark for quantization</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-the-quantized-model-with-vllm">Evaluating the quantized model with vLLM</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#gptq">GPTQ</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-autogptq">Installing AutoGPTQ</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#using-gptq-with-autogptq">Using GPTQ with AutoGPTQ</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#using-gptq-with-hugging-face-transformers">Using GPTQ with Hugging Face Transformers</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#exllama-v2-support">ExLlama-v2 support</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#bitsandbytes">bitsandbytes</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-bitsandbytes">Installing bitsandbytes</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#using-bitsandbytes-primitives">Using bitsandbytes primitives</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#using-bitsandbytes-with-hugging-face-transformers">Using bitsandbytes with Hugging Face Transformers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference-optimization/model-acceleration-libraries">Model acceleration libraries</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#flash-attention-2">Flash Attention 2</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-flash-attention-2">Installing Flash Attention 2</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#xformers">xFormers</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-ck-xformers">Installing CK xFormers</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-built-in-acceleration">PyTorch built-in acceleration</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-compilation">PyTorch compilation</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-tunableop">PyTorch TunableOp</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#fbgemm-and-fbgemm-gpu">FBGEMM and FBGEMM_GPU</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-fbgemm-gpu">Installing FBGEMM_GPU</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#set-up-the-miniconda-environment">Set up the Miniconda environment</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#install-the-rocm-components">Install the ROCm components</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#install-pytorch">Install PyTorch</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#perform-the-prebuild-and-build">Perform the prebuild and build</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#post-build-validation">Post-build validation</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-fbgemm">Testing FBGEMM</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference-optimization/optimizing-with-composable-kernel">Optimize with Composable Kernel</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#high-level-overview-a-ck-gemm-instance">High-level overview: a CK GEMM instance</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#template-parameter-definition">Template parameter definition</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-data-precision">Matrix data precision</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-data-layout">Matrix data layout</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-element-operation">Matrix element operation</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#tunable-parameters">Tunable parameters</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#instantiating-and-running-the-templated-kernel">Instantiating and running the templated kernel</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#developing-fused-int8-kernels-for-smoothquant-models">Developing fused INT8 kernels for SmoothQuant models</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#operation-flow-analysis">Operation flow analysis</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#developing-the-complete-function">Developing the complete function</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#binding-to-python">Binding to Python</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#int8-model-inference-and-performance">INT8 model inference and performance</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference-optimization/optimizing-triton-kernel">Optimize Triton kernels</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference-optimization/profiling-and-debugging">Profile and debug</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference-optimization/workload">Workload optimization</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#workload-tuning-strategy">Workload tuning strategy</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#measure-the-current-workload">Measure the current workload</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#mi300x-profiling-start">Identify tuning requirements</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#high-level-profiling-tools">High-level profiling tools</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-level-profiling-tools">Kernel-level profiling tools</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#analyze-and-tune">Analyze and tune</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#optimize-model-inference-with-vllm">Optimize model inference with vLLM</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-tunable-configurations">Auto-tunable configurations</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-tuning">Manual tuning</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#iterate-and-validate">Iterate and validate</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#profiling-tools">Profiling tools</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-profiler">PyTorch Profiler</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-profiling-tools">ROCm profiling tools</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#rocprofiler">ROCProfiler</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-compute-profiler">ROCm Compute Profiler</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-systems-profiler">ROCm Systems Profiler</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#vllm-performance-optimization">vLLM performance optimization</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-environment-variables">Performance environment variables</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-tuning-using-pytorch-tunableop">Auto-tuning using PyTorch TunableOp</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-tuning-based-on-vllm-engine-configurations">Performance tuning based on vLLM engine configurations</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-performance-by-throughput-measurement">Evaluating performance by throughput measurement</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#maximizing-vllm-instances-on-a-single-node">Maximizing vLLM instances on a single node</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#configure-the-gpu-memory-utilization-parameter">Configure the gpu_memory_utilization parameter</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#run-vllm-on-multiple-gpus">Run vLLM on multiple GPUs</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#choose-an-attention-backend">Choose an attention backend</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#vllm-engine-arguments">vLLM engine arguments</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#configure-the-max-num-seqs-parameter">Configure the max-num-seqs parameter</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#use-the-float16-dtype">Use the float16 dtype</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-step-scheduling">Multi-step scheduling</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-executor-backend">Distributed executor backend</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-mode-max-seq-len-to-capture">Graph mode max-seq-len-to-capture</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#whether-to-enable-chunked-prefill">Whether to enable chunked prefill</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-support">Quantization support</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#fp8-quantization">FP8 quantization</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#awq-quantization">AWQ quantization</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#fp8-kv-cached-dtype">fp8 kv-cached-dtype</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-tunableop">PyTorch TunableOp</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#workflow">Workflow</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#offline-tuning">Offline tuning</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-inductor-max-autotune-tuning-knobs">PyTorch inductor max-autotune tuning knobs</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#triton-backend">Triton backend</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#composable-kernel-backend">Composable Kernel backend</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-library-tuning">ROCm library tuning</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#gemm-general-matrix-multiplication">GEMM (general matrix multiplication)</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#hipblaslt-benchmarking">hipBLASLt benchmarking</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#hipblaslt-auto-tuning-using-hipblaslt-bench">hipBLASLt auto-tuning using hipblaslt-bench</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#hipblaslt-backend-assembly-generator-tuning">hipBLASLt backend assembly generator tuning</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#tensilelite-tuning-flow">TensileLite tuning flow</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-initial-solution-parameters">Step 1: Initial solution parameters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-benchmark-common-parameters">Step 2: Benchmark common parameters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-fork-parameters">Step 3: Fork parameters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-benchmark-fork-parameters">Step 4: Benchmark fork parameters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-join-parameters">Step 5: Join parameters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-benchmark-join-parameters">Step 6: Benchmark join parameters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#step-7-benchmark-final-parameters">Step 7: Benchmark final parameters</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#update-logic-yaml-files">Update logic YAML files</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#tensile-optimization-and-performance-tuning-tips">Tensile optimization and performance tuning tips</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-composable-kernel-gemm-kernels">Optimizing Composable Kernel GEMM kernels</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#miopen">MIOpen</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#convolution">Convolution</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#tuning-in-miopen">Tuning in MIOpen</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-fastest-kernel">Finding the fastest kernel</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#rccl">RCCL</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#use-all-eight-gpus">Use all eight GPUs</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#disable-numa-auto-balancing">Disable NUMA auto-balancing</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#disable-acs-for-multi-node-rccl">Disable ACS for multi-node RCCL</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#run-rccl-unittests">Run RCCL-Unittests</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#npkit-profiler">NPKit profiler</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#rccl-tests">RCCL-tests</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#use-one-process-per-gpu-mode">Use one-process-per-GPU mode</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#rccl-in-e2e-workloads">RCCL in E2E workloads</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#triton-kernel-performance-optimization">Triton kernel performance optimization</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-tunable-kernel-configurations">Auto-tunable kernel configurations</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-gpu-resource-utilization">Overall GPU resource utilization</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#mlir-analysis">MLIR analysis</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#isa-assembly-analysis">ISA assembly analysis</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#hip-performance-optimization">HIP performance optimization</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#parallel-execution-and-gpu-hardware-utilization">Parallel execution and GPU hardware utilization</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-usage-optimization">Memory usage optimization</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnostic-and-performance-analysis">Diagnostic and performance analysis</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#debug-memory-access-faults">Debug memory access faults</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-the-occupancy-of-a-kernel">Compute the occupancy of a kernel</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#special-considerations">Special considerations</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-gpu-communications">Multi-GPU communications</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-fsdp-and-rccl-settings">Multi-node FSDP and RCCL settings</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference external nav-link" href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/">AI tutorials</a></li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-hpc/index">Use ROCm for HPC</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/system-optimization/index">System optimization</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/gpu-performance/mi300x">AMD Instinct MI300X performance guides</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/system-debugging">System debugging</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-language-and-system-level-debug-flags-and-environment-variables">ROCm language and system-level debug, flags, and environment variables</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocr-error-code">ROCr error code</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#command-to-dump-firmware-version-and-get-linux-kernel-version">Command to dump firmware version and get Linux kernel version</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#debug-flags">Debug flags</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocr-level-environment-variables-for-debug">ROCr level environment variables for debug</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#turn-off-page-retry-on-gfx9-vega-devices">Turn off page retry on GFX9/Vega devices</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#hip-environment-variables-3-x">HIP environment variables 3.x</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#opencl-debug-flags">OpenCL debug flags</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#pcie-debug">PCIe-debug</a></li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/compiler-topics">Use advanced compiler features</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference external nav-link" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/index.html">ROCm compiler infrastructure</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference external nav-link" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/conceptual/using-gpu-sanitizer.html">Use AddressSanitizer</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference external nav-link" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/conceptual/openmp.html">OpenMP support</a></li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/setting-cus">Set the number of CUs</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/Bar-Memory">Troubleshoot BAR access limitation</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-physical-address-limitation">Handling physical address limitation</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#bar-configuration-for-amd-gpus">BAR configuration for AMD GPUs</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-bar-usage-on-amd-gpus">Example of BAR usage on AMD GPUs</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference external nav-link" href="https://github.com/amd/rocm-examples">ROCm examples</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Conceptual</span></p>
<ul class="nav section-nav flex-column">
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/gpu-arch">GPU architecture overview</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/gpu-arch/mi300">MI300 microarchitecture</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#node-level-architecture">Node-level architecture</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference external nav-link" href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-mi300-cdna3-instruction-set-architecture.pdf">AMD Instinct MI300/CDNA3 ISA</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference external nav-link" href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-3-white-paper.pdf">White paper</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/gpu-arch/mi300-mi200-performance-counters">MI300 and MI200 Performance counter</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#mi300-and-mi200-series-performance-counters">MI300 and MI200 series performance counters</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#command-processor-counters">Command processor counters</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#command-processor-fetcher-counters">Command processor-fetcher counters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#command-processor-compute-counters">Command processor-compute counters</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#graphics-register-bus-manager-counters">Graphics register bus manager counters</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#shader-processor-input-counters">Shader processor input counters</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-unit-counters">Compute unit counters</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#instruction-mix">Instruction mix</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-fused-multiply-add-operation-counters">Matrix fused multiply-add operation counters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#level-counters">Level counters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#wavefront-counters">Wavefront counters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#wavefront-cycle-counters">Wavefront cycle counters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#lds-counters">LDS counters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#miscellaneous-counters">Miscellaneous counters</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#l1-instruction-cache-l1i-and-scalar-l1-data-cache-l1d-counters">L1 instruction cache (L1i) and scalar L1 data cache (L1d) counters</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-l1-cache-subsystem-counters">Vector L1 cache subsystem counters</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#texture-addressing-unit-counters">Texture addressing unit counters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#texture-data-unit-counters">Texture data unit counters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#texture-cache-per-pipe-counters">Texture cache per pipe counters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#texture-cache-arbiter-counters">Texture cache arbiter counters</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#l2-cache-access-counters">L2 cache access counters</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#mi300-and-mi200-series-derived-metrics-list">MI300 and MI200 series derived metrics list</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#hardware-counters-by-and-over-all-texture-addressing-unit-instances">Hardware counters by and over all texture addressing unit instances</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#hardware-counters-over-all-texture-cache-per-channel-instances">Hardware counters over all texture cache per channel instances</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#hardware-counters-by-for-or-over-all-texture-cache-per-pipe-instances">Hardware counters by, for, or over all texture cache per pipe instances</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#hardware-counter-over-all-texture-data-unit-instances">Hardware counter over all texture data unit instances</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/gpu-arch/mi250">MI250 microarchitecture</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#node-level-architecture">Node-level architecture</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference external nav-link" href="https://www.amd.com/system/files/TechDocs/instinct-mi200-cdna2-instruction-set-architecture.pdf">AMD Instinct MI200/CDNA2 ISA</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference external nav-link" href="https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna2-white-paper.pdf">White paper</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/gpu-arch/mi100">MI100 microarchitecture</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#microarchitecture">Microarchitecture</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference external nav-link" href="https://www.amd.com/system/files/TechDocs/instinct-mi100-cdna1-shader-instruction-set-architecture%C2%A0.pdf">AMD Instinct MI100/CDNA1 ISA</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference external nav-link" href="https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna-white-paper.pdf">White paper</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/file-reorg">File structure (Linux FHS)</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#adopting-the-fhs">Adopting the FHS</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#changes-from-earlier-rocm-versions">Changes from earlier ROCm versions</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-fhs-reorganization-backward-compatibility">ROCm FHS reorganization: backward compatibility</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#wrapper-header-files">Wrapper header files</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#executable-files">Executable files</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#library-files">Library files</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#cmake-config-files">CMake config files</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#changes-required-in-applications-using-rocm">Changes required in applications using ROCm</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#changes-in-versioning-specifications">Changes in versioning specifications</a></li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/gpu-isolation">GPU isolation techniques</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-variables">Environment variables</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#rocr-visible-devices"><code class="docutils literal notranslate"><span class="pre">ROCR_VISIBLE_DEVICES</span></code></a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-device-ordinal"><code class="docutils literal notranslate"><span class="pre">GPU_DEVICE_ORDINAL</span></code></a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#hip-visible-devices"><code class="docutils literal notranslate"><span class="pre">HIP_VISIBLE_DEVICES</span></code></a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#cuda-visible-devices"><code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code></a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#omp-default-device"><code class="docutils literal notranslate"><span class="pre">OMP_DEFAULT_DEVICE</span></code></a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#docker">Docker</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-passthrough-to-virtual-machines">GPU passthrough to virtual machines</a></li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/cmake-packages">Using CMake</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-dependencies">Finding dependencies</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-hip-in-cmake">Using HIP in CMake</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-hip-single-source-programming-model">Using the HIP single-source programming model</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#consuming-rocm-c-c-libraries">Consuming ROCm C/C++ libraries</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#consuming-the-hip-api-in-c-code">Consuming the HIP API in C++ code</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#compiling-device-code-in-c-language-mode">Compiling device code in C++ language mode</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-cmake-packages">ROCm CMake packages</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-cmake-presets">Using CMake presets</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-hip-with-presets">Using HIP with presets</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/ai-pytorch-inception">Inception v3 with PyTorch</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-training">Deep learning training</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-phases">Training phases</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#case-studies">Case studies</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#inception-v3-with-pytorch">Inception V3 with PyTorch</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-a-pre-trained-model">Evaluating a pre-trained model</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#training-inception-v3">Training Inception V3</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-model-with-cifar-10-on-pytorch">Custom model with CIFAR-10 on PyTorch</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#case-study-tensorflow-with-fashion-mnist">Case study: TensorFlow with Fashion-MNIST</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#case-study-tensorflow-with-text-classification">Case study: TensorFlow with text classification</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav section-nav flex-column">
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-reference/api-libraries">ROCm libraries</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-reference/rocm-tools">ROCm tools, compilers, and runtimes</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-reference/gpu-arch-specs">Accelerator and GPU hardware specifications</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#glossary">Glossary</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-reference/gpu-atomics-operation">Hardware atomics operation support</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#support-summary">Support summary</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#amd-instinct-accelerators">AMD Instinct™ accelerators</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#amd-gfx-generic-targets">AMD gfx generic targets</a></li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#gpus-atomics-support">GPUs atomics support</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#integer-atomics-operations">Integer atomics operations</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">AMD Instinct accelerators</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">AMD gfx generic targets</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#bitwise-atomics-operations">Bitwise atomics operations</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">AMD Instinct accelerators</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">AMD gfx generic targets</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#float-atomics-operations">Float atomics operations</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">AMD Instinct accelerators</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">AMD gfx generic targets</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-reference/precision-support">Precision support</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#integral-types">Integral types</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#floating-point-types">Floating-point types</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#level-of-support-definitions">Level of support definitions</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-type-support-by-hardware-architecture">Data type support by hardware architecture</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#hip-c-type-implementation-support">HIP C++ type implementation support</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-units-support">Compute units support</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-core-support">Matrix core support</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#atomic-operations-support">Atomic operations support</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-type-support-in-rocm-libraries">Data type support in ROCm libraries</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#libraries-input-output-type-support">Libraries input/output type support</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#hipdatatype-enumeration">hipDataType enumeration</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-reference/graph-safe-support">Graph safe support</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Contribute</span></p>
<ul class="nav section-nav flex-column">
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-contribute/contributing">Contributing to the ROCm documentation</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-rocm-repositories">The ROCm repositories</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#editing-and-adding-to-the-documentation">Editing and adding to the documentation</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-contribute/toolchain">ROCm documentation toolchain</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-docs-core">rocm-docs-core</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#sphinx">Sphinx</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#sphinx-external-toc">Sphinx External ToC</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#sphinx-book-theme">Sphinx-book-theme</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#sphinx-design">Sphinx Design</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#doxygen">Doxygen</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#breathe">Breathe</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#read-the-docs">Read the Docs</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-contribute/building">Building documentation</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#github">GitHub</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#command-line">Command line</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#visual-studio-code">Visual Studio Code</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-contribute/feedback">Providing feedback about the ROCm documentation</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#participating-in-discussions-through-github-discussions">Participating in discussions through GitHub Discussions</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#submitting-issues-through-github-issues">Submitting issues through GitHub Issues</a></li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-about/license">ROCm licenses</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-component-licenses">ROCm component licenses</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#package-licensing">Package licensing</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<section class="tex2jax_ignore mathjax_ignore" id="amd-rocm-documentation">
<h1>AMD ROCm documentation<a class="headerlink" href="#amd-rocm-documentation" title="Link to this heading">#</a></h1><div class="sd-container-fluid sd-sphinx-override sd-p-0 sd-mt-2 sd-mb-4 sd-p-2 sd-rounded-1 docutils" id="rocm-docs-core-article-info">
<div class="sd-row sd-row-cols-2 sd-gx-2 sd-gy-1 docutils">
<div class="sd-col sd-d-flex-row sd-align-minor-center docutils">
<div class="sd-container-fluid sd-sphinx-override docutils">
<div class="sd-row sd-row-cols-2 sd-row-cols-xs-2 sd-row-cols-sm-3 sd-row-cols-md-3 sd-row-cols-lg-3 sd-gx-3 sd-gy-1 docutils">
<div class="sd-col sd-col-auto sd-d-flex-row sd-align-minor-center docutils">
<p class="sd-p-0 sd-m-0" style="color:gray;">
<span class="sd-pr-2 article-info-date-svg">
<svg aria-hidden="true" class="sd-octicon sd-octicon-calendar" height="16.0px" version="1.1" viewbox="0 0 16 16" width="16.0px">
<path d="M4.75 0a.75.75 0 01.75.75V2h5V.75a.75.75 0 011.5 0V2h1.25c.966 0 1.75.784 1.75 1.75v10.5A1.75 1.75 0 0113.25 16H2.75A1.75 1.75 0 011 14.25V3.75C1 2.784 1.784 2 2.75 2H4V.75A.75.75 0 014.75 0zm0 3.5h8.5a.25.25 0 01.25.25V6h-11V3.75a.25.25 0 01.25-.25h2zm-2.25 4v6.75c0 .138.112.25.25.25h10.5a.25.25 0 00.25-.25V7.5h-11z" fill-rule="evenodd"></path>
</svg>
</span>
                            2025-06-05
                        </p>
</div>
<div class="sd-col sd-col-auto sd-d-flex-row sd-align-minor-center docutils">
<p class="sd-p-0 sd-m-0" style="color:gray;">
<span class="sd-pr-2 article-info-read-time-svg">
<svg aria-hidden="true" class="sd-octicon sd-octicon-clock" height="16.0px" version="1.1" viewbox="0 0 16 16" width="16.0px">
<path d="M1.5 8a6.5 6.5 0 1113 0 6.5 6.5 0 01-13 0zM8 0a8 8 0 100 16A8 8 0 008 0zm.5 4.75a.75.75 0 00-1.5 0v3.5a.75.75 0 00.471.696l2.5 1a.75.75 0 00.557-1.392L8.5 7.742V4.75z" fill-rule="evenodd"></path>
</svg>
</span>
                            624 min read time
                        </p>
</div>
<div class="sd-col sd-col-auto sd-d-flex-row sd-align-minor-center docutils" style="color:gray;">
                        Applies to Linux and Windows
                    </div>
<div class="sd-col sd-col-auto sd-d-flex-row sd-align-minor-center docutils">
<p class="sd-p-0 sd-m-0" style="color:gray;"></p>
</div>
</div>
</div>
</div>
</div>
</div>

<p>ROCm is an open-source software platform optimized to extract HPC and AI workload
performance from AMD Instinct accelerators and AMD Radeon GPUs while maintaining
compatibility with industry software frameworks. For more information, see
<a class="reference internal" href="#document-what-is-rocm"><span class="std std-doc">What is ROCm?</span></a></p>
<p>ROCm supports multiple programming languages and programming interfaces such as
<a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/index.html" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-doc">HIP (Heterogeneous-Compute Interface for Portability)</span></a>, OpenCL,
and OpenMP, as explained in the <a class="reference internal" href="#document-how-to/programming_guide"><span class="std std-doc">Programming guide</span></a>.</p>
<p>If you’re using AMD Radeon™ PRO or Radeon GPUs in a workstation setting with a display connected, review <a class="reference external" href="https://rocm.docs.amd.com/projects/radeon/en/latest/index.html" title="(in Use ROCm™ on Radeon™ GPUs Documentation)"><span class="xref std std-doc">Radeon-specific ROCm documentation</span></a>.</p>
<p>ROCm documentation is organized into the following categories:</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 rocm-doc-grid docutils">
<div class="sd-row sd-row-cols-1 sd-row-cols-xs-1 sd-row-cols-sm-2 sd-row-cols-md-2 sd-row-cols-lg-2 sd-g-3 sd-g-xs-3 sd-g-sm-3 sd-g-md-3 sd-g-lg-3 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body rocm-card-banner rocm-hue-2 docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
Install</div>
<ul class="simple">
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">ROCm on Linux</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-windows/en/latest/reference/system-requirements.html" title="(in HIP SDK installation on Windows v6.0.0)"><span class="xref std std-doc">HIP SDK on Windows</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/radeon/en/latest/index.html">ROCm on Radeon GPUs</a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#document-how-to/deep-learning-rocm"><span class="doc">Deep learning frameworks</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#document-how-to/build-rocm"><span class="doc">Build from source</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body rocm-card-banner rocm-hue-12 docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
How to</div>
<ul class="simple">
<li><p class="sd-card-text"><a class="reference internal" href="#document-how-to/rocm-for-ai/index"><span class="std std-doc">Use ROCm for AI</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/">AI tutorials</a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#document-how-to/rocm-for-hpc/index"><span class="std std-doc">Use ROCm for HPC</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#document-how-to/system-optimization/index"><span class="std std-doc">System optimization</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#document-how-to/tuning-guides/mi300x/index"><span class="std std-doc">AMD Instinct MI300X performance validation and tuning</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#document-how-to/system-debugging"><span class="std std-doc">System debugging</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#document-conceptual/compiler-topics"><span class="std std-doc">Use advanced compiler features</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#document-how-to/setting-cus"><span class="doc std std-doc">Set the number of CUs</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#document-how-to/Bar-Memory"><span class="std std-doc">Troubleshoot BAR access limitation</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://github.com/amd/rocm-examples">ROCm examples</a></p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body rocm-card-banner rocm-hue-8 docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
Conceptual</div>
<ul class="simple">
<li><p class="sd-card-text"><a class="reference internal" href="#document-conceptual/gpu-arch"><span class="std std-doc">GPU architecture overview</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#document-conceptual/file-reorg"><span class="std std-doc">File structure (Linux FHS)</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#document-conceptual/gpu-isolation"><span class="std std-doc">GPU isolation techniques</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#document-conceptual/cmake-packages"><span class="std std-doc">Using CMake</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#document-conceptual/ai-pytorch-inception"><span class="std std-doc">Inception v3 with PyTorch</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body rocm-card-banner rocm-hue-6 docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
Reference</div>
<!-- markdownlint-disable MD051 -->
<ul class="simple">
<li><p class="sd-card-text"><a class="reference internal" href="#document-reference/api-libraries"><span class="std std-doc">ROCm libraries</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#document-reference/rocm-tools"><span class="std std-doc">ROCm tools, compilers, and runtimes</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#document-reference/gpu-arch-specs"><span class="std std-doc">Accelerator and  GPU hardware specifications</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#document-reference/precision-support"><span class="std std-doc">Precision support</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#document-reference/graph-safe-support"><span class="std std-doc">Graph safe support</span></a></p></li>
</ul>
<!-- markdownlint-enable MD051 -->
</div>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper compound">
<span id="document-what-is-rocm"></span><section id="what-is-rocm">
<h2>What is ROCm?<a class="headerlink" href="#what-is-rocm" title="Link to this heading">#</a></h2>
<p>ROCm is a software stack, composed primarily of open-source software, that
provides the tools for programming AMD Graphics Processing Units (GPUs), from
low-level kernels to high-level end-user applications.</p>
<a class="reference internal image-reference" href="_images/rocm-software-stack-6_4_0.jpg"><img alt="AMD's ROCm software stack and enabling technologies." class="align-center" src="_images/rocm-software-stack-6_4_0.jpg" style="width: 800px;"/>
</a>
<p>Specifically, ROCm provides the tools for
<a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/index.html" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-doc">HIP (Heterogeneous-computing Interface for Portability)</span></a>,
OpenCL and OpenMP. These include compilers, libraries for high-level
functions, debuggers, profilers and runtimes.</p>
<section id="rocm-components">
<h3>ROCm components<a class="headerlink" href="#rocm-components" title="Link to this heading">#</a></h3>
<p>ROCm consists of the following components. For information on the license associated with each component,
see <a class="reference internal" href="#document-about/license"><span class="doc">ROCm licensing</span></a>.</p>
<section id="libraries">
<h4>Libraries<a class="headerlink" href="#libraries" title="Link to this heading">#</a></h4>
<section id="machine-learning-computer-vision">
<h5>Machine Learning &amp; Computer Vision<a class="headerlink" href="#machine-learning-computer-vision" title="Link to this heading">#</a></h5>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/composable_kernel/en/latest/index.html" title="(in Composable Kernel Documentation v1.1.0)"><span class="xref std std-doc">Composable Kernel</span></a></p></td>
<td><p>Provides a programming model for writing performance critical kernels for machine learning workloads across multiple architectures</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/AMDMIGraphX/en/latest/index.html" title="(in MIGraphX v2.12.0)"><span class="xref std std-doc">MIGraphX</span></a></p></td>
<td><p>Graph inference engine that accelerates machine learning model inference</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/MIOpen/en/latest/index.html" title="(in MIOpen Documentation v3.4.0)"><span class="xref std std-doc">MIOpen</span></a></p></td>
<td><p>An open source deep-learning library</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/MIVisionX/en/latest/index.html" title="(in MIVisionX Documentation v3.2.0)"><span class="xref std std-doc">MIVisionX</span></a></p></td>
<td><p>Set of comprehensive computer vision and machine learning libraries, utilities, and applications</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rpp/en/latest/index.html" title="(in RPP documentation v1.9.10)"><span class="xref std std-doc">ROCm Performance Primitives (RPP)</span></a></p></td>
<td><p>Comprehensive high-performance computer vision library for AMD processors with HIP/OpenCL/CPU back-ends</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocAL/en/latest/index.html" title="(in rocAL Documentation v2.2.0)"><span class="xref std std-doc">rocAL</span></a></p></td>
<td><p>An augmentation library designed to decode and process images and videos</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocDecode/en/latest/index.html" title="(in rocDecode documentation v0.10.0)"><span class="xref std std-doc">rocDecode</span></a></p></td>
<td><p>High-performance SDK for access to video decoding features on AMD GPUs</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocJPEG/en/latest/index.html" title="(in rocJPEG Documentation v0.8.0)"><span class="xref std std-doc">rocJPEG</span></a></p></td>
<td><p>Library for decoding JPG images on AMD GPUs</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocPyDecode/en/latest/index.html" title="(in rocPyDecode v0.3.1)"><span class="xref std std-doc">rocPyDecode</span></a></p></td>
<td><p>Provides access to rocDecode APIs in both Python and C/C++ languages</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="communication">
<h5>Communication<a class="headerlink" href="#communication" title="Link to this heading">#</a></h5>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rccl/en/latest/index.html" title="(in RCCL Documentation v2.22.3)"><span class="xref std std-doc">RCCL</span></a></p></td>
<td><p>Standalone library that provides multi-GPU and multi-node collective communication primitives</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocSHMEM/en/latest/index.html" title="(in rocSHMEM v2.0.1)"><span class="xref std std-doc">rocSHMEM</span></a></p></td>
<td><p>An intra-kernel networking library that provides GPU-centric networking through an OpenSHMEM-like interface</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="math">
<h5>Math<a class="headerlink" href="#math" title="Link to this heading">#</a></h5>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ROCm/half/">half</a></p></td>
<td><p>C++ header-only library that provides an IEEE 754 conformant, 16-bit half-precision floating-point type, along with corresponding arithmetic operators, type conversions, and common mathematical functions</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipBLAS/en/latest/index.html" title="(in hipBLAS Documentation v2.4.0)"><span class="xref std std-doc">hipBLAS</span></a></p></td>
<td><p>BLAS-marshaling library that supports <a class="reference external" href="https://rocm.docs.amd.com/projects/rocBLAS/en/latest/index.html" title="(in rocBLAS Documentation v4.4.1)"><span class="xref std std-doc">rocBLAS</span></a> and cuBLAS backends</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipBLASLt/en/latest/index.html" title="(in hipBLASLt Documentation v0.12.1)"><span class="xref std std-doc">hipBLASLt</span></a></p></td>
<td><p>Provides general matrix-matrix operations with a flexible API and extends functionalities beyond traditional BLAS library</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipFFT/en/latest/index.html" title="(in hipFFT Documentation v1.0.18)"><span class="xref std std-doc">hipFFT</span></a></p></td>
<td><p>Fast Fourier transforms (FFT)-marshalling library that supports rocFFT or cuFFT backends</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipfort/en/latest/index.html" title="(in hipfort Documentation v0.6.0)"><span class="xref std std-doc">hipfort</span></a></p></td>
<td><p>Fortran interface library for accessing GPU Kernels</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipRAND/en/latest/index.html" title="(in hipRAND Documentation v2.12.0)"><span class="xref std std-doc">hipRAND</span></a></p></td>
<td><p>Ports CUDA applications that use the cuRAND library into the HIP layer</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipSOLVER/en/latest/index.html" title="(in hipSOLVER Documentation v2.4.0)"><span class="xref std std-doc">hipSOLVER</span></a></p></td>
<td><p>An LAPACK-marshalling library that supports <a class="reference external" href="https://rocm.docs.amd.com/projects/rocSOLVER/en/latest/index.html" title="(in rocSOLVER Documentation v3.28.2)"><span class="xref std std-doc">rocSOLVER</span></a> and cuSOLVER backends</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipSPARSE/en/latest/index.html" title="(in hipSPARSE Documentation v3.2.0)"><span class="xref std std-doc">hipSPARSE</span></a></p></td>
<td><p>SPARSE-marshalling library that supports <a class="reference external" href="https://rocm.docs.amd.com/projects/rocSPARSE/en/latest/index.html" title="(in rocSPARSE Documentation v3.4.0)"><span class="xref std std-doc">rocSPARSE</span></a> and cuSPARSE backends</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipSPARSELt/en/latest/index.html" title="(in hipSPARSELt Documentation v0.2.3)"><span class="xref std std-doc">hipSPARSELt</span></a></p></td>
<td><p>SPARSE-marshalling library with multiple supported backends</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocALUTION/en/latest/index.html" title="(in rocALUTION Documentation v3.2.3)"><span class="xref std std-doc">rocALUTION</span></a></p></td>
<td><p>Sparse linear algebra library for exploring fine-grained parallelism on ROCm runtime and toolchains</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocBLAS/en/latest/index.html" title="(in rocBLAS Documentation v4.4.1)"><span class="xref std std-doc">rocBLAS</span></a></p></td>
<td><p>BLAS implementation (in the HIP programming language) on the ROCm runtime and toolchains</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocFFT/en/latest/index.html" title="(in rocFFT Documentation v1.0.32)"><span class="xref std std-doc">rocFFT</span></a></p></td>
<td><p>Software library for computing fast Fourier transforms (FFTs) written in HIP</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocRAND/en/latest/index.html" title="(in rocRAND Documentation v3.3.0)"><span class="xref std std-doc">rocRAND</span></a></p></td>
<td><p>Provides functions that generate pseudorandom and quasirandom numbers</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocSOLVER/en/latest/index.html" title="(in rocSOLVER Documentation v3.28.2)"><span class="xref std std-doc">rocSOLVER</span></a></p></td>
<td><p>An implementation of LAPACK routines on ROCm software, implemented in the HIP programming language and optimized for AMD’s latest discrete GPUs</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocSPARSE/en/latest/index.html" title="(in rocSPARSE Documentation v3.4.0)"><span class="xref std std-doc">rocSPARSE</span></a></p></td>
<td><p>Exposes a common interface that provides BLAS for sparse computation implemented on ROCm runtime and toolchains (in the HIP programming language)</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocWMMA/en/latest/index.html" title="(in rocWMMA Documentation v1.7.0)"><span class="xref std std-doc">rocWMMA</span></a></p></td>
<td><p>C++ library for accelerating mixed-precision matrix multiply-accumulate (MMA) operations</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/Tensile/en/latest/src/index.html" title="(in Tensile Documentation v4.43.0)"><span class="xref std std-doc">Tensile</span></a></p></td>
<td><p>Creates benchmark-driven backend libraries for GEMMs, GEMM-like problems, and general N-dimensional tensor contractions</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="primitives">
<h5>Primitives<a class="headerlink" href="#primitives" title="Link to this heading">#</a></h5>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipCUB/en/latest/index.html" title="(in hipCUB Documentation v3.4.0)"><span class="xref std std-doc">hipCUB</span></a></p></td>
<td><p>Thin header-only wrapper library on top of <a class="reference external" href="https://rocm.docs.amd.com/projects/rocPRIM/en/latest/index.html" title="(in rocPRIM Documentation v3.4.1)"><span class="xref std std-doc">rocPRIM</span></a> or CUB that allows project porting using the CUB library to the HIP layer</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipTensor/en/latest/index.html" title="(in hipTensor Documentation v1.5.0)"><span class="xref std std-doc">hipTensor</span></a></p></td>
<td><p>AMD’s C++ library for accelerating tensor primitives based on the composable kernel library</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocPRIM/en/latest/index.html" title="(in rocPRIM Documentation v3.4.1)"><span class="xref std std-doc">rocPRIM</span></a></p></td>
<td><p>Header-only library for HIP parallel primitives</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocThrust/en/latest/index.html" title="(in rocThrust Documentation v3.3.0)"><span class="xref std std-doc">rocThrust</span></a></p></td>
<td><p>Parallel algorithm library</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="tools">
<h4>Tools<a class="headerlink" href="#tools" title="Link to this heading">#</a></h4>
<section id="system-management">
<h5>System Management<a class="headerlink" href="#system-management" title="Link to this heading">#</a></h5>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/amdsmi/en/latest/index.html" title="(in AMD SMI v25.5.1)"><span class="xref std std-doc">AMD SMI</span></a></p></td>
<td><p>System management interface to control AMD GPU settings, monitor performance, and retrieve device and process information</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rdc/en/latest/index.html" title="(in ROCm Data Center Documentation)"><span class="xref std std-doc">ROCm Data Center Tool</span></a></p></td>
<td><p>Simplifies administration and addresses key infrastructure challenges in AMD GPUs in cluster and data-center environments</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocminfo/en/latest/index.html" title="(in rocminfo v1.0.0)"><span class="xref std std-doc">rocminfo</span></a></p></td>
<td><p>Reports system information</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocm_smi_lib/en/latest/index.html" title="(in ROCm SMI LIB Documentation v7.7.0)"><span class="xref std std-doc">ROCm SMI</span></a></p></td>
<td><p>C library for Linux that provides a user space interface for applications to monitor and control GPU applications</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCmValidationSuite/en/latest/index.html" title="(in RVS Documentation v1.1.0)"><span class="xref std std-doc">ROCm Validation Suite</span></a></p></td>
<td><p>Detects and troubleshoots common problems affecting AMD GPUs running in a high-performance computing environment</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="performance">
<h5>Performance<a class="headerlink" href="#performance" title="Link to this heading">#</a></h5>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocm_bandwidth_test/en/latest/index.html" title="(in rocm_bandwidth_test)"><span class="xref std std-doc">ROCm Bandwidth Test</span></a></p></td>
<td><p>Captures the performance characteristics of buffer copying and kernel read/write operations</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler-compute/en/latest/index.html" title="(in ROCm Compute Profiler v3.1.1)"><span class="xref std std-doc">ROCm Compute Profiler</span></a></p></td>
<td><p>Kernel-level profiling for machine learning and high performance computing (HPC) workloads</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler-systems/en/latest/index.html" title="(in rocprofiler-systems v1.0.2)"><span class="xref std std-doc">ROCm Systems Profiler</span></a></p></td>
<td><p>Comprehensive profiling and tracing of applications running on the CPU or the CPU and GPU</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler/en/latest/index.html" title="(in rocprofiler Documentation v2.0.0)"><span class="xref std std-doc">ROCProfiler</span></a></p></td>
<td><p>Profiling tool for HIP applications</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler-sdk/en/latest/index.html" title="(in Rocprofiler SDK v0.6.0)"><span class="xref std std-doc">ROCprofiler-SDK</span></a></p></td>
<td><p>Toolkit for developing analysis tools for profiling and tracing GPU compute applications. This toolkit is in beta and subject to change</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/roctracer/en/latest/index.html" title="(in roctracer Documentation v4.1.0)"><span class="xref std std-doc">ROCTracer</span></a></p></td>
<td><p>Intercepts runtime API calls and traces asynchronous activity</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprof-compute-viewer/en/amd-mainline/">ROCprof Compute Viewer</a> is a tool for visualizing and analyzing GPU thread trace data collected using <a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler-sdk/en/latest/index.html" title="(in Rocprofiler SDK v0.6.0)"><span class="xref std std-doc">rocprofv3</span></a>.
Note that <a class="reference external" href="https://rocm.docs.amd.com/projects/rocprof-compute-viewer/en/amd-mainline/">ROCprof Compute Viewer</a> is in an early access state. Running production workloads is not recommended.</p>
</div>
</section>
<section id="development">
<h5>Development<a class="headerlink" href="#development" title="Link to this heading">#</a></h5>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/HIPIFY/en/latest/index.html" title="(in HIPIFY Documentation)"><span class="xref std std-doc">HIPIFY</span></a></p></td>
<td><p>Translates CUDA source code into portable HIP C++</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCmCMakeBuildTools/en/latest/index.html" title="(in ROCm CMake Build Tools v0.14.0)"><span class="xref std std-doc">ROCm CMake</span></a></p></td>
<td><p>Collection of CMake modules for common build and development tasks</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCdbgapi/en/latest/index.html" title="(in ROCdbgapi Documentation v0.77.2)"><span class="xref std std-doc">ROCdbgapi</span></a></p></td>
<td><p>ROCm debugger API library</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCgdb/en/latest/index.html" title="(in ROCgdb Documentation v15.2)"><span class="xref std std-doc">ROCm Debugger (ROCgdb)</span></a></p></td>
<td><p>Source-level debugger for Linux, based on the GNU Debugger (GDB)</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocr_debug_agent/en/latest/index.html" title="(in rocr_debug_agent v2.0.4)"><span class="xref std std-doc">ROCr Debug Agent</span></a></p></td>
<td><p>Prints the state of all AMD GPU wavefronts that caused a queue error by sending a SIGQUIT signal to the process while the program is running</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="compilers">
<h4>Compilers<a class="headerlink" href="#compilers" title="Link to this heading">#</a></h4>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/HIPCC/en/latest/index.html" title="(in HIPCC Documentation v1.1.1)"><span class="xref std std-doc">HIPCC</span></a></p></td>
<td><p>Compiler driver utility that calls Clang or NVCC and passes the appropriate include and library options for the target compiler and HIP infrastructure</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/index.html" title="(in llvm-project Documentation v19.0.0)"><span class="xref std std-doc">ROCm compilers</span></a></p></td>
<td><p>ROCm LLVM compiler infrastructure</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ROCm/flang/">FLANG</a></p></td>
<td><p>An out-of-tree Fortran compiler targeting LLVM</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="runtimes">
<h4>Runtimes<a class="headerlink" href="#runtimes" title="Link to this heading">#</a></h4>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/understand/amd_clr.html" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-doc">AMD Compute Language Runtime (CLR)</span></a></p></td>
<td><p>Contains source code for AMD’s compute language runtimes: HIP and OpenCL</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/index.html" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-doc">HIP</span></a></p></td>
<td><p>AMD’s GPU programming language extension and the GPU runtime</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCR-Runtime/en/latest/index.html" title="(in ROCR Documentation v1.15.0)"><span class="xref std std-doc">ROCR-Runtime</span></a></p></td>
<td><p>User-mode API interfaces and libraries necessary for host applications to launch compute kernels on available HSA ROCm kernel agents</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
</section>
<span id="document-about/release-notes"></span><!-- Do not edit this file!                                 -->
<!-- This file is autogenerated with                        -->
<!--   tools/autotag/tag_script.py                          -->
<!-- Disable lints since this is an auto-generated file.    -->
<!-- markdownlint-disable blanks-around-headers             -->
<!-- markdownlint-disable no-duplicate-header               -->
<!-- markdownlint-disable no-blanks-blockquote              -->
<!-- markdownlint-disable ul-indent                         -->
<!-- markdownlint-disable no-trailing-spaces                -->
<!-- markdownlint-disable reference-links-images            -->
<!-- markdownlint-disable no-missing-space-atx              -->
<!-- spellcheck-disable                                     -->
<section class="tex2jax_ignore mathjax_ignore" id="rocm-6-4-3-release-notes">
<h2>ROCm 6.4.3 release notes<a class="headerlink" href="#rocm-6-4-3-release-notes" title="Link to this heading">#</a></h2>
<p>The release notes provide a summary of notable changes since the previous ROCm release.</p>
<ul class="simple">
<li><p><a class="reference internal" href="#release-highlights">Release highlights</a></p></li>
<li><p><a class="reference internal" href="#operating-system-and-hardware-support-changes">Operating system and hardware support changes</a></p></li>
<li><p><a class="reference internal" href="#rocm-components">ROCm components versioning</a></p></li>
<li><p><a class="reference internal" href="#detailed-component-changes">Detailed component changes</a></p></li>
<li><p><a class="reference internal" href="#rocm-known-issues">ROCm known issues</a></p></li>
<li><p><a class="reference internal" href="#rocm-upcoming-changes">ROCm upcoming changes</a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you’re using AMD Radeon™ PRO or Radeon GPUs in a workstation setting with a display connected, see the <a class="reference external" href="https://rocm.docs.amd.com/projects/radeon/en/latest/docs/compatibility/native_linux/native_linux_compatibility.html">Use ROCm on Radeon GPUs</a>
documentation to verify compatibility and system requirements.</p>
</div>
<section id="release-highlights">
<h3>Release highlights<a class="headerlink" href="#release-highlights" title="Link to this heading">#</a></h3>
<p>ROCm 6.4.3 is a quality release that resolves the following issues. For changes to individual components, see <a class="reference internal" href="#detailed-component-changes">Detailed component changes</a>.</p>
<section id="amdgpu-driver-updates">
<h4>AMDGPU driver updates<a class="headerlink" href="#amdgpu-driver-updates" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Resolved an issue causing performance degradation in communication operations, caused by increased latency in certain RCCL applications. The fix prevents unnecessary queue eviction during the fork process.</p></li>
<li><p>Fixed an issue in the AMDGPU driver’s scheduler constraints that could cause queue preemption to fail during workload execution.</p></li>
</ul>
</section>
<section id="rocm-smi-update">
<h4>ROCm SMI update<a class="headerlink" href="#rocm-smi-update" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Fixed the failure to load GPU data like System Clock (SCLK) by adjusting the logic for retrieving GPU board voltage.</p></li>
</ul>
</section>
<section id="rocm-documentation-updates">
<h4>ROCm documentation updates<a class="headerlink" href="#rocm-documentation-updates" title="Link to this heading">#</a></h4>
<p>ROCm documentation continues to be updated to provide clearer and more comprehensive guidance for a wider variety of user needs and use cases.</p>
<ul>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/">Tutorials for AI developers</a> have been expanded with the following five new tutorials:</p>
<ul class="simple">
<li><p>Inference tutorials</p>
<ul>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/inference/opea_deployment_and_evaluation.html">ChatQnA vLLM deployment and performance evaluation</a></p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/inference/t2v_comfyui_radeon.html">Text-to-video generation with ComfyUI</a></p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/inference/deepseek_janus_cpu_gpu.html">DeepSeek Janus Pro on CPU or GPU</a></p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/inference/vllm_v1_DSR1.html">DeepSeek-R1 with vLLM V1</a></p></li>
</ul>
</li>
<li><p>GPU development and optimization tutorial: <a class="reference external" href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/gpu_dev_optimize/aiter_mla_decode_kernel.html">MLA decoding kernel of AITER library</a></p></li>
</ul>
<p>For more information about the changes, see <a class="reference external" href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/changelog.html">Changelog for the AI Developer Hub</a>.</p>
</li>
<li><p>ROCm provides a comprehensive ecosystem for deep learning development. For more details, see <a class="reference external" href="https://rocm.docs.amd.com/en/docs-6.4.3/how-to/deep-learning-rocm.html">Deep learning frameworks for ROCm</a>. AMD ROCm adds support for the following deep learning frameworks:</p>
<ul class="simple">
<li><p>Taichi is an open-source, imperative, and parallel programming language designed for high-performance numerical computation. Embedded in Python, it leverages just-in-time (JIT) compilation frameworks such as LLVM to accelerate compute-intensive Python code by compiling it to native GPU or CPU instructions. It is currently supported on ROCm 6.3.2. For more information, see <a class="reference external" href="https://rocm.docs.amd.com/en/docs-6.4.3/compatibility/ml-compatibility/taichi-compatibility.html">Taichi compatibility</a>.</p></li>
<li><p>Megablocks is a light-weight library for mixture-of-experts (MoE) training. The core of the system is efficient “dropless-MoE” and standard MoE layers. Megablocks is integrated with Megatron-LM, where data and pipeline parallel training of MoEs is supported. It is currently supported on ROCm 6.3.0. For more information, see <a class="reference external" href="https://rocm.docs.amd.com/en/docs-6.4.3/compatibility/ml-compatibility/megablocks-compatibility.html">Megablocks compatibility</a>.</p></li>
</ul>
</li>
<li><p>The <a class="reference external" href="https://rocm.docs.amd.com/en/latest/reference/precision-support.html">Data types and precision support</a> topic now includes new hardware and library support information.</p></li>
</ul>
</section>
</section>
<section id="operating-system-and-hardware-support-changes">
<h3>Operating system and hardware support changes<a class="headerlink" href="#operating-system-and-hardware-support-changes" title="Link to this heading">#</a></h3>
<p>Operating system and hardware support remain unchanged in this release.</p>
<p>See the <a class="reference internal" href="#document-compatibility/compatibility-matrix"><span class="std std-doc">Compatibility
matrix</span></a>
for more information about operating system and hardware compatibility.</p>
</section>
<section id="rocm-components">
<h3>ROCm components<a class="headerlink" href="#rocm-components" title="Link to this heading">#</a></h3>
<p>The following table lists the versions of ROCm components for ROCm 6.4.3.
Click <span class="fab fa-github"></span> to go to the component’s source code on GitHub.</p>
<div class="pst-scrollable-table-container">
<table class="table" id="rocm-rn-components">
<thead>
<tr>
<th>Category</th>
<th>Group</th>
<th>Name</th>
<th>Version</th>
<th></th>
</tr>
</thead>
<colgroup>
<col span="1"/>
<col span="1"/>
</colgroup>
<tbody class="rocm-components-libs rocm-components-ml">
<tr>
<th rowspan="9">Libraries</th>
<th rowspan="9">Machine learning and computer vision</th>
<td><a href="https://rocm.docs.amd.com/projects/composable_kernel/en/docs-6.4.3/index.html">Composable Kernel</a></td>
<td>1.1.0</td>
<td><a href="https://github.com/ROCm/composable_kernel"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/AMDMIGraphX/en/docs-6.4.3/index.html">MIGraphX</a></td>
<td>2.12.0</td>
<td><a href="https://github.com/ROCm/AMDMIGraphX"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/MIOpen/en/docs-6.4.3/index.html">MIOpen</a></td>
<td>3.4.0</td>
<td><a href="https://github.com/ROCm/MIOpen"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/MIVisionX/en/docs-6.4.3/index.html">MIVisionX</a></td>
<td>3.2.0</td>
<td><a href="https://github.com/ROCm/MIVisionX"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocAL/en/docs-6.4.3/index.html">rocAL</a></td>
<td>2.2.0</td>
<td><a href="https://github.com/ROCm/rocAL"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocDecode/en/docs-6.4.3/index.html">rocDecode</a></td>
<td>0.10.0</td>
<td><a href="https://github.com/ROCm/rocDecode"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocJPEG/en/docs-6.4.3/index.html">rocJPEG</a></td>
<td>0.8.0</td>
<td><a href="https://github.com/ROCm/rocJPEG"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocPyDecode/en/docs-6.4.3/index.html">rocPyDecode</a></td>
<td>0.3.1</td>
<td><a href="https://github.com/ROCm/rocPyDecode"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rpp/en/docs-6.4.3/index.html">RPP</a></td>
<td>1.9.10</td>
<td><a href="https://github.com/ROCm/rpp"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
</tbody>
<tbody class="rocm-components-libs rocm-components-communication tbody-reverse-zebra">
<tr>
<th rowspan="2"></th>
<th rowspan="2">Communication</th>
<td><a href="https://rocm.docs.amd.com/projects/rccl/en/docs-6.4.3/index.html">RCCL</a></td>
<td>2.22.3</td>
<td><a href="https://github.com/ROCm/rccl"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocSHMEM/en/docs-6.4.3/index.html">rocSHMEM</a></td>
<td>2.0.1</td>
<td><a href="https://github.com/ROCm/rocSHMEM"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
</tbody>
<tbody class="rocm-components-libs rocm-components-math tbody-reverse-zebra">
<tr>
<th rowspan="16"></th>
<th rowspan="16">Math</th>
<td><a href="https://rocm.docs.amd.com/projects/hipBLAS/en/docs-6.4.3/index.html">hipBLAS</a></td>
<td>2.4.0</td>
<td><a href="https://github.com/ROCm/hipBLAS"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/hipBLASLt/en/docs-6.4.3/index.html">hipBLASLt</a></td>
<td>0.12.1</td>
<td><a href="https://github.com/ROCm/hipBLASLt"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/hipFFT/en/docs-6.4.3/index.html">hipFFT</a></td>
<td>1.0.18</td>
<td><a href="https://github.com/ROCm/hipFFT"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/hipfort/en/docs-6.4.3/index.html">hipfort</a></td>
<td>0.6.0</td>
<td><a href="https://github.com/ROCm/hipfort"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/hipRAND/en/docs-6.4.3/index.html">hipRAND</a></td>
<td>2.12.0</td>
<td><a href="https://github.com/ROCm/hipRAND"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/hipSOLVER/en/docs-6.4.3/index.html">hipSOLVER</a></td>
<td>2.4.0</td>
<td><a href="https://github.com/ROCm/hipSOLVER"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/hipSPARSE/en/docs-6.4.3/index.html">hipSPARSE</a></td>
<td>3.2.0</td>
<td><a href="https://github.com/ROCm/hipSPARSE"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/hipSPARSELt/en/docs-6.4.3/index.html">hipSPARSELt</a></td>
<td>0.2.3</td>
<td><a href="https://github.com/ROCm/hipSPARSELt"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocALUTION/en/docs-6.4.3/index.html">rocALUTION</a></td>
<td>3.2.3</td>
<td><a href="https://github.com/ROCm/rocALUTION"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocBLAS/en/docs-6.4.3/index.html">rocBLAS</a></td>
<td>4.4.1</td>
<td><a href="https://github.com/ROCm/rocBLAS"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocFFT/en/docs-6.4.3/index.html">rocFFT</a></td>
<td>1.0.32</td>
<td><a href="https://github.com/ROCm/rocFFT"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocRAND/en/docs-6.4.3/index.html">rocRAND</a></td>
<td>3.3.0</td>
<td><a href="https://github.com/ROCm/rocRAND"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocSOLVER/en/docs-6.4.3/index.html">rocSOLVER</a></td>
<td>3.28.2</td>
<td><a href="https://github.com/ROCm/rocSOLVER"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocSPARSE/en/docs-6.4.3/index.html">rocSPARSE</a></td>
<td>3.4.0</td>
<td><a href="https://github.com/ROCm/rocSPARSE"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocWMMA/en/docs-6.4.3/index.html">rocWMMA</a></td>
<td>1.7.0</td>
<td><a href="https://github.com/ROCm/rocWMMA"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/Tensile/en/docs-6.4.3/src/index.html">Tensile</a></td>
<td>4.43.0</td>
<td><a href="https://github.com/ROCm/Tensile"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
</tbody>
<tbody class="rocm-components-libs rocm-components-primitives tbody-reverse-zebra">
<tr>
<th rowspan="4"></th>
<th rowspan="4">Primitives</th>
<td><a href="https://rocm.docs.amd.com/projects/hipCUB/en/docs-6.4.3/index.html">hipCUB</a></td>
<td>3.4.0</td>
<td><a href="https://github.com/ROCm/hipCUB"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/hipTensor/en/docs-6.4.3/index.html">hipTensor</a></td>
<td>1.5.0</td>
<td><a href="https://github.com/ROCm/hipTensor"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocPRIM/en/docs-6.4.3/index.html">rocPRIM</a></td>
<td>3.4.1</td>
<td><a href="https://github.com/ROCm/rocPRIM"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocThrust/en/docs-6.4.3/index.html">rocThrust</a></td>
<td>3.3.0</td>
<td><a href="https://github.com/ROCm/rocThrust"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
</tbody>
<tbody class="rocm-components-tools rocm-components-system tbody-reverse-zebra">
<tr>
<th rowspan="7">Tools</th>
<th rowspan="7">System management</th>
<td><a href="https://rocm.docs.amd.com/projects/amdsmi/en/docs-6.4.3/index.html">AMD SMI</a></td>
<td>25.5.1</td>
<td><a href="https://github.com/ROCm/amdsmi"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rdc/en/docs-6.4.3/index.html">ROCm Data Center Tool</a></td>
<td>0.3.0</td>
<td><a href="https://github.com/ROCm/rdc"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocminfo/en/docs-6.4.3/index.html">rocminfo</a></td>
<td>1.0.0</td>
<td><a href="https://github.com/ROCm/rocminfo"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocm_smi_lib/en/docs-6.4.3/index.html">ROCm SMI</a></td>
<td>7.5.0 ⇒ <a href="#rocm-smi-7-7-0">7.7.0</a></td>
<td><a href="https://github.com/ROCm/rocm_smi_lib"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/ROCmValidationSuite/en/docs-6.4.3/index.html">ROCm Validation Suite</a></td>
<td>1.1.0</td>
<td><a href="https://github.com/ROCm/ROCmValidationSuite"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
</tbody>
<tbody class="rocm-components-tools rocm-components-perf">
<tr>
<th rowspan="6"></th>
<th rowspan="6">Performance</th>
<td><a href="https://rocm.docs.amd.com/projects/rocm_bandwidth_test/en/docs-6.4.3/index.html">ROCm Bandwidth
                        Test</a></td>
<td>1.4.0</td>
<td><a href="https://github.com/ROCm/rocm_bandwidth_test/"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocprofiler-compute/en/docs-6.4.3/index.html">ROCm Compute Profiler</a></td>
<td>3.1.1</td>
<td><a href="https://github.com/ROCm/rocprofiler-compute"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocprofiler-systems/en/docs-6.4.3/index.html">ROCm Systems Profiler</a></td>
<td>1.0.2</td>
<td><a href="https://github.com/ROCm/rocprofiler-systems"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocprofiler/en/docs-6.4.3/index.html">ROCProfiler</a></td>
<td>2.0.0</td>
<td><a href="https://github.com/ROCm/ROCProfiler/"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocprofiler-sdk/en/docs-6.4.3/index.html">ROCprofiler-SDK</a></td>
<td>0.6.0</td>
<td><a href="https://github.com/ROCm/rocprofiler-sdk/"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/roctracer/en/docs-6.4.3/index.html">ROCTracer</a></td>
<td>4.1.0</td>
<td><a href="https://github.com/ROCm/ROCTracer/"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
</tbody>
<tbody class="rocm-components-tools rocm-components-dev">
<tr>
<th rowspan="5"></th>
<th rowspan="5">Development</th>
<td><a href="https://rocm.docs.amd.com/projects/HIPIFY/en/docs-6.4.3/index.html">HIPIFY</a></td>
<td>19.0.0</td>
<td><a href="https://github.com/ROCm/HIPIFY/"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/ROCdbgapi/en/docs-6.4.3/index.html">ROCdbgapi</a></td>
<td>0.77.2</td>
<td><a href="https://github.com/ROCm/ROCdbgapi/"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/ROCmCMakeBuildTools/en/docs-6.4.3/index.html">ROCm CMake</a></td>
<td>0.14.0</td>
<td><a href="https://github.com/ROCm/rocm-cmake/"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/ROCgdb/en/docs-6.4.3/index.html">ROCm Debugger (ROCgdb)</a>
</td>
<td>15.2</td>
<td><a href="https://github.com/ROCm/ROCgdb/"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/rocr_debug_agent/en/docs-6.4.3/index.html">ROCr Debug Agent</a>
</td>
<td>2.0.4</td>
<td><a href="https://github.com/ROCm/rocr_debug_agent/"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
</tbody>
<tbody class="rocm-components-compilers tbody-reverse-zebra">
<tr>
<th colspan="2" rowspan="2">Compilers</th>
<td><a href="https://rocm.docs.amd.com/projects/HIPCC/en/docs-6.4.3/index.html">HIPCC</a></td>
<td>1.1.1</td>
<td><a href="https://github.com/ROCm/llvm-project/tree/amd-staging/amd/hipcc"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/llvm-project/en/docs-6.4.3/index.html">llvm-project</a></td>
<td>19.0.0</td>
<td><a href="https://github.com/ROCm/llvm-project/"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
</tbody>
<tbody class="rocm-components-runtimes tbody-reverse-zebra">
<tr>
<th colspan="2" rowspan="2">Runtimes</th>
<td><a href="https://rocm.docs.amd.com/projects/HIP/en/docs-6.4.3/index.html">HIP</a></td>
<td>6.4.3</td>
<td><a href="https://github.com/ROCm/HIP/"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
<tr>
<td><a href="https://rocm.docs.amd.com/projects/ROCR-Runtime/en/docs-6.4.3/index.html">ROCr Runtime</a></td>
<td>1.15.0</td>
<td><a href="https://github.com/ROCm/ROCR-Runtime/"><i class="fab fa-github fa-lg"></i></a></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="detailed-component-changes">
<h3>Detailed component changes<a class="headerlink" href="#detailed-component-changes" title="Link to this heading">#</a></h3>
<p>The following sections describe key changes to ROCm components.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For a historical overview of ROCm component updates, see the <a class="reference internal" href="#document-release/changelog"><span class="doc">ROCm consolidated changelog</span></a>.</p>
</div>
<section id="rocm-smi-7-7-0">
<h4><strong>ROCm SMI</strong> (7.7.0)<a class="headerlink" href="#rocm-smi-7-7-0" title="Link to this heading">#</a></h4>
<section id="added">
<h5>Added<a class="headerlink" href="#added" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p>Support for getting the GPU Board voltage.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the full <a class="reference external" href="https://github.com/ROCm/rocm_smi_lib/blob/release/rocm-rel-6.4/CHANGELOG.md">ROCm SMI changelog</a> for details, examples, and in-depth descriptions.</p>
</div>
</section>
</section>
</section>
<section id="rocm-known-issues">
<h3>ROCm known issues<a class="headerlink" href="#rocm-known-issues" title="Link to this heading">#</a></h3>
<p>ROCm known issues are noted on <span class="fab fa-github"></span> <a class="reference external" href="https://github.com/ROCm/ROCm/labels/Verified%20Issue">GitHub</a>. For known
issues related to individual components, review the <a class="reference internal" href="#detailed-component-changes">Detailed component changes</a>.</p>
</section>
<section id="rocm-upcoming-changes">
<h3>ROCm upcoming changes<a class="headerlink" href="#rocm-upcoming-changes" title="Link to this heading">#</a></h3>
<p>The following changes to the ROCm software stack are anticipated for future releases.</p>
<section id="amd-smi-migration-to-amdgpu-driver-repository">
<h4>AMD SMI migration to AMDGPU driver repository<a class="headerlink" href="#amd-smi-migration-to-amdgpu-driver-repository" title="Link to this heading">#</a></h4>
<p>In a future release, <a class="reference external" href="https://github.com/ROCm/amdsmi">AMD SMI</a> will be relocated from the ROCm organization repository to a new AMDTools repository to better align with its system-level functionality. <code class="docutils literal notranslate"><span class="pre">amd-smi-lib</span></code> will no longer be included in the <code class="docutils literal notranslate"><span class="pre">rocm-developer-tools</span></code> meta-package included with your standard ROCm installation. Instead, it will be packaged with the AMDGPU driver installation.</p>
</section>
<section id="rocm-smi-deprecation">
<h4>ROCm SMI deprecation<a class="headerlink" href="#rocm-smi-deprecation" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://github.com/ROCm/rocm_smi_lib">ROCm SMI</a> will be phased out in an
upcoming ROCm release and will enter maintenance mode. After this transition,
only critical bug fixes will be addressed and no further feature development
will take place.</p>
<p>It’s strongly recommended to transition your projects to <a class="reference external" href="https://github.com/ROCm/amdsmi">AMD
SMI</a>, the successor to ROCm SMI. AMD SMI
includes all the features of the ROCm SMI and will continue to receive regular
updates, new functionality, and ongoing support. For more information on AMD
SMI, see the <a class="reference external" href="https://rocm.docs.amd.com/projects/amdsmi/en/latest/">AMD SMI documentation</a>.</p>
</section>
<section id="roctracer-rocprofiler-rocprof-and-rocprofv2-deprecation">
<h4>ROCTracer, ROCProfiler, rocprof, and rocprofv2 deprecation<a class="headerlink" href="#roctracer-rocprofiler-rocprof-and-rocprofv2-deprecation" title="Link to this heading">#</a></h4>
<p>Development and support for ROCTracer, ROCProfiler, <code class="docutils literal notranslate"><span class="pre">rocprof</span></code>, and <code class="docutils literal notranslate"><span class="pre">rocprofv2</span></code> are being phased out in favor of ROCprofiler-SDK in upcoming ROCm releases. Starting with ROCm 6.4, only critical defect fixes will be addressed for older versions of the profiling tools and libraries. All users are encouraged to upgrade to the latest version of the ROCprofiler-SDK library and the (<code class="docutils literal notranslate"><span class="pre">rocprofv3</span></code>) tool to ensure continued support and access to new features. ROCprofiler-SDK is still in beta today and will be production-ready in a future ROCm release.</p>
<p>It’s anticipated that ROCTracer, ROCProfiler, <code class="docutils literal notranslate"><span class="pre">rocprof</span></code>, and <code class="docutils literal notranslate"><span class="pre">rocprofv2</span></code> will reach end-of-life by future releases, aligning with Q1 of 2026.</p>
</section>
<section id="amdgpu-wavefront-size-compiler-macro-deprecation">
<h4>AMDGPU wavefront size compiler macro deprecation<a class="headerlink" href="#amdgpu-wavefront-size-compiler-macro-deprecation" title="Link to this heading">#</a></h4>
<p>Access to the wavefront size as a compile-time constant via the <code class="docutils literal notranslate"><span class="pre">__AMDGCN_WAVEFRONT_SIZE</span></code>
and <code class="docutils literal notranslate"><span class="pre">__AMDGCN_WAVEFRONT_SIZE__</span></code> macros or the <code class="docutils literal notranslate"><span class="pre">constexpr</span> <span class="pre">warpSize</span></code> variable is deprecated
and will be disabled in a future release.</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">__AMDGCN_WAVEFRONT_SIZE__</span></code> macro and <code class="docutils literal notranslate"><span class="pre">__AMDGCN_WAVEFRONT_SIZE</span></code> alias will be removed in an upcoming release.
It is recommended to remove any use of this macro. For more information, see
<a class="reference external" href="https://rocm.docs.amd.com/projects/llvm-project/en/docs-6.4.3/LLVM/clang/html/AMDGPUSupport.html">AMDGPU support</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">warpSize</span></code> will only be available as a non-<code class="docutils literal notranslate"><span class="pre">constexpr</span></code> variable. Where required,
the wavefront size should be queried via the <code class="docutils literal notranslate"><span class="pre">warpSize</span></code> variable in device code,
or via <code class="docutils literal notranslate"><span class="pre">hipGetDeviceProperties</span></code> in host code. Neither of these will result in a compile-time constant. For more information, see <a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/docs-6.4.3/how-to/hip_cpp_language_extensions.html#warpsize">warpSize</a>.</p></li>
<li><p>For cases where compile-time evaluation of the wavefront size cannot be avoided,
uses of <code class="docutils literal notranslate"><span class="pre">__AMDGCN_WAVEFRONT_SIZE</span></code>, <code class="docutils literal notranslate"><span class="pre">__AMDGCN_WAVEFRONT_SIZE__</span></code>, or <code class="docutils literal notranslate"><span class="pre">warpSize</span></code>
can be replaced with a user-defined macro or <code class="docutils literal notranslate"><span class="pre">constexpr</span></code> variable with the wavefront
size(s) for the target hardware. For example:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>   <span class="c1">#if defined(__GFX9__)</span>
   <span class="c1">#define MY_MACRO_FOR_WAVEFRONT_SIZE 64</span>
   <span class="c1">#else</span>
   <span class="c1">#define MY_MACRO_FOR_WAVEFRONT_SIZE 32</span>
   <span class="c1">#endif</span>
</pre></div>
</div>
</section>
<section id="hipcc-perl-scripts-deprecation">
<h4>HIPCC Perl scripts deprecation<a class="headerlink" href="#hipcc-perl-scripts-deprecation" title="Link to this heading">#</a></h4>
<p>The HIPCC Perl scripts (<code class="docutils literal notranslate"><span class="pre">hipcc.pl</span></code> and <code class="docutils literal notranslate"><span class="pre">hipconfig.pl</span></code>) will be removed in an upcoming release.</p>
</section>
<section id="changes-to-rocm-object-tooling">
<h4>Changes to ROCm Object Tooling<a class="headerlink" href="#changes-to-rocm-object-tooling" title="Link to this heading">#</a></h4>
<p>ROCm Object Tooling tools <code class="docutils literal notranslate"><span class="pre">roc-obj-ls</span></code>, <code class="docutils literal notranslate"><span class="pre">roc-obj-extract</span></code>, and <code class="docutils literal notranslate"><span class="pre">roc-obj</span></code> are
deprecated in ROCm 6.4, and will be removed in a future release. Functionality
has been added to the <code class="docutils literal notranslate"><span class="pre">llvm-objdump</span> <span class="pre">--offloading</span></code> tool option to extract all
clang-offload-bundles into individual code objects found within the objects
or executables passed as input.  The <code class="docutils literal notranslate"><span class="pre">llvm-objdump</span> <span class="pre">--offloading</span></code> tool option also
supports the <code class="docutils literal notranslate"><span class="pre">--arch-name</span></code> option, and only extracts code objects found with
the specified target architecture. See <a class="reference external" href="https://llvm.org/docs/CommandGuide/llvm-objdump.html">llvm-objdump</a>
for more information.</p>
</section>
<section id="hip-runtime-api-changes">
<h4>HIP runtime API changes<a class="headerlink" href="#hip-runtime-api-changes" title="Link to this heading">#</a></h4>
<p>There are a number of upcoming changes planned for HIP runtime API in an upcoming major release
that are not backward compatible with prior releases. Most of these changes increase
alignment between HIP and CUDA APIs or behavior. Some of the upcoming changes are to
clean up header files, remove namespace collision, and have a clear separation between
<code class="docutils literal notranslate"><span class="pre">hipRTC</span></code> and HIP runtime. For more information, see <a class="reference external" href="https://rocm.blogs.amd.com/ecosystems-and-partners/transition-to-hip-7.0-blog/README.html">HIP 7.0 Is Coming: What You Need to Know to Stay Ahead</a>.</p>
</section>
</section>
</section>
<span id="document-compatibility/compatibility-matrix"></span><section id="compatibility-matrix">
<h2>Compatibility matrix<a class="headerlink" href="#compatibility-matrix" title="Link to this heading">#</a></h2>
<p>Use this matrix to view the ROCm compatibility and system requirements across successive major and minor releases.</p>
<p>You can also refer to the <a class="reference internal" href="#past-rocm-compatibility-matrix"><span class="std std-ref">past versions of ROCm compatibility matrix</span></a>.</p>
<p>Accelerators and GPUs listed in the following table support compute workloads (no display
information or graphics). If you’re using ROCm with AMD Radeon or Radeon Pro GPUs for graphics
workloads, see the <a class="reference external" href="https://rocm.docs.amd.com/projects/radeon/en/latest/docs/compatibility.html">Use ROCm on Radeon GPU documentation</a> to verify
compatibility and system requirements.</p>
<div class="format-big-table docutils container">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head stub"><p>ROCm Version</p></th>
<th class="head"><p>6.4.3</p></th>
<th class="head"><p>6.4.2</p></th>
<th class="head"><p>6.3.0</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><th class="stub"><p><a class="reference internal" href="#os-kernel-versions"><span class="std std-ref">Operating systems &amp; kernels</span></a></p></th>
<td><p>Ubuntu 24.04.2</p></td>
<td><p>Ubuntu 24.04.2</p></td>
<td><p>Ubuntu 24.04.2</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p>Ubuntu 22.04.5</p></td>
<td><p>Ubuntu 22.04.5</p></td>
<td><p>Ubuntu 22.04.5</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p>RHEL 9.6, 9.4</p></td>
<td><p>RHEL 9.6, 9.4</p></td>
<td><p>RHEL 9.5, 9.4</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p>RHEL 8.10</p></td>
<td><p>RHEL 8.10</p></td>
<td><p>RHEL 8.10</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p>SLES 15 SP7, SP6</p></td>
<td><p>SLES 15 SP7, SP6</p></td>
<td><p>SLES 15 SP6, SP5</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p>Oracle Linux 9, 8 <a class="footnote-reference brackets" href="#mi300x" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p></td>
<td><p>Oracle Linux 9, 8 <a class="footnote-reference brackets" href="#mi300x" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p></td>
<td><p>Oracle Linux 8.10 <a class="footnote-reference brackets" href="#mi300x" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p>Debian 12 <a class="footnote-reference brackets" href="#single-node" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a></p></td>
<td><p>Debian 12 <a class="footnote-reference brackets" href="#single-node" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a></p></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p>Azure Linux 3.0 <a class="footnote-reference brackets" href="#mi300x" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p></td>
<td><p>Azure Linux 3.0 <a class="footnote-reference brackets" href="#mi300x" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td id="architecture-support-compatibility-matrix"></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">Architecture</span></a></p></th>
<td><p>CDNA3</p></td>
<td><p>CDNA3</p></td>
<td><p>CDNA3</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p>CDNA2</p></td>
<td><p>CDNA2</p></td>
<td><p>CDNA2</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p>CDNA</p></td>
<td><p>CDNA</p></td>
<td><p>CDNA</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p>RDNA4</p></td>
<td><p>RDNA4</p></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p>RDNA3</p></td>
<td><p>RDNA3</p></td>
<td><p>RDNA3</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p>RDNA2</p></td>
<td><p>RDNA2</p></td>
<td><p>RDNA2</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td></td>
<td id="gpu-support-compatibility-matrix"></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">GPU / LLVM target</span></a></p></th>
<td><p>gfx1201 <a class="footnote-reference brackets" href="#rdna-os" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a></p></td>
<td><p>gfx1201 <a class="footnote-reference brackets" href="#rdna-os" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a></p></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p>gfx1200 <a class="footnote-reference brackets" href="#rdna-os" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a></p></td>
<td><p>gfx1200 <a class="footnote-reference brackets" href="#rdna-os" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a></p></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p>gfx1101 <a class="footnote-reference brackets" href="#rdna-os" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#xt-os" id="id13" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a></p></td>
<td><p>gfx1101 <a class="footnote-reference brackets" href="#rdna-os" id="id14" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#xt-os" id="id15" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a></p></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p>gfx1100</p></td>
<td><p>gfx1100</p></td>
<td><p>gfx1100</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p>gfx1030</p></td>
<td><p>gfx1030</p></td>
<td><p>gfx1030</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p>gfx942</p></td>
<td><p>gfx942</p></td>
<td><p>gfx942</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p>gfx90a</p></td>
<td><p>gfx90a</p></td>
<td><p>gfx90a</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p>gfx908</p></td>
<td><p>gfx908</p></td>
<td><p>gfx908</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p>FRAMEWORK SUPPORT</p></th>
<td></td>
<td id="framework-support-compatibility-matrix"></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference internal" href="#document-compatibility/ml-compatibility/pytorch-compatibility"><span class="doc">PyTorch</span></a></p></th>
<td><p>2.6, 2.5, 2.4, 2.3</p></td>
<td><p>2.6, 2.5, 2.4, 2.3</p></td>
<td><p>2.4, 2.3, 2.2, 2.1, 2.0, 1.13</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference internal" href="#document-compatibility/ml-compatibility/tensorflow-compatibility"><span class="doc">TensorFlow</span></a></p></th>
<td><p>2.18.1, 2.17.1, 2.16.2</p></td>
<td><p>2.18.1, 2.17.1, 2.16.2</p></td>
<td><p>2.17.0, 2.16.2, 2.15.1</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference internal" href="#document-compatibility/ml-compatibility/jax-compatibility"><span class="doc">JAX</span></a></p></th>
<td><p>0.4.35</p></td>
<td><p>0.4.35</p></td>
<td><p>0.4.31</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference internal" href="#document-compatibility/ml-compatibility/stanford-megatron-lm-compatibility"><span class="doc">Stanford Megatron-LM</span></a></p></th>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
<td><p>85f95ae</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference internal" href="#document-compatibility/ml-compatibility/megablocks-compatibility"><span class="doc">Megablocks</span></a></p></th>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
<td><p>0.7.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://onnxruntime.ai/docs/build/eps.html#amd-migraphx">ONNX Runtime</a></p></th>
<td><p>1.2</p></td>
<td><p>1.2</p></td>
<td><p>1.17.3</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p>THIRD PARTY COMMS</p></th>
<td></td>
<td id="thirdpartycomms-support-compatibility-matrix"></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://github.com/ROCm/ucc">UCC</a></p></th>
<td><p>&gt;=1.3.0</p></td>
<td><p>&gt;=1.3.0</p></td>
<td><p>&gt;=1.3.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://github.com/ROCm/ucx">UCX</a></p></th>
<td><p>&gt;=1.15.0</p></td>
<td><p>&gt;=1.15.0</p></td>
<td><p>&gt;=1.15.0</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p>THIRD PARTY ALGORITHM</p></th>
<td></td>
<td id="thirdpartyalgorithm-support-compatibility-matrix"></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p>Thrust</p></th>
<td><p>2.5.0</p></td>
<td><p>2.5.0</p></td>
<td><p>2.3.2</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>CUB</p></th>
<td><p>2.5.0</p></td>
<td><p>2.5.0</p></td>
<td><p>2.3.2</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p>KMD &amp; USER SPACE <a class="footnote-reference brackets" href="#kfd-support" id="id16" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a></p></th>
<td></td>
<td id="kfd-userspace-support-compatibility-matrix"></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/user-kernel-space-compat-matrix.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">KMD versions</span></a></p></th>
<td><p>6.4.x, 6.3.x, 6.2.x, 6.1.x</p></td>
<td><p>6.4.x, 6.3.x, 6.2.x, 6.1.x</p></td>
<td><p>6.4.x, 6.3.x, 6.2.x, 6.1.x</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p>ML &amp; COMPUTER VISION</p></th>
<td></td>
<td id="mllibs-support-compatibility-matrix"></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/composable_kernel/en/latest/index.html" title="(in Composable Kernel Documentation v1.1.0)"><span class="xref std std-doc">Composable Kernel</span></a></p></th>
<td><p>1.1.0</p></td>
<td><p>1.1.0</p></td>
<td><p>1.1.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/AMDMIGraphX/en/latest/index.html" title="(in MIGraphX v2.12.0)"><span class="xref std std-doc">MIGraphX</span></a></p></th>
<td><p>2.12.0</p></td>
<td><p>2.12.0</p></td>
<td><p>2.11.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/MIOpen/en/latest/index.html" title="(in MIOpen Documentation v3.4.0)"><span class="xref std std-doc">MIOpen</span></a></p></th>
<td><p>3.4.0</p></td>
<td><p>3.4.0</p></td>
<td><p>3.3.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/MIVisionX/en/latest/index.html" title="(in MIVisionX Documentation v3.2.0)"><span class="xref std std-doc">MIVisionX</span></a></p></th>
<td><p>3.2.0</p></td>
<td><p>3.2.0</p></td>
<td><p>3.1.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocAL/en/latest/index.html" title="(in rocAL Documentation v2.2.0)"><span class="xref std std-doc">rocAL</span></a></p></th>
<td><p>2.2.0</p></td>
<td><p>2.2.0</p></td>
<td><p>2.1.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocDecode/en/latest/index.html" title="(in rocDecode documentation v0.10.0)"><span class="xref std std-doc">rocDecode</span></a></p></th>
<td><p>0.10.0</p></td>
<td><p>0.10.0</p></td>
<td><p>0.8.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocJPEG/en/latest/index.html" title="(in rocJPEG Documentation v0.8.0)"><span class="xref std std-doc">rocJPEG</span></a></p></th>
<td><p>0.8.0</p></td>
<td><p>0.8.0</p></td>
<td><p>0.6.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocPyDecode/en/latest/index.html" title="(in rocPyDecode v0.3.1)"><span class="xref std std-doc">rocPyDecode</span></a></p></th>
<td><p>0.3.1</p></td>
<td><p>0.3.1</p></td>
<td><p>0.2.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rpp/en/latest/index.html" title="(in RPP documentation v1.9.10)"><span class="xref std std-doc">RPP</span></a></p></th>
<td><p>1.9.10</p></td>
<td><p>1.9.10</p></td>
<td><p>1.9.1</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p>COMMUNICATION</p></th>
<td></td>
<td id="commlibs-support-compatibility-matrix"></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rccl/en/latest/index.html" title="(in RCCL Documentation v2.22.3)"><span class="xref std std-doc">RCCL</span></a></p></th>
<td><p>2.22.3</p></td>
<td><p>2.22.3</p></td>
<td><p>2.21.5</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocSHMEM/en/latest/index.html" title="(in rocSHMEM v2.0.1)"><span class="xref std std-doc">rocSHMEM</span></a></p></th>
<td><p>2.0.1</p></td>
<td><p>2.0.1</p></td>
<td><p>N/A</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p>MATH LIBS</p></th>
<td></td>
<td id="mathlibs-support-compatibility-matrix"></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://github.com/ROCm/half">half</a></p></th>
<td><p>1.12.0</p></td>
<td><p>1.12.0</p></td>
<td><p>1.12.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipBLAS/en/latest/index.html" title="(in hipBLAS Documentation v2.4.0)"><span class="xref std std-doc">hipBLAS</span></a></p></th>
<td><p>2.4.0</p></td>
<td><p>2.4.0</p></td>
<td><p>2.3.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipBLASLt/en/latest/index.html" title="(in hipBLASLt Documentation v0.12.1)"><span class="xref std std-doc">hipBLASLt</span></a></p></th>
<td><p>0.12.1</p></td>
<td><p>0.12.1</p></td>
<td><p>0.10.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipFFT/en/latest/index.html" title="(in hipFFT Documentation v1.0.18)"><span class="xref std std-doc">hipFFT</span></a></p></th>
<td><p>1.0.18</p></td>
<td><p>1.0.18</p></td>
<td><p>1.0.17</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipfort/en/latest/index.html" title="(in hipfort Documentation v0.6.0)"><span class="xref std std-doc">hipfort</span></a></p></th>
<td><p>0.6.0</p></td>
<td><p>0.6.0</p></td>
<td><p>0.5.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipRAND/en/latest/index.html" title="(in hipRAND Documentation v2.12.0)"><span class="xref std std-doc">hipRAND</span></a></p></th>
<td><p>2.12.0</p></td>
<td><p>2.12.0</p></td>
<td><p>2.11.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipSOLVER/en/latest/index.html" title="(in hipSOLVER Documentation v2.4.0)"><span class="xref std std-doc">hipSOLVER</span></a></p></th>
<td><p>2.4.0</p></td>
<td><p>2.4.0</p></td>
<td><p>2.3.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipSPARSE/en/latest/index.html" title="(in hipSPARSE Documentation v3.2.0)"><span class="xref std std-doc">hipSPARSE</span></a></p></th>
<td><p>3.2.0</p></td>
<td><p>3.2.0</p></td>
<td><p>3.1.2</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipSPARSELt/en/latest/index.html" title="(in hipSPARSELt Documentation v0.2.3)"><span class="xref std std-doc">hipSPARSELt</span></a></p></th>
<td><p>0.2.3</p></td>
<td><p>0.2.3</p></td>
<td><p>0.2.2</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocALUTION/en/latest/index.html" title="(in rocALUTION Documentation v3.2.3)"><span class="xref std std-doc">rocALUTION</span></a></p></th>
<td><p>3.2.3</p></td>
<td><p>3.2.3</p></td>
<td><p>3.2.1</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocBLAS/en/latest/index.html" title="(in rocBLAS Documentation v4.4.1)"><span class="xref std std-doc">rocBLAS</span></a></p></th>
<td><p>4.4.1</p></td>
<td><p>4.4.1</p></td>
<td><p>4.3.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocFFT/en/latest/index.html" title="(in rocFFT Documentation v1.0.32)"><span class="xref std std-doc">rocFFT</span></a></p></th>
<td><p>1.0.32</p></td>
<td><p>1.0.32</p></td>
<td><p>1.0.31</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocRAND/en/latest/index.html" title="(in rocRAND Documentation v3.3.0)"><span class="xref std std-doc">rocRAND</span></a></p></th>
<td><p>3.3.0</p></td>
<td><p>3.3.0</p></td>
<td><p>3.2.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocSOLVER/en/latest/index.html" title="(in rocSOLVER Documentation v3.28.2)"><span class="xref std std-doc">rocSOLVER</span></a></p></th>
<td><p>3.28.2</p></td>
<td><p>3.28.2</p></td>
<td><p>3.27.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocSPARSE/en/latest/index.html" title="(in rocSPARSE Documentation v3.4.0)"><span class="xref std std-doc">rocSPARSE</span></a></p></th>
<td><p>3.4.0</p></td>
<td><p>3.4.0</p></td>
<td><p>3.3.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocWMMA/en/latest/index.html" title="(in rocWMMA Documentation v1.7.0)"><span class="xref std std-doc">rocWMMA</span></a></p></th>
<td><p>1.7.0</p></td>
<td><p>1.7.0</p></td>
<td><p>1.6.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/Tensile/en/latest/src/index.html" title="(in Tensile Documentation v4.43.0)"><span class="xref std std-doc">Tensile</span></a></p></th>
<td><p>4.43.0</p></td>
<td><p>4.43.0</p></td>
<td><p>4.42.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p>PRIMITIVES</p></th>
<td></td>
<td id="primitivelibs-support-compatibility-matrix"></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipCUB/en/latest/index.html" title="(in hipCUB Documentation v3.4.0)"><span class="xref std std-doc">hipCUB</span></a></p></th>
<td><p>3.4.0</p></td>
<td><p>3.4.0</p></td>
<td><p>3.3.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipTensor/en/latest/index.html" title="(in hipTensor Documentation v1.5.0)"><span class="xref std std-doc">hipTensor</span></a></p></th>
<td><p>1.5.0</p></td>
<td><p>1.5.0</p></td>
<td><p>1.4.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocPRIM/en/latest/index.html" title="(in rocPRIM Documentation v3.4.1)"><span class="xref std std-doc">rocPRIM</span></a></p></th>
<td><p>3.4.1</p></td>
<td><p>3.4.1</p></td>
<td><p>3.3.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocThrust/en/latest/index.html" title="(in rocThrust Documentation v3.3.0)"><span class="xref std std-doc">rocThrust</span></a></p></th>
<td><p>3.3.0</p></td>
<td><p>3.3.0</p></td>
<td><p>3.3.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p>SUPPORT LIBS</p></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://github.com/ROCm/hipother">hipother</a></p></th>
<td><p>6.4.43483</p></td>
<td><p>6.4.43483</p></td>
<td><p>6.3.42131</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://github.com/ROCm/rocm-core">rocm-core</a></p></th>
<td><p>6.4.3</p></td>
<td><p>6.4.2</p></td>
<td><p>6.3.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://github.com/ROCm/ROCT-Thunk-Interface">ROCT-Thunk-Interface</a></p></th>
<td><p>N/A <a class="footnote-reference brackets" href="#roct-rocr" id="id17" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a></p></td>
<td><p>N/A <a class="footnote-reference brackets" href="#roct-rocr" id="id18" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a></p></td>
<td><p>N/A <a class="footnote-reference brackets" href="#roct-rocr" id="id19" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p>SYSTEM MGMT TOOLS</p></th>
<td></td>
<td id="tools-support-compatibility-matrix"></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/amdsmi/en/latest/index.html" title="(in AMD SMI v25.5.1)"><span class="xref std std-doc">AMD SMI</span></a></p></th>
<td><p>25.5.1</p></td>
<td><p>25.5.1</p></td>
<td><p>24.7.1</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rdc/en/latest/index.html" title="(in ROCm Data Center Documentation)"><span class="xref std std-doc">ROCm Data Center Tool</span></a></p></th>
<td><p>0.3.0</p></td>
<td><p>0.3.0</p></td>
<td><p>0.3.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocminfo/en/latest/index.html" title="(in rocminfo v1.0.0)"><span class="xref std std-doc">rocminfo</span></a></p></th>
<td><p>1.0.0</p></td>
<td><p>1.0.0</p></td>
<td><p>1.0.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocm_smi_lib/en/latest/index.html" title="(in ROCm SMI LIB Documentation v7.7.0)"><span class="xref std std-doc">ROCm SMI</span></a></p></th>
<td><p>7.7.0</p></td>
<td><p>7.5.0</p></td>
<td><p>7.4.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCmValidationSuite/en/latest/index.html" title="(in RVS Documentation v1.1.0)"><span class="xref std std-doc">ROCm Validation Suite</span></a></p></th>
<td><p>1.1.0</p></td>
<td><p>1.1.0</p></td>
<td><p>1.1.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p>PERFORMANCE TOOLS</p></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocm_bandwidth_test/en/latest/index.html" title="(in rocm_bandwidth_test)"><span class="xref std std-doc">ROCm Bandwidth Test</span></a></p></th>
<td><p>1.4.0</p></td>
<td><p>1.4.0</p></td>
<td><p>1.4.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler-compute/en/latest/index.html" title="(in ROCm Compute Profiler v3.1.1)"><span class="xref std std-doc">ROCm Compute Profiler</span></a></p></th>
<td><p>3.1.1</p></td>
<td><p>3.1.1</p></td>
<td><p>3.0.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler-systems/en/latest/index.html" title="(in rocprofiler-systems v1.0.2)"><span class="xref std std-doc">ROCm Systems Profiler</span></a></p></th>
<td><p>1.0.2</p></td>
<td><p>1.0.2</p></td>
<td><p>0.1.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler/en/latest/index.html" title="(in rocprofiler Documentation v2.0.0)"><span class="xref std std-doc">ROCProfiler</span></a></p></th>
<td><p>2.0.60403</p></td>
<td><p>2.0.60402</p></td>
<td><p>2.0.60300</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler-sdk/en/latest/index.html" title="(in Rocprofiler SDK v0.6.0)"><span class="xref std std-doc">ROCprofiler-SDK</span></a></p></th>
<td><p>0.6.0</p></td>
<td><p>0.6.0</p></td>
<td><p>0.5.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/roctracer/en/latest/index.html" title="(in roctracer Documentation v4.1.0)"><span class="xref std std-doc">ROCTracer</span></a></p></th>
<td><p>4.1.60403</p></td>
<td><p>4.1.60402</p></td>
<td><p>4.1.60300</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p>DEVELOPMENT TOOLS</p></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/HIPIFY/en/latest/index.html" title="(in HIPIFY Documentation)"><span class="xref std std-doc">HIPIFY</span></a></p></th>
<td><p>19.0.0</p></td>
<td><p>19.0.0</p></td>
<td><p>18.0.0.24455</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCmCMakeBuildTools/en/latest/index.html" title="(in ROCm CMake Build Tools v0.14.0)"><span class="xref std std-doc">ROCm CMake</span></a></p></th>
<td><p>0.14.0</p></td>
<td><p>0.14.0</p></td>
<td><p>0.14.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCdbgapi/en/latest/index.html" title="(in ROCdbgapi Documentation v0.77.2)"><span class="xref std std-doc">ROCdbgapi</span></a></p></th>
<td><p>0.77.2</p></td>
<td><p>0.77.2</p></td>
<td><p>0.77.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCgdb/en/latest/index.html" title="(in ROCgdb Documentation v15.2)"><span class="xref std std-doc">ROCm Debugger (ROCgdb)</span></a></p></th>
<td><p>15.2.0</p></td>
<td><p>15.2.0</p></td>
<td><p>15.2.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://github.com/ROCm/rocprofiler-register">rocprofiler-register</a></p></th>
<td><p>0.4.0</p></td>
<td><p>0.4.0</p></td>
<td><p>0.4.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocr_debug_agent/en/latest/index.html" title="(in rocr_debug_agent v2.0.4)"><span class="xref std std-doc">ROCr Debug Agent</span></a></p></th>
<td><p>2.0.4</p></td>
<td><p>2.0.4</p></td>
<td><p>2.0.3</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p>COMPILERS</p></th>
<td></td>
<td id="compilers-support-compatibility-matrix"></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://github.com/ROCm/clang-ocl">clang-ocl</a></p></th>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/HIPCC/en/latest/index.html" title="(in HIPCC Documentation v1.1.1)"><span class="xref std std-doc">hipCC</span></a></p></th>
<td><p>1.1.1</p></td>
<td><p>1.1.1</p></td>
<td><p>1.1.1</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://github.com/ROCm/flang">Flang</a></p></th>
<td><p>19.0.0.25224</p></td>
<td><p>19.0.0.25224</p></td>
<td><p>18.0.0.24455</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/index.html" title="(in llvm-project Documentation v19.0.0)"><span class="xref std std-doc">llvm-project</span></a></p></th>
<td><p>19.0.0.25224</p></td>
<td><p>19.0.0.25224</p></td>
<td><p>18.0.0.24491</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://github.com/ROCm/llvm-project/tree/amd-staging/openmp">OpenMP</a></p></th>
<td><p>19.0.0.25224</p></td>
<td><p>19.0.0.25224</p></td>
<td><p>18.0.0.24491</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p>RUNTIMES</p></th>
<td></td>
<td id="runtime-support-compatibility-matrix"></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/understand/amd_clr.html" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-doc">AMD CLR</span></a></p></th>
<td><p>6.4.43484</p></td>
<td><p>6.4.43484</p></td>
<td><p>6.3.42131</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/index.html" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-doc">HIP</span></a></p></th>
<td><p>6.4.43484</p></td>
<td><p>6.4.43484</p></td>
<td><p>6.3.42131</p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://github.com/ROCm/clr/tree/develop/opencl">OpenCL Runtime</a></p></th>
<td><p>2.0.0</p></td>
<td><p>2.0.0</p></td>
<td><p>2.0.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCR-Runtime/en/latest/index.html" title="(in ROCR Documentation v1.15.0)"><span class="xref std std-doc">ROCr Runtime</span></a></p></th>
<td><p>1.15.0</p></td>
<td><p>1.15.0</p></td>
<td><p>1.14.0</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<p class="rubric">Footnotes</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="mi300x" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a href="#id1" role="doc-backlink">1</a>,<a href="#id2" role="doc-backlink">2</a>,<a href="#id3" role="doc-backlink">3</a>,<a href="#id6" role="doc-backlink">4</a>,<a href="#id7" role="doc-backlink">5</a>)</span>
<p>Oracle Linux and Azure Linux are supported only on AMD Instinct MI300X.</p>
</aside>
<aside class="footnote brackets" id="single-node" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a href="#id4" role="doc-backlink">1</a>,<a href="#id5" role="doc-backlink">2</a>)</span>
<p>Debian 12 is supported only on AMD Instinct MI300X for single-node functionality.</p>
</aside>
<aside class="footnote brackets" id="rdna-os" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a href="#id8" role="doc-backlink">1</a>,<a href="#id9" role="doc-backlink">2</a>,<a href="#id10" role="doc-backlink">3</a>,<a href="#id11" role="doc-backlink">4</a>,<a href="#id12" role="doc-backlink">5</a>,<a href="#id14" role="doc-backlink">6</a>)</span>
<p>Radeon AI PRO R9700, Radeon RX 9070 XT (gfx1201), Radeon RX 9060 XT (gfx1200), Radeon PRO W7700 (gfx1101), and Radeon RX 7800 XT (gfx1101) are supported only on Ubuntu 24.04.2, Ubuntu 22.04.5, RHEL 9.6, and RHEL 9.4.</p>
</aside>
<aside class="footnote brackets" id="xt-os" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a href="#id13" role="doc-backlink">1</a>,<a href="#id15" role="doc-backlink">2</a>)</span>
<p>Radeon RX 7700 XT (gfx1101) is supported only on Ubuntu 24.04.2 and RHEL 9.6.</p>
</aside>
<aside class="footnote brackets" id="kfd-support" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a href="#id16" role="doc-backlink">5</a><span class="fn-bracket">]</span></span>
<p>As of ROCm 6.4.0, forward and backward compatibility between the AMD Kernel-mode GPU Driver (KMD) and its user space software is provided up to a year apart. For earlier ROCm releases, the compatibility is provided for +/- 2 releases. The tested user space versions on this page were accurate as of the time of initial ROCm release. For the most up-to-date information, see the latest version of this information at <a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/user-kernel-space-compat-matrix.html">User and kernel-space support matrix</a>.</p>
</aside>
<aside class="footnote brackets" id="roct-rocr" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a href="#id17" role="doc-backlink">1</a>,<a href="#id18" role="doc-backlink">2</a>,<a href="#id19" role="doc-backlink">3</a>)</span>
<p>Starting from ROCm 6.3.0, the ROCT Thunk Interface is included as part of the ROCr runtime package.</p>
</aside>
</aside>
<section id="operating-systems-kernel-and-glibc-versions">
<span id="os-kernel-versions"></span><h3>Operating systems, kernel and Glibc versions<a class="headerlink" href="#operating-systems-kernel-and-glibc-versions" title="Link to this heading">#</a></h3>
<p>Use this lookup table to confirm which operating system and kernel versions are supported with ROCm.</p>
<div class="pst-scrollable-table-container"><table class="table">
<colgroup>
<col style="width: 36.4%">
<col style="width: 18.2%">
<col style="width: 27.3%"/>
<col style="width: 18.2%"/>
</col></col></colgroup>
<thead>
<tr class="row-odd"><th class="head stub"><p>OS</p></th>
<th class="head"><p>Version</p></th>
<th class="head"><p>Kernel</p></th>
<th class="head"><p>Glibc</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://ubuntu.com/about/release-cycle#ubuntu-kernel-release-cycle">Ubuntu</a></p></th>
<td><p>24.04.2</p></td>
<td><p>6.8 GA, 6.11 HWE</p></td>
<td><p>2.39</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://ubuntu.com/about/release-cycle#ubuntu-kernel-release-cycle">Ubuntu</a></p></th>
<td><p>22.04.5</p></td>
<td><p>5.15 GA, 6.8 HWE</p></td>
<td><p>2.35</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://access.redhat.com/articles/3078#RHEL9">Red Hat Enterprise Linux (RHEL 9)</a></p></th>
<td><p>9.6</p></td>
<td><p>5.14+</p></td>
<td><p>2.34</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p>9.5</p></td>
<td><p>5.14+</p></td>
<td><p>2.34</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p>9.4</p></td>
<td><p>5.14+</p></td>
<td><p>2.34</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p>9.3</p></td>
<td><p>5.14+</p></td>
<td><p>2.34</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://access.redhat.com/articles/3078#RHEL8">Red Hat Enterprise Linux (RHEL 8)</a></p></th>
<td><p>8.10</p></td>
<td><p>4.18.0+</p></td>
<td><p>2.28</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p>8.9</p></td>
<td><p>4.18.0</p></td>
<td><p>2.28</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://www.suse.com/support/kb/doc/?id=000019587#SLE15SP4">SUSE Linux Enterprise Server (SLES)</a></p></th>
<td><p>15 SP7</p></td>
<td><p>6.11.0+</p></td>
<td><p>2.38</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p>15 SP6</p></td>
<td><p>6.5.0+, 6.4.0</p></td>
<td><p>2.38</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p>15 SP5</p></td>
<td><p>5.14.21</p></td>
<td><p>2.31</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference external" href="https://blogs.oracle.com/scoter/post/oracle-linux-and-unbreakable-enterprise-kernel-uek-releases">Oracle Linux</a></p></th>
<td><p>9</p></td>
<td><p>5.15.0 (UEK)</p></td>
<td><p>2.35</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p>8</p></td>
<td><p>5.15.0 (UEK)</p></td>
<td><p>2.28</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://www.debian.org/download">Debian</a></p></th>
<td><p>12</p></td>
<td><p>6.1</p></td>
<td><p>2.36</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference external" href="https://techcommunity.microsoft.com/blog/linuxandopensourceblog/azure-linux-3-0-now-in-preview-on-azure-kubernetes-service-v1-31/4287229">Azure Linux</a></p></th>
<td><p>3.0</p></td>
<td><p>6.6.60</p></td>
<td><p>2.38</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>See <a class="reference external" href="https://access.redhat.com/articles/3078">Red Hat Enterprise Linux Release Dates</a> to learn about the specific kernel versions supported on Red Hat Enterprise Linux (RHEL).</p></li>
<li><p>See <a class="reference external" href="https://www.suse.com/support/kb/doc/?id=000019587">List of SUSE Linux Enterprise Server kernel</a> to learn about the specific kernel version supported on SUSE Linux Enterprise Server (SLES).</p></li>
</ul>
</div>
</section>
<section id="past-versions-of-rocm-compatibility-matrix">
<span id="past-rocm-compatibility-matrix"></span><h3>Past versions of ROCm compatibility matrix<a class="headerlink" href="#past-versions-of-rocm-compatibility-matrix" title="Link to this heading">#</a></h3>
<p>Expand for full historical view of:</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">ROCm 6.0 - Present</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">You can <a class="reference external" href="../downloads/compatibility-matrix-historical-6.0.csv">download the entire .csv</a> for offline reference.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head stub"><p class="sd-card-text">ROCm Version</p></th>
<th class="head"><p class="sd-card-text">6.4.3</p></th>
<th class="head"><p class="sd-card-text">6.4.2</p></th>
<th class="head"><p class="sd-card-text">6.4.1</p></th>
<th class="head"><p class="sd-card-text">6.4.0</p></th>
<th class="head"><p class="sd-card-text">6.3.3</p></th>
<th class="head"><p class="sd-card-text">6.3.2</p></th>
<th class="head"><p class="sd-card-text">6.3.1</p></th>
<th class="head"><p class="sd-card-text">6.3.0</p></th>
<th class="head"><p class="sd-card-text">6.2.4</p></th>
<th class="head"><p class="sd-card-text">6.2.2</p></th>
<th class="head"><p class="sd-card-text">6.2.1</p></th>
<th class="head"><p class="sd-card-text">6.2.0</p></th>
<th class="head"><p class="sd-card-text">6.1.5</p></th>
<th class="head"><p class="sd-card-text">6.1.2</p></th>
<th class="head"><p class="sd-card-text">6.1.1</p></th>
<th class="head"><p class="sd-card-text">6.1.0</p></th>
<th class="head"><p class="sd-card-text">6.0.2</p></th>
<th class="head"><p class="sd-card-text">6.0.0</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference internal" href="#os-kernel-versions"><span class="std std-ref">Operating systems &amp; kernels</span></a></p></th>
<td><p class="sd-card-text">Ubuntu 24.04.2</p></td>
<td><p class="sd-card-text">Ubuntu 24.04.2</p></td>
<td><p class="sd-card-text">Ubuntu 24.04.2</p></td>
<td><p class="sd-card-text">Ubuntu 24.04.2</p></td>
<td><p class="sd-card-text">Ubuntu 24.04.2</p></td>
<td><p class="sd-card-text">Ubuntu 24.04.2</p></td>
<td><p class="sd-card-text">Ubuntu 24.04.2</p></td>
<td><p class="sd-card-text">Ubuntu 24.04.2</p></td>
<td><p class="sd-card-text">Ubuntu 24.04.1, 24.04</p></td>
<td><p class="sd-card-text">Ubuntu 24.04.1, 24.04</p></td>
<td><p class="sd-card-text">Ubuntu 24.04.1, 24.04</p></td>
<td><p class="sd-card-text">Ubuntu 24.04</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p class="sd-card-text">Ubuntu 22.04.5</p></td>
<td><p class="sd-card-text">Ubuntu 22.04.5</p></td>
<td><p class="sd-card-text">Ubuntu 22.04.5</p></td>
<td><p class="sd-card-text">Ubuntu 22.04.5</p></td>
<td><p class="sd-card-text">Ubuntu 22.04.5</p></td>
<td><p class="sd-card-text">Ubuntu 22.04.5</p></td>
<td><p class="sd-card-text">Ubuntu 22.04.5</p></td>
<td><p class="sd-card-text">Ubuntu 22.04.5</p></td>
<td><p class="sd-card-text">Ubuntu 22.04.5, 22.04.4</p></td>
<td><p class="sd-card-text">Ubuntu 22.04.5, 22.04.4</p></td>
<td><p class="sd-card-text">Ubuntu 22.04.5, 22.04.4</p></td>
<td><p class="sd-card-text">Ubuntu 22.04.5, 22.04.4</p></td>
<td><p class="sd-card-text">Ubuntu 22.04.5, 22.04.4, 22.04.3</p></td>
<td><p class="sd-card-text">Ubuntu 22.04.4, 22.04.3</p></td>
<td><p class="sd-card-text">Ubuntu 22.04.4, 22.04.3</p></td>
<td><p class="sd-card-text">Ubuntu 22.04.4, 22.04.3</p></td>
<td><p class="sd-card-text">Ubuntu 22.04.4, 22.04.3, 22.04.2</p></td>
<td><p class="sd-card-text">Ubuntu 22.04.4, 22.04.3, 22.04.2</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><p class="sd-card-text">Ubuntu 20.04.6, 20.04.5</p></td>
<td><p class="sd-card-text">Ubuntu 20.04.6, 20.04.5</p></td>
<td><p class="sd-card-text">Ubuntu 20.04.6, 20.04.5</p></td>
<td><p class="sd-card-text">Ubuntu 20.04.6, 20.04.5</p></td>
<td><p class="sd-card-text">Ubuntu 20.04.6, 20.04.5</p></td>
<td><p class="sd-card-text">Ubuntu 20.04.6, 20.04.5</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p class="sd-card-text">RHEL 9.6, 9.4</p></td>
<td><p class="sd-card-text">RHEL 9.6, 9.4</p></td>
<td><p class="sd-card-text">RHEL 9.6, 9.5, 9.4</p></td>
<td><p class="sd-card-text">RHEL 9.5, 9.4</p></td>
<td><p class="sd-card-text">RHEL 9.5, 9.4</p></td>
<td><p class="sd-card-text">RHEL 9.5, 9.4</p></td>
<td><p class="sd-card-text">RHEL 9.5, 9.4</p></td>
<td><p class="sd-card-text">RHEL 9.5, 9.4</p></td>
<td><p class="sd-card-text">RHEL 9.4, 9.3</p></td>
<td><p class="sd-card-text">RHEL 9.4, 9.3</p></td>
<td><p class="sd-card-text">RHEL 9.4, 9.3</p></td>
<td><p class="sd-card-text">RHEL 9.4, 9.3</p></td>
<td><p class="sd-card-text">RHEL 9.4, 9.3, 9.2</p></td>
<td><p class="sd-card-text">RHEL 9.4, 9.3, 9.2</p></td>
<td><p class="sd-card-text">RHEL 9.4, 9.3, 9.2</p></td>
<td><p class="sd-card-text">RHEL 9.4, 9.3, 9.2</p></td>
<td><p class="sd-card-text">RHEL 9.3, 9.2</p></td>
<td><p class="sd-card-text">RHEL 9.3, 9.2</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p class="sd-card-text">RHEL 8.10</p></td>
<td><p class="sd-card-text">RHEL 8.10</p></td>
<td><p class="sd-card-text">RHEL 8.10</p></td>
<td><p class="sd-card-text">RHEL 8.10</p></td>
<td><p class="sd-card-text">RHEL 8.10</p></td>
<td><p class="sd-card-text">RHEL 8.10</p></td>
<td><p class="sd-card-text">RHEL 8.10</p></td>
<td><p class="sd-card-text">RHEL 8.10</p></td>
<td><p class="sd-card-text">RHEL 8.10, 8.9</p></td>
<td><p class="sd-card-text">RHEL 8.10, 8.9</p></td>
<td><p class="sd-card-text">RHEL 8.10, 8.9</p></td>
<td><p class="sd-card-text">RHEL 8.10, 8.9</p></td>
<td><p class="sd-card-text">RHEL 8.9, 8.8</p></td>
<td><p class="sd-card-text">RHEL 8.9, 8.8</p></td>
<td><p class="sd-card-text">RHEL 8.9, 8.8</p></td>
<td><p class="sd-card-text">RHEL 8.9, 8.8</p></td>
<td><p class="sd-card-text">RHEL 8.9, 8.8</p></td>
<td><p class="sd-card-text">RHEL 8.9, 8.8</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p class="sd-card-text">SLES 15 SP7, SP6</p></td>
<td><p class="sd-card-text">SLES 15 SP7, SP6</p></td>
<td><p class="sd-card-text">SLES 15 SP6</p></td>
<td><p class="sd-card-text">SLES 15 SP6</p></td>
<td><p class="sd-card-text">SLES 15 SP6, SP5</p></td>
<td><p class="sd-card-text">SLES 15 SP6, SP5</p></td>
<td><p class="sd-card-text">SLES 15 SP6, SP5</p></td>
<td><p class="sd-card-text">SLES 15 SP6, SP5</p></td>
<td><p class="sd-card-text">SLES 15 SP6, SP5</p></td>
<td><p class="sd-card-text">SLES 15 SP6, SP5</p></td>
<td><p class="sd-card-text">SLES 15 SP6, SP5</p></td>
<td><p class="sd-card-text">SLES 15 SP6, SP5</p></td>
<td><p class="sd-card-text">SLES 15 SP5, SP4</p></td>
<td><p class="sd-card-text">SLES 15 SP5, SP4</p></td>
<td><p class="sd-card-text">SLES 15 SP5, SP4</p></td>
<td><p class="sd-card-text">SLES 15 SP5, SP4</p></td>
<td><p class="sd-card-text">SLES 15 SP5, SP4</p></td>
<td><p class="sd-card-text">SLES 15 SP5, SP4</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><p class="sd-card-text">CentOS 7.9</p></td>
<td><p class="sd-card-text">CentOS 7.9</p></td>
<td><p class="sd-card-text">CentOS 7.9</p></td>
<td><p class="sd-card-text">CentOS 7.9</p></td>
<td><p class="sd-card-text">CentOS 7.9</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p class="sd-card-text">Oracle Linux 9, 8 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id21" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Oracle Linux 9, 8 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id22" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Oracle Linux 9, 8 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id23" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Oracle Linux 9, 8 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id24" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Oracle Linux 8.10 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id25" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Oracle Linux 8.10 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id26" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Oracle Linux 8.10 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id27" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Oracle Linux 8.10 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id28" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Oracle Linux 8.9 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id29" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Oracle Linux 8.9 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id30" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Oracle Linux 8.9 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id31" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Oracle Linux 8.9 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id32" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Oracle Linux 8.9 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id33" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Oracle Linux 8.9 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id34" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Oracle Linux 8.9 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id35" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p class="sd-card-text">Debian 12 <a class="footnote-reference brackets" href="#single-node-past-60" id="id36" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Debian 12 <a class="footnote-reference brackets" href="#single-node-past-60" id="id37" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Debian 12 <a class="footnote-reference brackets" href="#single-node-past-60" id="id38" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Debian 12 <a class="footnote-reference brackets" href="#single-node-past-60" id="id39" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Debian 12 <a class="footnote-reference brackets" href="#single-node-past-60" id="id40" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Debian 12 <a class="footnote-reference brackets" href="#single-node-past-60" id="id41" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Debian 12 <a class="footnote-reference brackets" href="#single-node-past-60" id="id42" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a></p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p class="sd-card-text">Azure Linux 3.0 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id43" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Azure Linux 3.0 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id44" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Azure Linux 3.0 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id45" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Azure Linux 3.0 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id46" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Azure Linux 3.0 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id47" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">Azure Linux 3.0 <a class="footnote-reference brackets" href="#mi300x-past-60" id="id48" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td id="architecture-support-compatibility-matrix-past-60"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">Architecture</span></a></p></th>
<td><p class="sd-card-text">CDNA3</p></td>
<td><p class="sd-card-text">CDNA3</p></td>
<td><p class="sd-card-text">CDNA3</p></td>
<td><p class="sd-card-text">CDNA3</p></td>
<td><p class="sd-card-text">CDNA3</p></td>
<td><p class="sd-card-text">CDNA3</p></td>
<td><p class="sd-card-text">CDNA3</p></td>
<td><p class="sd-card-text">CDNA3</p></td>
<td><p class="sd-card-text">CDNA3</p></td>
<td><p class="sd-card-text">CDNA3</p></td>
<td><p class="sd-card-text">CDNA3</p></td>
<td><p class="sd-card-text">CDNA3</p></td>
<td><p class="sd-card-text">CDNA3</p></td>
<td><p class="sd-card-text">CDNA3</p></td>
<td><p class="sd-card-text">CDNA3</p></td>
<td><p class="sd-card-text">CDNA3</p></td>
<td><p class="sd-card-text">CDNA3</p></td>
<td><p class="sd-card-text">CDNA3</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p class="sd-card-text">CDNA2</p></td>
<td><p class="sd-card-text">CDNA2</p></td>
<td><p class="sd-card-text">CDNA2</p></td>
<td><p class="sd-card-text">CDNA2</p></td>
<td><p class="sd-card-text">CDNA2</p></td>
<td><p class="sd-card-text">CDNA2</p></td>
<td><p class="sd-card-text">CDNA2</p></td>
<td><p class="sd-card-text">CDNA2</p></td>
<td><p class="sd-card-text">CDNA2</p></td>
<td><p class="sd-card-text">CDNA2</p></td>
<td><p class="sd-card-text">CDNA2</p></td>
<td><p class="sd-card-text">CDNA2</p></td>
<td><p class="sd-card-text">CDNA2</p></td>
<td><p class="sd-card-text">CDNA2</p></td>
<td><p class="sd-card-text">CDNA2</p></td>
<td><p class="sd-card-text">CDNA2</p></td>
<td><p class="sd-card-text">CDNA2</p></td>
<td><p class="sd-card-text">CDNA2</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p class="sd-card-text">CDNA</p></td>
<td><p class="sd-card-text">CDNA</p></td>
<td><p class="sd-card-text">CDNA</p></td>
<td><p class="sd-card-text">CDNA</p></td>
<td><p class="sd-card-text">CDNA</p></td>
<td><p class="sd-card-text">CDNA</p></td>
<td><p class="sd-card-text">CDNA</p></td>
<td><p class="sd-card-text">CDNA</p></td>
<td><p class="sd-card-text">CDNA</p></td>
<td><p class="sd-card-text">CDNA</p></td>
<td><p class="sd-card-text">CDNA</p></td>
<td><p class="sd-card-text">CDNA</p></td>
<td><p class="sd-card-text">CDNA</p></td>
<td><p class="sd-card-text">CDNA</p></td>
<td><p class="sd-card-text">CDNA</p></td>
<td><p class="sd-card-text">CDNA</p></td>
<td><p class="sd-card-text">CDNA</p></td>
<td><p class="sd-card-text">CDNA</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p class="sd-card-text">RDNA4</p></td>
<td><p class="sd-card-text">RDNA4</p></td>
<td><p class="sd-card-text">RDNA4</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p class="sd-card-text">RDNA3</p></td>
<td><p class="sd-card-text">RDNA3</p></td>
<td><p class="sd-card-text">RDNA3</p></td>
<td><p class="sd-card-text">RDNA3</p></td>
<td><p class="sd-card-text">RDNA3</p></td>
<td><p class="sd-card-text">RDNA3</p></td>
<td><p class="sd-card-text">RDNA3</p></td>
<td><p class="sd-card-text">RDNA3</p></td>
<td><p class="sd-card-text">RDNA3</p></td>
<td><p class="sd-card-text">RDNA3</p></td>
<td><p class="sd-card-text">RDNA3</p></td>
<td><p class="sd-card-text">RDNA3</p></td>
<td><p class="sd-card-text">RDNA3</p></td>
<td><p class="sd-card-text">RDNA3</p></td>
<td><p class="sd-card-text">RDNA3</p></td>
<td><p class="sd-card-text">RDNA3</p></td>
<td><p class="sd-card-text">RDNA3</p></td>
<td><p class="sd-card-text">RDNA3</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p class="sd-card-text">RDNA2</p></td>
<td><p class="sd-card-text">RDNA2</p></td>
<td><p class="sd-card-text">RDNA2</p></td>
<td><p class="sd-card-text">RDNA2</p></td>
<td><p class="sd-card-text">RDNA2</p></td>
<td><p class="sd-card-text">RDNA2</p></td>
<td><p class="sd-card-text">RDNA2</p></td>
<td><p class="sd-card-text">RDNA2</p></td>
<td><p class="sd-card-text">RDNA2</p></td>
<td><p class="sd-card-text">RDNA2</p></td>
<td><p class="sd-card-text">RDNA2</p></td>
<td><p class="sd-card-text">RDNA2</p></td>
<td><p class="sd-card-text">RDNA2</p></td>
<td><p class="sd-card-text">RDNA2</p></td>
<td><p class="sd-card-text">RDNA2</p></td>
<td><p class="sd-card-text">RDNA2</p></td>
<td><p class="sd-card-text">RDNA2</p></td>
<td><p class="sd-card-text">RDNA2</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td></td>
<td id="gpu-support-compatibility-matrix-past-60"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">GPU / LLVM target</span></a></p></th>
<td><p class="sd-card-text">gfx1201 <a class="footnote-reference brackets" href="#rdna-os-past-60" id="id49" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">gfx1201 <a class="footnote-reference brackets" href="#rdna-os-past-60" id="id50" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">gfx1201 <a class="footnote-reference brackets" href="#rdna-os-past-60" id="id51" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a></p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p class="sd-card-text">gfx1200 <a class="footnote-reference brackets" href="#rdna-os-past-60" id="id52" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">gfx1200 <a class="footnote-reference brackets" href="#rdna-os-past-60" id="id53" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">gfx1200 <a class="footnote-reference brackets" href="#rdna-os-past-60" id="id54" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a></p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p class="sd-card-text">gfx1101 <a class="footnote-reference brackets" href="#rdna-os-past-60" id="id55" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#xt-os-past-60" id="id56" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">gfx1101 <a class="footnote-reference brackets" href="#rdna-os-past-60" id="id57" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#xt-os-past-60" id="id58" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">gfx1101 <a class="footnote-reference brackets" href="#rdna-os-past-60" id="id59" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a></p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p class="sd-card-text">gfx1100</p></td>
<td><p class="sd-card-text">gfx1100</p></td>
<td><p class="sd-card-text">gfx1100</p></td>
<td><p class="sd-card-text">gfx1100</p></td>
<td><p class="sd-card-text">gfx1100</p></td>
<td><p class="sd-card-text">gfx1100</p></td>
<td><p class="sd-card-text">gfx1100</p></td>
<td><p class="sd-card-text">gfx1100</p></td>
<td><p class="sd-card-text">gfx1100</p></td>
<td><p class="sd-card-text">gfx1100</p></td>
<td><p class="sd-card-text">gfx1100</p></td>
<td><p class="sd-card-text">gfx1100</p></td>
<td><p class="sd-card-text">gfx1100</p></td>
<td><p class="sd-card-text">gfx1100</p></td>
<td><p class="sd-card-text">gfx1100</p></td>
<td><p class="sd-card-text">gfx1100</p></td>
<td><p class="sd-card-text">gfx1100</p></td>
<td><p class="sd-card-text">gfx1100</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p class="sd-card-text">gfx1030</p></td>
<td><p class="sd-card-text">gfx1030</p></td>
<td><p class="sd-card-text">gfx1030</p></td>
<td><p class="sd-card-text">gfx1030</p></td>
<td><p class="sd-card-text">gfx1030</p></td>
<td><p class="sd-card-text">gfx1030</p></td>
<td><p class="sd-card-text">gfx1030</p></td>
<td><p class="sd-card-text">gfx1030</p></td>
<td><p class="sd-card-text">gfx1030</p></td>
<td><p class="sd-card-text">gfx1030</p></td>
<td><p class="sd-card-text">gfx1030</p></td>
<td><p class="sd-card-text">gfx1030</p></td>
<td><p class="sd-card-text">gfx1030</p></td>
<td><p class="sd-card-text">gfx1030</p></td>
<td><p class="sd-card-text">gfx1030</p></td>
<td><p class="sd-card-text">gfx1030</p></td>
<td><p class="sd-card-text">gfx1030</p></td>
<td><p class="sd-card-text">gfx1030</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p class="sd-card-text">gfx942</p></td>
<td><p class="sd-card-text">gfx942</p></td>
<td><p class="sd-card-text">gfx942</p></td>
<td><p class="sd-card-text">gfx942</p></td>
<td><p class="sd-card-text">gfx942</p></td>
<td><p class="sd-card-text">gfx942</p></td>
<td><p class="sd-card-text">gfx942</p></td>
<td><p class="sd-card-text">gfx942</p></td>
<td><p class="sd-card-text">gfx942 <a class="footnote-reference brackets" href="#mi300-624-past-60" id="id60" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">gfx942 <a class="footnote-reference brackets" href="#mi300-622-past-60" id="id61" role="doc-noteref"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">gfx942 <a class="footnote-reference brackets" href="#mi300-621-past-60" id="id62" role="doc-noteref"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">gfx942 <a class="footnote-reference brackets" href="#mi300-620-past-60" id="id63" role="doc-noteref"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">gfx942 <a class="footnote-reference brackets" href="#mi300-612-past-60" id="id64" role="doc-noteref"><span class="fn-bracket">[</span>15<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">gfx942 <a class="footnote-reference brackets" href="#mi300-612-past-60" id="id65" role="doc-noteref"><span class="fn-bracket">[</span>15<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">gfx942 <a class="footnote-reference brackets" href="#mi300-611-past-60" id="id66" role="doc-noteref"><span class="fn-bracket">[</span>16<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">gfx942 <a class="footnote-reference brackets" href="#mi300-610-past-60" id="id67" role="doc-noteref"><span class="fn-bracket">[</span>17<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">gfx942 <a class="footnote-reference brackets" href="#mi300-602-past-60" id="id68" role="doc-noteref"><span class="fn-bracket">[</span>18<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">gfx942 <a class="footnote-reference brackets" href="#mi300-600-past-60" id="id69" role="doc-noteref"><span class="fn-bracket">[</span>19<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p class="sd-card-text">gfx90a</p></td>
<td><p class="sd-card-text">gfx90a</p></td>
<td><p class="sd-card-text">gfx90a</p></td>
<td><p class="sd-card-text">gfx90a</p></td>
<td><p class="sd-card-text">gfx90a</p></td>
<td><p class="sd-card-text">gfx90a</p></td>
<td><p class="sd-card-text">gfx90a</p></td>
<td><p class="sd-card-text">gfx90a</p></td>
<td><p class="sd-card-text">gfx90a</p></td>
<td><p class="sd-card-text">gfx90a</p></td>
<td><p class="sd-card-text">gfx90a</p></td>
<td><p class="sd-card-text">gfx90a</p></td>
<td><p class="sd-card-text">gfx90a</p></td>
<td><p class="sd-card-text">gfx90a</p></td>
<td><p class="sd-card-text">gfx90a</p></td>
<td><p class="sd-card-text">gfx90a</p></td>
<td><p class="sd-card-text">gfx90a</p></td>
<td><p class="sd-card-text">gfx90a</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p class="sd-card-text">gfx908</p></td>
<td><p class="sd-card-text">gfx908</p></td>
<td><p class="sd-card-text">gfx908</p></td>
<td><p class="sd-card-text">gfx908</p></td>
<td><p class="sd-card-text">gfx908</p></td>
<td><p class="sd-card-text">gfx908</p></td>
<td><p class="sd-card-text">gfx908</p></td>
<td><p class="sd-card-text">gfx908</p></td>
<td><p class="sd-card-text">gfx908</p></td>
<td><p class="sd-card-text">gfx908</p></td>
<td><p class="sd-card-text">gfx908</p></td>
<td><p class="sd-card-text">gfx908</p></td>
<td><p class="sd-card-text">gfx908</p></td>
<td><p class="sd-card-text">gfx908</p></td>
<td><p class="sd-card-text">gfx908</p></td>
<td><p class="sd-card-text">gfx908</p></td>
<td><p class="sd-card-text">gfx908</p></td>
<td><p class="sd-card-text">gfx908</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text">FRAMEWORK SUPPORT</p></th>
<td></td>
<td id="framework-support-compatibility-matrix-past-60"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference internal" href="#document-compatibility/ml-compatibility/pytorch-compatibility"><span class="doc">PyTorch</span></a></p></th>
<td><p class="sd-card-text">2.6, 2.5, 2.4, 2.3</p></td>
<td><p class="sd-card-text">2.6, 2.5, 2.4, 2.3</p></td>
<td><p class="sd-card-text">2.6, 2.5, 2.4, 2.3</p></td>
<td><p class="sd-card-text">2.6, 2.5, 2.4, 2.3</p></td>
<td><p class="sd-card-text">2.4, 2.3, 2.2, 1.13</p></td>
<td><p class="sd-card-text">2.4, 2.3, 2.2, 1.13</p></td>
<td><p class="sd-card-text">2.4, 2.3, 2.2, 1.13</p></td>
<td><p class="sd-card-text">2.4, 2.3, 2.2, 2.1, 2.0, 1.13</p></td>
<td><p class="sd-card-text">2.3, 2.2, 2.1, 2.0, 1.13</p></td>
<td><p class="sd-card-text">2.3, 2.2, 2.1, 2.0, 1.13</p></td>
<td><p class="sd-card-text">2.3, 2.2, 2.1, 2.0, 1.13</p></td>
<td><p class="sd-card-text">2.3, 2.2, 2.1, 2.0, 1.13</p></td>
<td><p class="sd-card-text">2.1, 2.0, 1.13</p></td>
<td><p class="sd-card-text">2.1, 2.0, 1.13</p></td>
<td><p class="sd-card-text">2.1, 2.0, 1.13</p></td>
<td><p class="sd-card-text">2.1, 2.0, 1.13</p></td>
<td><p class="sd-card-text">2.1, 2.0, 1.13</p></td>
<td><p class="sd-card-text">2.1, 2.0, 1.13</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference internal" href="#document-compatibility/ml-compatibility/tensorflow-compatibility"><span class="doc">TensorFlow</span></a></p></th>
<td><p class="sd-card-text">2.18.1, 2.17.1, 2.16.2</p></td>
<td><p class="sd-card-text">2.18.1, 2.17.1, 2.16.2</p></td>
<td><p class="sd-card-text">2.18.1, 2.17.1, 2.16.2</p></td>
<td><p class="sd-card-text">2.18.1, 2.17.1, 2.16.2</p></td>
<td><p class="sd-card-text">2.17.0, 2.16.2, 2.15.1</p></td>
<td><p class="sd-card-text">2.17.0, 2.16.2, 2.15.1</p></td>
<td><p class="sd-card-text">2.17.0, 2.16.2, 2.15.1</p></td>
<td><p class="sd-card-text">2.17.0, 2.16.2, 2.15.1</p></td>
<td><p class="sd-card-text">2.16.1, 2.15.1, 2.14.1</p></td>
<td><p class="sd-card-text">2.16.1, 2.15.1, 2.14.1</p></td>
<td><p class="sd-card-text">2.16.1, 2.15.1, 2.14.1</p></td>
<td><p class="sd-card-text">2.16.1, 2.15.1, 2.14.1</p></td>
<td><p class="sd-card-text">2.15.0, 2.14.0, 2.13.1</p></td>
<td><p class="sd-card-text">2.15.0, 2.14.0, 2.13.1</p></td>
<td><p class="sd-card-text">2.15.0, 2.14.0, 2.13.1</p></td>
<td><p class="sd-card-text">2.15.0, 2.14.0, 2.13.1</p></td>
<td><p class="sd-card-text">2.14.0, 2.13.1, 2.12.1</p></td>
<td><p class="sd-card-text">2.14.0, 2.13.1, 2.12.1</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference internal" href="#document-compatibility/ml-compatibility/jax-compatibility"><span class="doc">JAX</span></a></p></th>
<td><p class="sd-card-text">0.4.35</p></td>
<td><p class="sd-card-text">0.4.35</p></td>
<td><p class="sd-card-text">0.4.35</p></td>
<td><p class="sd-card-text">0.4.35</p></td>
<td><p class="sd-card-text">0.4.31</p></td>
<td><p class="sd-card-text">0.4.31</p></td>
<td><p class="sd-card-text">0.4.31</p></td>
<td><p class="sd-card-text">0.4.31</p></td>
<td><p class="sd-card-text">0.4.26</p></td>
<td><p class="sd-card-text">0.4.26</p></td>
<td><p class="sd-card-text">0.4.26</p></td>
<td><p class="sd-card-text">0.4.26</p></td>
<td><p class="sd-card-text">0.4.26</p></td>
<td><p class="sd-card-text">0.4.26</p></td>
<td><p class="sd-card-text">0.4.26</p></td>
<td><p class="sd-card-text">0.4.26</p></td>
<td><p class="sd-card-text">0.4.26</p></td>
<td><p class="sd-card-text">0.4.26</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference internal" href="#document-compatibility/ml-compatibility/verl-compatibility"><span class="doc">verl</span></a> <a class="footnote-reference brackets" href="#verl-compat" id="id70" role="doc-noteref"><span class="fn-bracket">[</span>20<span class="fn-bracket">]</span></a></p></th>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">0.3.0.post0</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference internal" href="#document-compatibility/ml-compatibility/stanford-megatron-lm-compatibility"><span class="doc">Stanford Megatron-LM</span></a></p></th>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">85f95ae</p></td>
<td><p class="sd-card-text">85f95ae</p></td>
<td><p class="sd-card-text">85f95ae</p></td>
<td><p class="sd-card-text">85f95ae</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference internal" href="#document-compatibility/ml-compatibility/dgl-compatibility"><span class="doc">DGL</span></a> <a class="footnote-reference brackets" href="#dgl-compat" id="id71" role="doc-noteref"><span class="fn-bracket">[</span>21<span class="fn-bracket">]</span></a></p></th>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">2.4.0</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference internal" href="#document-compatibility/ml-compatibility/megablocks-compatibility"><span class="doc">Megablocks</span></a></p></th>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">0.7.0</p></td>
<td><p class="sd-card-text">0.7.0</p></td>
<td><p class="sd-card-text">0.7.0</p></td>
<td><p class="sd-card-text">0.7.0</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference internal" href="#document-compatibility/ml-compatibility/taichi-compatibility"><span class="doc">Taichi</span></a> <a class="footnote-reference brackets" href="#taichi-compat" id="id72" role="doc-noteref"><span class="fn-bracket">[</span>22<span class="fn-bracket">]</span></a></p></th>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">1.8.0b1</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://onnxruntime.ai/docs/build/eps.html#amd-migraphx">ONNX Runtime</a></p></th>
<td><p class="sd-card-text">1.2</p></td>
<td><p class="sd-card-text">1.2</p></td>
<td><p class="sd-card-text">1.2</p></td>
<td><p class="sd-card-text">1.2</p></td>
<td><p class="sd-card-text">1.17.3</p></td>
<td><p class="sd-card-text">1.17.3</p></td>
<td><p class="sd-card-text">1.17.3</p></td>
<td><p class="sd-card-text">1.17.3</p></td>
<td><p class="sd-card-text">1.17.3</p></td>
<td><p class="sd-card-text">1.17.3</p></td>
<td><p class="sd-card-text">1.17.3</p></td>
<td><p class="sd-card-text">1.17.3</p></td>
<td><p class="sd-card-text">1.17.3</p></td>
<td><p class="sd-card-text">1.17.3</p></td>
<td><p class="sd-card-text">1.17.3</p></td>
<td><p class="sd-card-text">1.17.3</p></td>
<td><p class="sd-card-text">1.14.1</p></td>
<td><p class="sd-card-text">1.14.1</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text">THIRD PARTY COMMS</p></th>
<td></td>
<td id="thirdpartycomms-support-compatibility-matrix-past-60"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://github.com/ROCm/ucc">UCC</a></p></th>
<td><p class="sd-card-text">&gt;=1.3.0</p></td>
<td><p class="sd-card-text">&gt;=1.3.0</p></td>
<td><p class="sd-card-text">&gt;=1.3.0</p></td>
<td><p class="sd-card-text">&gt;=1.3.0</p></td>
<td><p class="sd-card-text">&gt;=1.3.0</p></td>
<td><p class="sd-card-text">&gt;=1.3.0</p></td>
<td><p class="sd-card-text">&gt;=1.3.0</p></td>
<td><p class="sd-card-text">&gt;=1.3.0</p></td>
<td><p class="sd-card-text">&gt;=1.3.0</p></td>
<td><p class="sd-card-text">&gt;=1.3.0</p></td>
<td><p class="sd-card-text">&gt;=1.3.0</p></td>
<td><p class="sd-card-text">&gt;=1.3.0</p></td>
<td><p class="sd-card-text">&gt;=1.3.0</p></td>
<td><p class="sd-card-text">&gt;=1.3.0</p></td>
<td><p class="sd-card-text">&gt;=1.3.0</p></td>
<td><p class="sd-card-text">&gt;=1.3.0</p></td>
<td><p class="sd-card-text">&gt;=1.2.0</p></td>
<td><p class="sd-card-text">&gt;=1.2.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://github.com/ROCm/ucx">UCX</a></p></th>
<td><p class="sd-card-text">&gt;=1.15.0</p></td>
<td><p class="sd-card-text">&gt;=1.15.0</p></td>
<td><p class="sd-card-text">&gt;=1.15.0</p></td>
<td><p class="sd-card-text">&gt;=1.15.0</p></td>
<td><p class="sd-card-text">&gt;=1.15.0</p></td>
<td><p class="sd-card-text">&gt;=1.15.0</p></td>
<td><p class="sd-card-text">&gt;=1.15.0</p></td>
<td><p class="sd-card-text">&gt;=1.15.0</p></td>
<td><p class="sd-card-text">&gt;=1.15.0</p></td>
<td><p class="sd-card-text">&gt;=1.15.0</p></td>
<td><p class="sd-card-text">&gt;=1.15.0</p></td>
<td><p class="sd-card-text">&gt;=1.15.0</p></td>
<td><p class="sd-card-text">&gt;=1.14.1</p></td>
<td><p class="sd-card-text">&gt;=1.14.1</p></td>
<td><p class="sd-card-text">&gt;=1.14.1</p></td>
<td><p class="sd-card-text">&gt;=1.14.1</p></td>
<td><p class="sd-card-text">&gt;=1.14.1</p></td>
<td><p class="sd-card-text">&gt;=1.14.1</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text">THIRD PARTY ALGORITHM</p></th>
<td></td>
<td id="thirdpartyalgorithm-support-compatibility-matrix-past-60"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text">Thrust</p></th>
<td><p class="sd-card-text">2.5.0</p></td>
<td><p class="sd-card-text">2.5.0</p></td>
<td><p class="sd-card-text">2.5.0</p></td>
<td><p class="sd-card-text">2.5.0</p></td>
<td><p class="sd-card-text">2.3.2</p></td>
<td><p class="sd-card-text">2.3.2</p></td>
<td><p class="sd-card-text">2.3.2</p></td>
<td><p class="sd-card-text">2.3.2</p></td>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.1.0</p></td>
<td><p class="sd-card-text">2.1.0</p></td>
<td><p class="sd-card-text">2.1.0</p></td>
<td><p class="sd-card-text">2.1.0</p></td>
<td><p class="sd-card-text">2.0.1</p></td>
<td><p class="sd-card-text">2.0.1</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text">CUB</p></th>
<td><p class="sd-card-text">2.5.0</p></td>
<td><p class="sd-card-text">2.5.0</p></td>
<td><p class="sd-card-text">2.5.0</p></td>
<td><p class="sd-card-text">2.5.0</p></td>
<td><p class="sd-card-text">2.3.2</p></td>
<td><p class="sd-card-text">2.3.2</p></td>
<td><p class="sd-card-text">2.3.2</p></td>
<td><p class="sd-card-text">2.3.2</p></td>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.1.0</p></td>
<td><p class="sd-card-text">2.1.0</p></td>
<td><p class="sd-card-text">2.1.0</p></td>
<td><p class="sd-card-text">2.1.0</p></td>
<td><p class="sd-card-text">2.0.1</p></td>
<td><p class="sd-card-text">2.0.1</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text">KMD &amp; USER SPACE <a class="footnote-reference brackets" href="#kfd-support-past-60" id="id76" role="doc-noteref"><span class="fn-bracket">[</span>23<span class="fn-bracket">]</span></a></p></th>
<td></td>
<td id="kfd-userspace-support-compatibility-matrix-past-60"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/user-kernel-space-compat-matrix.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">KMD versions</span></a></p></th>
<td><p class="sd-card-text">6.4.x, 6.3.x, 6.2.x, 6.1.x</p></td>
<td><p class="sd-card-text">6.4.x, 6.3.x, 6.2.x, 6.1.x</p></td>
<td><p class="sd-card-text">6.4.x, 6.3.x, 6.2.x, 6.1.x</p></td>
<td><p class="sd-card-text">6.4.x, 6.3.x, 6.2.x, 6.1.x</p></td>
<td><p class="sd-card-text">6.4.x, 6.3.x, 6.2.x, 6.1.x</p></td>
<td><p class="sd-card-text">6.4.x, 6.3.x, 6.2.x, 6.1.x</p></td>
<td><p class="sd-card-text">6.4.x, 6.3.x, 6.2.x, 6.1.x</p></td>
<td><p class="sd-card-text">6.4.x, 6.3.x, 6.2.x, 6.1.x</p></td>
<td><p class="sd-card-text">6.4.x, 6.3.x, 6.2.x, 6.1.x, 6.0.x</p></td>
<td><p class="sd-card-text">6.4.x, 6.3.x, 6.2.x, 6.1.x, 6.0.x</p></td>
<td><p class="sd-card-text">6.4.x, 6.3.x, 6.2.x, 6.1.x, 6.0.x</p></td>
<td><p class="sd-card-text">6.4.x, 6.3.x, 6.2.x, 6.1.x, 6.0.x</p></td>
<td><p class="sd-card-text">6.4.x, 6.3.x, 6.2.x, 6.1.x, 6.0.x, 5.7.x</p></td>
<td><p class="sd-card-text">6.4.x, 6.3.x, 6.2.x, 6.1.x, 6.0.x, 5.7.x</p></td>
<td><p class="sd-card-text">6.4.x, 6.3.x, 6.2.x, 6.1.x, 6.0.x, 5.7.x</p></td>
<td><p class="sd-card-text">6.4.x, 6.3.x, 6.2.x, 6.1.x, 6.0.x, 5.7.x</p></td>
<td><p class="sd-card-text">6.2.x, 6.1.x, 6.0.x, 5.7.x, 5.6.x</p></td>
<td><p class="sd-card-text">6.2.x, 6.1.x, 6.0.x, 5.7.x, 5.6.x</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text">ML &amp; COMPUTER VISION</p></th>
<td></td>
<td id="mllibs-support-compatibility-matrix-past-60"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/composable_kernel/en/latest/index.html" title="(in Composable Kernel Documentation v1.1.0)"><span class="xref std std-doc">Composable Kernel</span></a></p></th>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/AMDMIGraphX/en/latest/index.html" title="(in MIGraphX v2.12.0)"><span class="xref std std-doc">MIGraphX</span></a></p></th>
<td><p class="sd-card-text">2.12.0</p></td>
<td><p class="sd-card-text">2.12.0</p></td>
<td><p class="sd-card-text">2.12.0</p></td>
<td><p class="sd-card-text">2.12.0</p></td>
<td><p class="sd-card-text">2.11.0</p></td>
<td><p class="sd-card-text">2.11.0</p></td>
<td><p class="sd-card-text">2.11.0</p></td>
<td><p class="sd-card-text">2.11.0</p></td>
<td><p class="sd-card-text">2.10.0</p></td>
<td><p class="sd-card-text">2.10.0</p></td>
<td><p class="sd-card-text">2.10.0</p></td>
<td><p class="sd-card-text">2.10.0</p></td>
<td><p class="sd-card-text">2.9.0</p></td>
<td><p class="sd-card-text">2.9.0</p></td>
<td><p class="sd-card-text">2.9.0</p></td>
<td><p class="sd-card-text">2.9.0</p></td>
<td><p class="sd-card-text">2.8.0</p></td>
<td><p class="sd-card-text">2.8.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/MIOpen/en/latest/index.html" title="(in MIOpen Documentation v3.4.0)"><span class="xref std std-doc">MIOpen</span></a></p></th>
<td><p class="sd-card-text">3.4.0</p></td>
<td><p class="sd-card-text">3.4.0</p></td>
<td><p class="sd-card-text">3.4.0</p></td>
<td><p class="sd-card-text">3.4.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.0.0</p></td>
<td><p class="sd-card-text">3.0.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/MIVisionX/en/latest/index.html" title="(in MIVisionX Documentation v3.2.0)"><span class="xref std std-doc">MIVisionX</span></a></p></th>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.0.0</p></td>
<td><p class="sd-card-text">3.0.0</p></td>
<td><p class="sd-card-text">3.0.0</p></td>
<td><p class="sd-card-text">3.0.0</p></td>
<td><p class="sd-card-text">2.5.0</p></td>
<td><p class="sd-card-text">2.5.0</p></td>
<td><p class="sd-card-text">2.5.0</p></td>
<td><p class="sd-card-text">2.5.0</p></td>
<td><p class="sd-card-text">2.5.0</p></td>
<td><p class="sd-card-text">2.5.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocAL/en/latest/index.html" title="(in rocAL Documentation v2.2.0)"><span class="xref std std-doc">rocAL</span></a></p></th>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.1.0</p></td>
<td><p class="sd-card-text">2.1.0</p></td>
<td><p class="sd-card-text">2.1.0</p></td>
<td><p class="sd-card-text">2.1.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocDecode/en/latest/index.html" title="(in rocDecode documentation v0.10.0)"><span class="xref std std-doc">rocDecode</span></a></p></th>
<td><p class="sd-card-text">0.10.0</p></td>
<td><p class="sd-card-text">0.10.0</p></td>
<td><p class="sd-card-text">0.10.0</p></td>
<td><p class="sd-card-text">0.10.0</p></td>
<td><p class="sd-card-text">0.8.0</p></td>
<td><p class="sd-card-text">0.8.0</p></td>
<td><p class="sd-card-text">0.8.0</p></td>
<td><p class="sd-card-text">0.8.0</p></td>
<td><p class="sd-card-text">0.6.0</p></td>
<td><p class="sd-card-text">0.6.0</p></td>
<td><p class="sd-card-text">0.6.0</p></td>
<td><p class="sd-card-text">0.6.0</p></td>
<td><p class="sd-card-text">0.6.0</p></td>
<td><p class="sd-card-text">0.6.0</p></td>
<td><p class="sd-card-text">0.5.0</p></td>
<td><p class="sd-card-text">0.5.0</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocJPEG/en/latest/index.html" title="(in rocJPEG Documentation v0.8.0)"><span class="xref std std-doc">rocJPEG</span></a></p></th>
<td><p class="sd-card-text">0.8.0</p></td>
<td><p class="sd-card-text">0.8.0</p></td>
<td><p class="sd-card-text">0.8.0</p></td>
<td><p class="sd-card-text">0.8.0</p></td>
<td><p class="sd-card-text">0.6.0</p></td>
<td><p class="sd-card-text">0.6.0</p></td>
<td><p class="sd-card-text">0.6.0</p></td>
<td><p class="sd-card-text">0.6.0</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocPyDecode/en/latest/index.html" title="(in rocPyDecode v0.3.1)"><span class="xref std std-doc">rocPyDecode</span></a></p></th>
<td><p class="sd-card-text">0.3.1</p></td>
<td><p class="sd-card-text">0.3.1</p></td>
<td><p class="sd-card-text">0.3.1</p></td>
<td><p class="sd-card-text">0.3.1</p></td>
<td><p class="sd-card-text">0.2.0</p></td>
<td><p class="sd-card-text">0.2.0</p></td>
<td><p class="sd-card-text">0.2.0</p></td>
<td><p class="sd-card-text">0.2.0</p></td>
<td><p class="sd-card-text">0.1.0</p></td>
<td><p class="sd-card-text">0.1.0</p></td>
<td><p class="sd-card-text">0.1.0</p></td>
<td><p class="sd-card-text">0.1.0</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rpp/en/latest/index.html" title="(in RPP documentation v1.9.10)"><span class="xref std std-doc">RPP</span></a></p></th>
<td><p class="sd-card-text">1.9.10</p></td>
<td><p class="sd-card-text">1.9.10</p></td>
<td><p class="sd-card-text">1.9.10</p></td>
<td><p class="sd-card-text">1.9.10</p></td>
<td><p class="sd-card-text">1.9.1</p></td>
<td><p class="sd-card-text">1.9.1</p></td>
<td><p class="sd-card-text">1.9.1</p></td>
<td><p class="sd-card-text">1.9.1</p></td>
<td><p class="sd-card-text">1.8.0</p></td>
<td><p class="sd-card-text">1.8.0</p></td>
<td><p class="sd-card-text">1.8.0</p></td>
<td><p class="sd-card-text">1.8.0</p></td>
<td><p class="sd-card-text">1.5.0</p></td>
<td><p class="sd-card-text">1.5.0</p></td>
<td><p class="sd-card-text">1.5.0</p></td>
<td><p class="sd-card-text">1.5.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text">COMMUNICATION</p></th>
<td></td>
<td id="commlibs-support-compatibility-matrix-past-60"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rccl/en/latest/index.html" title="(in RCCL Documentation v2.22.3)"><span class="xref std std-doc">RCCL</span></a></p></th>
<td><p class="sd-card-text">2.22.3</p></td>
<td><p class="sd-card-text">2.22.3</p></td>
<td><p class="sd-card-text">2.22.3</p></td>
<td><p class="sd-card-text">2.22.3</p></td>
<td><p class="sd-card-text">2.21.5</p></td>
<td><p class="sd-card-text">2.21.5</p></td>
<td><p class="sd-card-text">2.21.5</p></td>
<td><p class="sd-card-text">2.21.5</p></td>
<td><p class="sd-card-text">2.20.5</p></td>
<td><p class="sd-card-text">2.20.5</p></td>
<td><p class="sd-card-text">2.20.5</p></td>
<td><p class="sd-card-text">2.20.5</p></td>
<td><p class="sd-card-text">2.18.6</p></td>
<td><p class="sd-card-text">2.18.6</p></td>
<td><p class="sd-card-text">2.18.6</p></td>
<td><p class="sd-card-text">2.18.6</p></td>
<td><p class="sd-card-text">2.18.3</p></td>
<td><p class="sd-card-text">2.18.3</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocSHMEM/en/latest/index.html" title="(in rocSHMEM v2.0.1)"><span class="xref std std-doc">rocSHMEM</span></a></p></th>
<td><p class="sd-card-text">2.0.1</p></td>
<td><p class="sd-card-text">2.0.1</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text">MATH LIBS</p></th>
<td></td>
<td id="mathlibs-support-compatibility-matrix-past-60"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://github.com/ROCm/half">half</a></p></th>
<td><p class="sd-card-text">1.12.0</p></td>
<td><p class="sd-card-text">1.12.0</p></td>
<td><p class="sd-card-text">1.12.0</p></td>
<td><p class="sd-card-text">1.12.0</p></td>
<td><p class="sd-card-text">1.12.0</p></td>
<td><p class="sd-card-text">1.12.0</p></td>
<td><p class="sd-card-text">1.12.0</p></td>
<td><p class="sd-card-text">1.12.0</p></td>
<td><p class="sd-card-text">1.12.0</p></td>
<td><p class="sd-card-text">1.12.0</p></td>
<td><p class="sd-card-text">1.12.0</p></td>
<td><p class="sd-card-text">1.12.0</p></td>
<td><p class="sd-card-text">1.12.0</p></td>
<td><p class="sd-card-text">1.12.0</p></td>
<td><p class="sd-card-text">1.12.0</p></td>
<td><p class="sd-card-text">1.12.0</p></td>
<td><p class="sd-card-text">1.12.0</p></td>
<td><p class="sd-card-text">1.12.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipBLAS/en/latest/index.html" title="(in hipBLAS Documentation v2.4.0)"><span class="xref std std-doc">hipBLAS</span></a></p></th>
<td><p class="sd-card-text">2.4.0</p></td>
<td><p class="sd-card-text">2.4.0</p></td>
<td><p class="sd-card-text">2.4.0</p></td>
<td><p class="sd-card-text">2.4.0</p></td>
<td><p class="sd-card-text">2.3.0</p></td>
<td><p class="sd-card-text">2.3.0</p></td>
<td><p class="sd-card-text">2.3.0</p></td>
<td><p class="sd-card-text">2.3.0</p></td>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.1.0</p></td>
<td><p class="sd-card-text">2.1.0</p></td>
<td><p class="sd-card-text">2.1.0</p></td>
<td><p class="sd-card-text">2.1.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipBLASLt/en/latest/index.html" title="(in hipBLASLt Documentation v0.12.1)"><span class="xref std std-doc">hipBLASLt</span></a></p></th>
<td><p class="sd-card-text">0.12.1</p></td>
<td><p class="sd-card-text">0.12.1</p></td>
<td><p class="sd-card-text">0.12.1</p></td>
<td><p class="sd-card-text">0.12.0</p></td>
<td><p class="sd-card-text">0.10.0</p></td>
<td><p class="sd-card-text">0.10.0</p></td>
<td><p class="sd-card-text">0.10.0</p></td>
<td><p class="sd-card-text">0.10.0</p></td>
<td><p class="sd-card-text">0.8.0</p></td>
<td><p class="sd-card-text">0.8.0</p></td>
<td><p class="sd-card-text">0.8.0</p></td>
<td><p class="sd-card-text">0.8.0</p></td>
<td><p class="sd-card-text">0.7.0</p></td>
<td><p class="sd-card-text">0.7.0</p></td>
<td><p class="sd-card-text">0.7.0</p></td>
<td><p class="sd-card-text">0.7.0</p></td>
<td><p class="sd-card-text">0.6.0</p></td>
<td><p class="sd-card-text">0.6.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipFFT/en/latest/index.html" title="(in hipFFT Documentation v1.0.18)"><span class="xref std std-doc">hipFFT</span></a></p></th>
<td><p class="sd-card-text">1.0.18</p></td>
<td><p class="sd-card-text">1.0.18</p></td>
<td><p class="sd-card-text">1.0.18</p></td>
<td><p class="sd-card-text">1.0.18</p></td>
<td><p class="sd-card-text">1.0.17</p></td>
<td><p class="sd-card-text">1.0.17</p></td>
<td><p class="sd-card-text">1.0.17</p></td>
<td><p class="sd-card-text">1.0.17</p></td>
<td><p class="sd-card-text">1.0.16</p></td>
<td><p class="sd-card-text">1.0.15</p></td>
<td><p class="sd-card-text">1.0.15</p></td>
<td><p class="sd-card-text">1.0.14</p></td>
<td><p class="sd-card-text">1.0.14</p></td>
<td><p class="sd-card-text">1.0.14</p></td>
<td><p class="sd-card-text">1.0.14</p></td>
<td><p class="sd-card-text">1.0.14</p></td>
<td><p class="sd-card-text">1.0.13</p></td>
<td><p class="sd-card-text">1.0.13</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipfort/en/latest/index.html" title="(in hipfort Documentation v0.6.0)"><span class="xref std std-doc">hipfort</span></a></p></th>
<td><p class="sd-card-text">0.6.0</p></td>
<td><p class="sd-card-text">0.6.0</p></td>
<td><p class="sd-card-text">0.6.0</p></td>
<td><p class="sd-card-text">0.6.0</p></td>
<td><p class="sd-card-text">0.5.1</p></td>
<td><p class="sd-card-text">0.5.1</p></td>
<td><p class="sd-card-text">0.5.0</p></td>
<td><p class="sd-card-text">0.5.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipRAND/en/latest/index.html" title="(in hipRAND Documentation v2.12.0)"><span class="xref std std-doc">hipRAND</span></a></p></th>
<td><p class="sd-card-text">2.12.0</p></td>
<td><p class="sd-card-text">2.12.0</p></td>
<td><p class="sd-card-text">2.12.0</p></td>
<td><p class="sd-card-text">2.12.0</p></td>
<td><p class="sd-card-text">2.11.1</p></td>
<td><p class="sd-card-text">2.11.1</p></td>
<td><p class="sd-card-text">2.11.1</p></td>
<td><p class="sd-card-text">2.11.0</p></td>
<td><p class="sd-card-text">2.11.1</p></td>
<td><p class="sd-card-text">2.11.0</p></td>
<td><p class="sd-card-text">2.11.0</p></td>
<td><p class="sd-card-text">2.11.0</p></td>
<td><p class="sd-card-text">2.10.16</p></td>
<td><p class="sd-card-text">2.10.16</p></td>
<td><p class="sd-card-text">2.10.16</p></td>
<td><p class="sd-card-text">2.10.16</p></td>
<td><p class="sd-card-text">2.10.16</p></td>
<td><p class="sd-card-text">2.10.16</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipSOLVER/en/latest/index.html" title="(in hipSOLVER Documentation v2.4.0)"><span class="xref std std-doc">hipSOLVER</span></a></p></th>
<td><p class="sd-card-text">2.4.0</p></td>
<td><p class="sd-card-text">2.4.0</p></td>
<td><p class="sd-card-text">2.4.0</p></td>
<td><p class="sd-card-text">2.4.0</p></td>
<td><p class="sd-card-text">2.3.0</p></td>
<td><p class="sd-card-text">2.3.0</p></td>
<td><p class="sd-card-text">2.3.0</p></td>
<td><p class="sd-card-text">2.3.0</p></td>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.2.0</p></td>
<td><p class="sd-card-text">2.1.1</p></td>
<td><p class="sd-card-text">2.1.1</p></td>
<td><p class="sd-card-text">2.1.1</p></td>
<td><p class="sd-card-text">2.1.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipSPARSE/en/latest/index.html" title="(in hipSPARSE Documentation v3.2.0)"><span class="xref std std-doc">hipSPARSE</span></a></p></th>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.1.2</p></td>
<td><p class="sd-card-text">3.1.2</p></td>
<td><p class="sd-card-text">3.1.2</p></td>
<td><p class="sd-card-text">3.1.2</p></td>
<td><p class="sd-card-text">3.1.1</p></td>
<td><p class="sd-card-text">3.1.1</p></td>
<td><p class="sd-card-text">3.1.1</p></td>
<td><p class="sd-card-text">3.1.1</p></td>
<td><p class="sd-card-text">3.0.1</p></td>
<td><p class="sd-card-text">3.0.1</p></td>
<td><p class="sd-card-text">3.0.1</p></td>
<td><p class="sd-card-text">3.0.1</p></td>
<td><p class="sd-card-text">3.0.0</p></td>
<td><p class="sd-card-text">3.0.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipSPARSELt/en/latest/index.html" title="(in hipSPARSELt Documentation v0.2.3)"><span class="xref std std-doc">hipSPARSELt</span></a></p></th>
<td><p class="sd-card-text">0.2.3</p></td>
<td><p class="sd-card-text">0.2.3</p></td>
<td><p class="sd-card-text">0.2.3</p></td>
<td><p class="sd-card-text">0.2.3</p></td>
<td><p class="sd-card-text">0.2.2</p></td>
<td><p class="sd-card-text">0.2.2</p></td>
<td><p class="sd-card-text">0.2.2</p></td>
<td><p class="sd-card-text">0.2.2</p></td>
<td><p class="sd-card-text">0.2.1</p></td>
<td><p class="sd-card-text">0.2.1</p></td>
<td><p class="sd-card-text">0.2.1</p></td>
<td><p class="sd-card-text">0.2.1</p></td>
<td><p class="sd-card-text">0.2.0</p></td>
<td><p class="sd-card-text">0.2.0</p></td>
<td><p class="sd-card-text">0.1.0</p></td>
<td><p class="sd-card-text">0.1.0</p></td>
<td><p class="sd-card-text">0.1.0</p></td>
<td><p class="sd-card-text">0.1.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocALUTION/en/latest/index.html" title="(in rocALUTION Documentation v3.2.3)"><span class="xref std std-doc">rocALUTION</span></a></p></th>
<td><p class="sd-card-text">3.2.3</p></td>
<td><p class="sd-card-text">3.2.3</p></td>
<td><p class="sd-card-text">3.2.3</p></td>
<td><p class="sd-card-text">3.2.2</p></td>
<td><p class="sd-card-text">3.2.1</p></td>
<td><p class="sd-card-text">3.2.1</p></td>
<td><p class="sd-card-text">3.2.1</p></td>
<td><p class="sd-card-text">3.2.1</p></td>
<td><p class="sd-card-text">3.2.1</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.1.1</p></td>
<td><p class="sd-card-text">3.1.1</p></td>
<td><p class="sd-card-text">3.1.1</p></td>
<td><p class="sd-card-text">3.1.1</p></td>
<td><p class="sd-card-text">3.0.3</p></td>
<td><p class="sd-card-text">3.0.3</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocBLAS/en/latest/index.html" title="(in rocBLAS Documentation v4.4.1)"><span class="xref std std-doc">rocBLAS</span></a></p></th>
<td><p class="sd-card-text">4.4.1</p></td>
<td><p class="sd-card-text">4.4.1</p></td>
<td><p class="sd-card-text">4.4.0</p></td>
<td><p class="sd-card-text">4.4.0</p></td>
<td><p class="sd-card-text">4.3.0</p></td>
<td><p class="sd-card-text">4.3.0</p></td>
<td><p class="sd-card-text">4.3.0</p></td>
<td><p class="sd-card-text">4.3.0</p></td>
<td><p class="sd-card-text">4.2.4</p></td>
<td><p class="sd-card-text">4.2.1</p></td>
<td><p class="sd-card-text">4.2.1</p></td>
<td><p class="sd-card-text">4.2.0</p></td>
<td><p class="sd-card-text">4.1.2</p></td>
<td><p class="sd-card-text">4.1.2</p></td>
<td><p class="sd-card-text">4.1.0</p></td>
<td><p class="sd-card-text">4.1.0</p></td>
<td><p class="sd-card-text">4.0.0</p></td>
<td><p class="sd-card-text">4.0.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocFFT/en/latest/index.html" title="(in rocFFT Documentation v1.0.32)"><span class="xref std std-doc">rocFFT</span></a></p></th>
<td><p class="sd-card-text">1.0.32</p></td>
<td><p class="sd-card-text">1.0.32</p></td>
<td><p class="sd-card-text">1.0.32</p></td>
<td><p class="sd-card-text">1.0.32</p></td>
<td><p class="sd-card-text">1.0.31</p></td>
<td><p class="sd-card-text">1.0.31</p></td>
<td><p class="sd-card-text">1.0.31</p></td>
<td><p class="sd-card-text">1.0.31</p></td>
<td><p class="sd-card-text">1.0.30</p></td>
<td><p class="sd-card-text">1.0.29</p></td>
<td><p class="sd-card-text">1.0.29</p></td>
<td><p class="sd-card-text">1.0.28</p></td>
<td><p class="sd-card-text">1.0.27</p></td>
<td><p class="sd-card-text">1.0.27</p></td>
<td><p class="sd-card-text">1.0.27</p></td>
<td><p class="sd-card-text">1.0.26</p></td>
<td><p class="sd-card-text">1.0.25</p></td>
<td><p class="sd-card-text">1.0.23</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocRAND/en/latest/index.html" title="(in rocRAND Documentation v3.3.0)"><span class="xref std std-doc">rocRAND</span></a></p></th>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.1.1</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.0.1</p></td>
<td><p class="sd-card-text">3.0.1</p></td>
<td><p class="sd-card-text">3.0.1</p></td>
<td><p class="sd-card-text">3.0.1</p></td>
<td><p class="sd-card-text">3.0.0</p></td>
<td><p class="sd-card-text">2.10.17</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocSOLVER/en/latest/index.html" title="(in rocSOLVER Documentation v3.28.2)"><span class="xref std std-doc">rocSOLVER</span></a></p></th>
<td><p class="sd-card-text">3.28.2</p></td>
<td><p class="sd-card-text">3.28.2</p></td>
<td><p class="sd-card-text">3.28.0</p></td>
<td><p class="sd-card-text">3.28.0</p></td>
<td><p class="sd-card-text">3.27.0</p></td>
<td><p class="sd-card-text">3.27.0</p></td>
<td><p class="sd-card-text">3.27.0</p></td>
<td><p class="sd-card-text">3.27.0</p></td>
<td><p class="sd-card-text">3.26.2</p></td>
<td><p class="sd-card-text">3.26.0</p></td>
<td><p class="sd-card-text">3.26.0</p></td>
<td><p class="sd-card-text">3.26.0</p></td>
<td><p class="sd-card-text">3.25.0</p></td>
<td><p class="sd-card-text">3.25.0</p></td>
<td><p class="sd-card-text">3.25.0</p></td>
<td><p class="sd-card-text">3.25.0</p></td>
<td><p class="sd-card-text">3.24.0</p></td>
<td><p class="sd-card-text">3.24.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocSPARSE/en/latest/index.html" title="(in rocSPARSE Documentation v3.4.0)"><span class="xref std std-doc">rocSPARSE</span></a></p></th>
<td><p class="sd-card-text">3.4.0</p></td>
<td><p class="sd-card-text">3.4.0</p></td>
<td><p class="sd-card-text">3.4.0</p></td>
<td><p class="sd-card-text">3.4.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.2.1</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.1.2</p></td>
<td><p class="sd-card-text">3.1.2</p></td>
<td><p class="sd-card-text">3.1.2</p></td>
<td><p class="sd-card-text">3.1.2</p></td>
<td><p class="sd-card-text">3.0.2</p></td>
<td><p class="sd-card-text">3.0.2</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocWMMA/en/latest/index.html" title="(in rocWMMA Documentation v1.7.0)"><span class="xref std std-doc">rocWMMA</span></a></p></th>
<td><p class="sd-card-text">1.7.0</p></td>
<td><p class="sd-card-text">1.7.0</p></td>
<td><p class="sd-card-text">1.7.0</p></td>
<td><p class="sd-card-text">1.7.0</p></td>
<td><p class="sd-card-text">1.6.0</p></td>
<td><p class="sd-card-text">1.6.0</p></td>
<td><p class="sd-card-text">1.6.0</p></td>
<td><p class="sd-card-text">1.6.0</p></td>
<td><p class="sd-card-text">1.5.0</p></td>
<td><p class="sd-card-text">1.5.0</p></td>
<td><p class="sd-card-text">1.5.0</p></td>
<td><p class="sd-card-text">1.5.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.3.0</p></td>
<td><p class="sd-card-text">1.3.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/Tensile/en/latest/src/index.html" title="(in Tensile Documentation v4.43.0)"><span class="xref std std-doc">Tensile</span></a></p></th>
<td><p class="sd-card-text">4.43.0</p></td>
<td><p class="sd-card-text">4.43.0</p></td>
<td><p class="sd-card-text">4.43.0</p></td>
<td><p class="sd-card-text">4.43.0</p></td>
<td><p class="sd-card-text">4.42.0</p></td>
<td><p class="sd-card-text">4.42.0</p></td>
<td><p class="sd-card-text">4.42.0</p></td>
<td><p class="sd-card-text">4.42.0</p></td>
<td><p class="sd-card-text">4.41.0</p></td>
<td><p class="sd-card-text">4.41.0</p></td>
<td><p class="sd-card-text">4.41.0</p></td>
<td><p class="sd-card-text">4.41.0</p></td>
<td><p class="sd-card-text">4.40.0</p></td>
<td><p class="sd-card-text">4.40.0</p></td>
<td><p class="sd-card-text">4.40.0</p></td>
<td><p class="sd-card-text">4.40.0</p></td>
<td><p class="sd-card-text">4.39.0</p></td>
<td><p class="sd-card-text">4.39.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text">PRIMITIVES</p></th>
<td></td>
<td id="primitivelibs-support-compatibility-matrix-past-60"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipCUB/en/latest/index.html" title="(in hipCUB Documentation v3.4.0)"><span class="xref std std-doc">hipCUB</span></a></p></th>
<td><p class="sd-card-text">3.4.0</p></td>
<td><p class="sd-card-text">3.4.0</p></td>
<td><p class="sd-card-text">3.4.0</p></td>
<td><p class="sd-card-text">3.4.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.2.1</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.0.0</p></td>
<td><p class="sd-card-text">3.0.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipTensor/en/latest/index.html" title="(in hipTensor Documentation v1.5.0)"><span class="xref std std-doc">hipTensor</span></a></p></th>
<td><p class="sd-card-text">1.5.0</p></td>
<td><p class="sd-card-text">1.5.0</p></td>
<td><p class="sd-card-text">1.5.0</p></td>
<td><p class="sd-card-text">1.5.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.3.0</p></td>
<td><p class="sd-card-text">1.3.0</p></td>
<td><p class="sd-card-text">1.3.0</p></td>
<td><p class="sd-card-text">1.3.0</p></td>
<td><p class="sd-card-text">1.2.0</p></td>
<td><p class="sd-card-text">1.2.0</p></td>
<td><p class="sd-card-text">1.2.0</p></td>
<td><p class="sd-card-text">1.2.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocPRIM/en/latest/index.html" title="(in rocPRIM Documentation v3.4.1)"><span class="xref std std-doc">rocPRIM</span></a></p></th>
<td><p class="sd-card-text">3.4.1</p></td>
<td><p class="sd-card-text">3.4.1</p></td>
<td><p class="sd-card-text">3.4.0</p></td>
<td><p class="sd-card-text">3.4.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.2.2</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.2.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.0.0</p></td>
<td><p class="sd-card-text">3.0.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocThrust/en/latest/index.html" title="(in rocThrust Documentation v3.3.0)"><span class="xref std std-doc">rocThrust</span></a></p></th>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.3.0</p></td>
<td><p class="sd-card-text">3.1.1</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.0.1</p></td>
<td><p class="sd-card-text">3.0.1</p></td>
<td><p class="sd-card-text">3.0.1</p></td>
<td><p class="sd-card-text">3.0.1</p></td>
<td><p class="sd-card-text">3.0.1</p></td>
<td><p class="sd-card-text">3.0.0</p></td>
<td><p class="sd-card-text">3.0.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text">SUPPORT LIBS</p></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://github.com/ROCm/hipother">hipother</a></p></th>
<td><p class="sd-card-text">6.4.43483</p></td>
<td><p class="sd-card-text">6.4.43483</p></td>
<td><p class="sd-card-text">6.4.43483</p></td>
<td><p class="sd-card-text">6.4.43482</p></td>
<td><p class="sd-card-text">6.3.42134</p></td>
<td><p class="sd-card-text">6.3.42134</p></td>
<td><p class="sd-card-text">6.3.42133</p></td>
<td><p class="sd-card-text">6.3.42131</p></td>
<td><p class="sd-card-text">6.2.41134</p></td>
<td><p class="sd-card-text">6.2.41134</p></td>
<td><p class="sd-card-text">6.2.41134</p></td>
<td><p class="sd-card-text">6.2.41133</p></td>
<td><p class="sd-card-text">6.1.40093</p></td>
<td><p class="sd-card-text">6.1.40093</p></td>
<td><p class="sd-card-text">6.1.40092</p></td>
<td><p class="sd-card-text">6.1.40091</p></td>
<td><p class="sd-card-text">6.1.32831</p></td>
<td><p class="sd-card-text">6.1.32830</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://github.com/ROCm/rocm-core">rocm-core</a></p></th>
<td><p class="sd-card-text">6.4.3</p></td>
<td><p class="sd-card-text">6.4.2</p></td>
<td><p class="sd-card-text">6.4.1</p></td>
<td><p class="sd-card-text">6.4.0</p></td>
<td><p class="sd-card-text">6.3.3</p></td>
<td><p class="sd-card-text">6.3.2</p></td>
<td><p class="sd-card-text">6.3.1</p></td>
<td><p class="sd-card-text">6.3.0</p></td>
<td><p class="sd-card-text">6.2.4</p></td>
<td><p class="sd-card-text">6.2.2</p></td>
<td><p class="sd-card-text">6.2.1</p></td>
<td><p class="sd-card-text">6.2.0</p></td>
<td><p class="sd-card-text">6.1.5</p></td>
<td><p class="sd-card-text">6.1.2</p></td>
<td><p class="sd-card-text">6.1.1</p></td>
<td><p class="sd-card-text">6.1.0</p></td>
<td><p class="sd-card-text">6.0.2</p></td>
<td><p class="sd-card-text">6.0.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://github.com/ROCm/ROCT-Thunk-Interface">ROCT-Thunk-Interface</a></p></th>
<td><p class="sd-card-text">N/A <a class="footnote-reference brackets" href="#roct-rocr-past-60" id="id81" role="doc-noteref"><span class="fn-bracket">[</span>24<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">N/A <a class="footnote-reference brackets" href="#roct-rocr-past-60" id="id82" role="doc-noteref"><span class="fn-bracket">[</span>24<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">N/A <a class="footnote-reference brackets" href="#roct-rocr-past-60" id="id83" role="doc-noteref"><span class="fn-bracket">[</span>24<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">N/A <a class="footnote-reference brackets" href="#roct-rocr-past-60" id="id84" role="doc-noteref"><span class="fn-bracket">[</span>24<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">N/A <a class="footnote-reference brackets" href="#roct-rocr-past-60" id="id85" role="doc-noteref"><span class="fn-bracket">[</span>24<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">N/A <a class="footnote-reference brackets" href="#roct-rocr-past-60" id="id86" role="doc-noteref"><span class="fn-bracket">[</span>24<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">N/A <a class="footnote-reference brackets" href="#roct-rocr-past-60" id="id87" role="doc-noteref"><span class="fn-bracket">[</span>24<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">N/A <a class="footnote-reference brackets" href="#roct-rocr-past-60" id="id88" role="doc-noteref"><span class="fn-bracket">[</span>24<span class="fn-bracket">]</span></a></p></td>
<td><p class="sd-card-text">20240607.5.7</p></td>
<td><p class="sd-card-text">20240607.5.7</p></td>
<td><p class="sd-card-text">20240607.4.05</p></td>
<td><p class="sd-card-text">20240607.1.4246</p></td>
<td><p class="sd-card-text">20240125.5.08</p></td>
<td><p class="sd-card-text">20240125.5.08</p></td>
<td><p class="sd-card-text">20240125.5.08</p></td>
<td><p class="sd-card-text">20240125.3.30</p></td>
<td><p class="sd-card-text">20231016.2.245</p></td>
<td><p class="sd-card-text">20231016.2.245</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text">SYSTEM MGMT TOOLS</p></th>
<td></td>
<td id="tools-support-compatibility-matrix-past-60"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/amdsmi/en/latest/index.html" title="(in AMD SMI v25.5.1)"><span class="xref std std-doc">AMD SMI</span></a></p></th>
<td><p class="sd-card-text">25.5.1</p></td>
<td><p class="sd-card-text">25.5.1</p></td>
<td><p class="sd-card-text">25.4.2</p></td>
<td><p class="sd-card-text">25.3.0</p></td>
<td><p class="sd-card-text">24.7.1</p></td>
<td><p class="sd-card-text">24.7.1</p></td>
<td><p class="sd-card-text">24.7.1</p></td>
<td><p class="sd-card-text">24.7.1</p></td>
<td><p class="sd-card-text">24.6.3</p></td>
<td><p class="sd-card-text">24.6.3</p></td>
<td><p class="sd-card-text">24.6.3</p></td>
<td><p class="sd-card-text">24.6.2</p></td>
<td><p class="sd-card-text">24.5.1</p></td>
<td><p class="sd-card-text">24.5.1</p></td>
<td><p class="sd-card-text">24.5.1</p></td>
<td><p class="sd-card-text">24.4.1</p></td>
<td><p class="sd-card-text">23.4.2</p></td>
<td><p class="sd-card-text">23.4.2</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rdc/en/latest/index.html" title="(in ROCm Data Center Documentation)"><span class="xref std std-doc">ROCm Data Center Tool</span></a></p></th>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocminfo/en/latest/index.html" title="(in rocminfo v1.0.0)"><span class="xref std std-doc">rocminfo</span></a></p></th>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocm_smi_lib/en/latest/index.html" title="(in ROCm SMI LIB Documentation v7.7.0)"><span class="xref std std-doc">ROCm SMI</span></a></p></th>
<td><p class="sd-card-text">7.7.0</p></td>
<td><p class="sd-card-text">7.5.0</p></td>
<td><p class="sd-card-text">7.5.0</p></td>
<td><p class="sd-card-text">7.5.0</p></td>
<td><p class="sd-card-text">7.4.0</p></td>
<td><p class="sd-card-text">7.4.0</p></td>
<td><p class="sd-card-text">7.4.0</p></td>
<td><p class="sd-card-text">7.4.0</p></td>
<td><p class="sd-card-text">7.3.0</p></td>
<td><p class="sd-card-text">7.3.0</p></td>
<td><p class="sd-card-text">7.3.0</p></td>
<td><p class="sd-card-text">7.3.0</p></td>
<td><p class="sd-card-text">7.2.0</p></td>
<td><p class="sd-card-text">7.2.0</p></td>
<td><p class="sd-card-text">7.0.0</p></td>
<td><p class="sd-card-text">7.0.0</p></td>
<td><p class="sd-card-text">6.0.2</p></td>
<td><p class="sd-card-text">6.0.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCmValidationSuite/en/latest/index.html" title="(in RVS Documentation v1.1.0)"><span class="xref std std-doc">ROCm Validation Suite</span></a></p></th>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.1.0</p></td>
<td><p class="sd-card-text">1.0.60204</p></td>
<td><p class="sd-card-text">1.0.60202</p></td>
<td><p class="sd-card-text">1.0.60201</p></td>
<td><p class="sd-card-text">1.0.60200</p></td>
<td><p class="sd-card-text">1.0.60105</p></td>
<td><p class="sd-card-text">1.0.60102</p></td>
<td><p class="sd-card-text">1.0.60101</p></td>
<td><p class="sd-card-text">1.0.60100</p></td>
<td><p class="sd-card-text">1.0.60002</p></td>
<td><p class="sd-card-text">1.0.60000</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text">PERFORMANCE TOOLS</p></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocm_bandwidth_test/en/latest/index.html" title="(in rocm_bandwidth_test)"><span class="xref std std-doc">ROCm Bandwidth Test</span></a></p></th>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
<td><p class="sd-card-text">1.4.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler-compute/en/latest/index.html" title="(in ROCm Compute Profiler v3.1.1)"><span class="xref std std-doc">ROCm Compute Profiler</span></a></p></th>
<td><p class="sd-card-text">3.1.1</p></td>
<td><p class="sd-card-text">3.1.1</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.1.0</p></td>
<td><p class="sd-card-text">3.0.0</p></td>
<td><p class="sd-card-text">3.0.0</p></td>
<td><p class="sd-card-text">3.0.0</p></td>
<td><p class="sd-card-text">3.0.0</p></td>
<td><p class="sd-card-text">2.0.1</p></td>
<td><p class="sd-card-text">2.0.1</p></td>
<td><p class="sd-card-text">2.0.1</p></td>
<td><p class="sd-card-text">2.0.1</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler-systems/en/latest/index.html" title="(in rocprofiler-systems v1.0.2)"><span class="xref std std-doc">ROCm Systems Profiler</span></a></p></th>
<td><p class="sd-card-text">1.0.2</p></td>
<td><p class="sd-card-text">1.0.2</p></td>
<td><p class="sd-card-text">1.0.1</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">0.1.2</p></td>
<td><p class="sd-card-text">0.1.1</p></td>
<td><p class="sd-card-text">0.1.0</p></td>
<td><p class="sd-card-text">0.1.0</p></td>
<td><p class="sd-card-text">1.11.2</p></td>
<td><p class="sd-card-text">1.11.2</p></td>
<td><p class="sd-card-text">1.11.2</p></td>
<td><p class="sd-card-text">1.11.2</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler/en/latest/index.html" title="(in rocprofiler Documentation v2.0.0)"><span class="xref std std-doc">ROCProfiler</span></a></p></th>
<td><p class="sd-card-text">2.0.60403</p></td>
<td><p class="sd-card-text">2.0.60402</p></td>
<td><p class="sd-card-text">2.0.60401</p></td>
<td><p class="sd-card-text">2.0.60400</p></td>
<td><p class="sd-card-text">2.0.60303</p></td>
<td><p class="sd-card-text">2.0.60302</p></td>
<td><p class="sd-card-text">2.0.60301</p></td>
<td><p class="sd-card-text">2.0.60300</p></td>
<td><p class="sd-card-text">2.0.60204</p></td>
<td><p class="sd-card-text">2.0.60202</p></td>
<td><p class="sd-card-text">2.0.60201</p></td>
<td><p class="sd-card-text">2.0.60200</p></td>
<td><p class="sd-card-text">2.0.60105</p></td>
<td><p class="sd-card-text">2.0.60102</p></td>
<td><p class="sd-card-text">2.0.60101</p></td>
<td><p class="sd-card-text">2.0.60100</p></td>
<td><p class="sd-card-text">2.0.60002</p></td>
<td><p class="sd-card-text">2.0.60000</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler-sdk/en/latest/index.html" title="(in Rocprofiler SDK v0.6.0)"><span class="xref std std-doc">ROCprofiler-SDK</span></a></p></th>
<td><p class="sd-card-text">0.6.0</p></td>
<td><p class="sd-card-text">0.6.0</p></td>
<td><p class="sd-card-text">0.6.0</p></td>
<td><p class="sd-card-text">0.6.0</p></td>
<td><p class="sd-card-text">0.5.0</p></td>
<td><p class="sd-card-text">0.5.0</p></td>
<td><p class="sd-card-text">0.5.0</p></td>
<td><p class="sd-card-text">0.5.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/roctracer/en/latest/index.html" title="(in roctracer Documentation v4.1.0)"><span class="xref std std-doc">ROCTracer</span></a></p></th>
<td><p class="sd-card-text">4.1.60403</p></td>
<td><p class="sd-card-text">4.1.60402</p></td>
<td><p class="sd-card-text">4.1.60401</p></td>
<td><p class="sd-card-text">4.1.60400</p></td>
<td><p class="sd-card-text">4.1.60303</p></td>
<td><p class="sd-card-text">4.1.60302</p></td>
<td><p class="sd-card-text">4.1.60301</p></td>
<td><p class="sd-card-text">4.1.60300</p></td>
<td><p class="sd-card-text">4.1.60204</p></td>
<td><p class="sd-card-text">4.1.60202</p></td>
<td><p class="sd-card-text">4.1.60201</p></td>
<td><p class="sd-card-text">4.1.60200</p></td>
<td><p class="sd-card-text">4.1.60105</p></td>
<td><p class="sd-card-text">4.1.60102</p></td>
<td><p class="sd-card-text">4.1.60101</p></td>
<td><p class="sd-card-text">4.1.60100</p></td>
<td><p class="sd-card-text">4.1.60002</p></td>
<td><p class="sd-card-text">4.1.60000</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text">DEVELOPMENT TOOLS</p></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/HIPIFY/en/latest/index.html" title="(in HIPIFY Documentation)"><span class="xref std std-doc">HIPIFY</span></a></p></th>
<td><p class="sd-card-text">19.0.0</p></td>
<td><p class="sd-card-text">19.0.0</p></td>
<td><p class="sd-card-text">19.0.0</p></td>
<td><p class="sd-card-text">19.0.0</p></td>
<td><p class="sd-card-text">18.0.0.25012</p></td>
<td><p class="sd-card-text">18.0.0.25012</p></td>
<td><p class="sd-card-text">18.0.0.24491</p></td>
<td><p class="sd-card-text">18.0.0.24455</p></td>
<td><p class="sd-card-text">18.0.0.24392</p></td>
<td><p class="sd-card-text">18.0.0.24355</p></td>
<td><p class="sd-card-text">18.0.0.24355</p></td>
<td><p class="sd-card-text">18.0.0.24232</p></td>
<td><p class="sd-card-text">17.0.0.24193</p></td>
<td><p class="sd-card-text">17.0.0.24193</p></td>
<td><p class="sd-card-text">17.0.0.24154</p></td>
<td><p class="sd-card-text">17.0.0.24103</p></td>
<td><p class="sd-card-text">17.0.0.24012</p></td>
<td><p class="sd-card-text">17.0.0.23483</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCmCMakeBuildTools/en/latest/index.html" title="(in ROCm CMake Build Tools v0.14.0)"><span class="xref std std-doc">ROCm CMake</span></a></p></th>
<td><p class="sd-card-text">0.14.0</p></td>
<td><p class="sd-card-text">0.14.0</p></td>
<td><p class="sd-card-text">0.14.0</p></td>
<td><p class="sd-card-text">0.14.0</p></td>
<td><p class="sd-card-text">0.14.0</p></td>
<td><p class="sd-card-text">0.14.0</p></td>
<td><p class="sd-card-text">0.14.0</p></td>
<td><p class="sd-card-text">0.14.0</p></td>
<td><p class="sd-card-text">0.13.0</p></td>
<td><p class="sd-card-text">0.13.0</p></td>
<td><p class="sd-card-text">0.13.0</p></td>
<td><p class="sd-card-text">0.13.0</p></td>
<td><p class="sd-card-text">0.12.0</p></td>
<td><p class="sd-card-text">0.12.0</p></td>
<td><p class="sd-card-text">0.12.0</p></td>
<td><p class="sd-card-text">0.12.0</p></td>
<td><p class="sd-card-text">0.11.0</p></td>
<td><p class="sd-card-text">0.11.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCdbgapi/en/latest/index.html" title="(in ROCdbgapi Documentation v0.77.2)"><span class="xref std std-doc">ROCdbgapi</span></a></p></th>
<td><p class="sd-card-text">0.77.2</p></td>
<td><p class="sd-card-text">0.77.2</p></td>
<td><p class="sd-card-text">0.77.2</p></td>
<td><p class="sd-card-text">0.77.2</p></td>
<td><p class="sd-card-text">0.77.0</p></td>
<td><p class="sd-card-text">0.77.0</p></td>
<td><p class="sd-card-text">0.77.0</p></td>
<td><p class="sd-card-text">0.77.0</p></td>
<td><p class="sd-card-text">0.76.0</p></td>
<td><p class="sd-card-text">0.76.0</p></td>
<td><p class="sd-card-text">0.76.0</p></td>
<td><p class="sd-card-text">0.76.0</p></td>
<td><p class="sd-card-text">0.71.0</p></td>
<td><p class="sd-card-text">0.71.0</p></td>
<td><p class="sd-card-text">0.71.0</p></td>
<td><p class="sd-card-text">0.71.0</p></td>
<td><p class="sd-card-text">0.71.0</p></td>
<td><p class="sd-card-text">0.71.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCgdb/en/latest/index.html" title="(in ROCgdb Documentation v15.2)"><span class="xref std std-doc">ROCm Debugger (ROCgdb)</span></a></p></th>
<td><p class="sd-card-text">15.2.0</p></td>
<td><p class="sd-card-text">15.2.0</p></td>
<td><p class="sd-card-text">15.2.0</p></td>
<td><p class="sd-card-text">15.2.0</p></td>
<td><p class="sd-card-text">15.2.0</p></td>
<td><p class="sd-card-text">15.2.0</p></td>
<td><p class="sd-card-text">15.2.0</p></td>
<td><p class="sd-card-text">15.2.0</p></td>
<td><p class="sd-card-text">14.2.0</p></td>
<td><p class="sd-card-text">14.2.0</p></td>
<td><p class="sd-card-text">14.2.0</p></td>
<td><p class="sd-card-text">14.2.0</p></td>
<td><p class="sd-card-text">14.1.0</p></td>
<td><p class="sd-card-text">14.1.0</p></td>
<td><p class="sd-card-text">14.1.0</p></td>
<td><p class="sd-card-text">14.1.0</p></td>
<td><p class="sd-card-text">13.2.0</p></td>
<td><p class="sd-card-text">13.2.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://github.com/ROCm/rocprofiler-register">rocprofiler-register</a></p></th>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.4.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">0.3.0</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocr_debug_agent/en/latest/index.html" title="(in rocr_debug_agent v2.0.4)"><span class="xref std std-doc">ROCr Debug Agent</span></a></p></th>
<td><p class="sd-card-text">2.0.4</p></td>
<td><p class="sd-card-text">2.0.4</p></td>
<td><p class="sd-card-text">2.0.4</p></td>
<td><p class="sd-card-text">2.0.4</p></td>
<td><p class="sd-card-text">2.0.3</p></td>
<td><p class="sd-card-text">2.0.3</p></td>
<td><p class="sd-card-text">2.0.3</p></td>
<td><p class="sd-card-text">2.0.3</p></td>
<td><p class="sd-card-text">2.0.3</p></td>
<td><p class="sd-card-text">2.0.3</p></td>
<td><p class="sd-card-text">2.0.3</p></td>
<td><p class="sd-card-text">2.0.3</p></td>
<td><p class="sd-card-text">2.0.3</p></td>
<td><p class="sd-card-text">2.0.3</p></td>
<td><p class="sd-card-text">2.0.3</p></td>
<td><p class="sd-card-text">2.0.3</p></td>
<td><p class="sd-card-text">2.0.3</p></td>
<td><p class="sd-card-text">2.0.3</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text">COMPILERS</p></th>
<td></td>
<td id="compilers-support-compatibility-matrix-past-60"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://github.com/ROCm/clang-ocl">clang-ocl</a></p></th>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">N/A</p></td>
<td><p class="sd-card-text">0.5.0</p></td>
<td><p class="sd-card-text">0.5.0</p></td>
<td><p class="sd-card-text">0.5.0</p></td>
<td><p class="sd-card-text">0.5.0</p></td>
<td><p class="sd-card-text">0.5.0</p></td>
<td><p class="sd-card-text">0.5.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/HIPCC/en/latest/index.html" title="(in HIPCC Documentation v1.1.1)"><span class="xref std std-doc">hipCC</span></a></p></th>
<td><p class="sd-card-text">1.1.1</p></td>
<td><p class="sd-card-text">1.1.1</p></td>
<td><p class="sd-card-text">1.1.1</p></td>
<td><p class="sd-card-text">1.1.1</p></td>
<td><p class="sd-card-text">1.1.1</p></td>
<td><p class="sd-card-text">1.1.1</p></td>
<td><p class="sd-card-text">1.1.1</p></td>
<td><p class="sd-card-text">1.1.1</p></td>
<td><p class="sd-card-text">1.1.1</p></td>
<td><p class="sd-card-text">1.1.1</p></td>
<td><p class="sd-card-text">1.1.1</p></td>
<td><p class="sd-card-text">1.1.1</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
<td><p class="sd-card-text">1.0.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://github.com/ROCm/flang">Flang</a></p></th>
<td><p class="sd-card-text">19.0.0.25224</p></td>
<td><p class="sd-card-text">19.0.0.25224</p></td>
<td><p class="sd-card-text">19.0.0.25184</p></td>
<td><p class="sd-card-text">19.0.0.25133</p></td>
<td><p class="sd-card-text">18.0.0.25012</p></td>
<td><p class="sd-card-text">18.0.0.25012</p></td>
<td><p class="sd-card-text">18.0.0.24491</p></td>
<td><p class="sd-card-text">18.0.0.24455</p></td>
<td><p class="sd-card-text">18.0.0.24392</p></td>
<td><p class="sd-card-text">18.0.0.24355</p></td>
<td><p class="sd-card-text">18.0.0.24355</p></td>
<td><p class="sd-card-text">18.0.0.24232</p></td>
<td><p class="sd-card-text">17.0.0.24193</p></td>
<td><p class="sd-card-text">17.0.0.24193</p></td>
<td><p class="sd-card-text">17.0.0.24154</p></td>
<td><p class="sd-card-text">17.0.0.24103</p></td>
<td><p class="sd-card-text">17.0.0.24012</p></td>
<td><p class="sd-card-text">17.0.0.23483</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/index.html" title="(in llvm-project Documentation v19.0.0)"><span class="xref std std-doc">llvm-project</span></a></p></th>
<td><p class="sd-card-text">19.0.0.25224</p></td>
<td><p class="sd-card-text">19.0.0.25224</p></td>
<td><p class="sd-card-text">19.0.0.25184</p></td>
<td><p class="sd-card-text">19.0.0.25133</p></td>
<td><p class="sd-card-text">18.0.0.25012</p></td>
<td><p class="sd-card-text">18.0.0.25012</p></td>
<td><p class="sd-card-text">18.0.0.24491</p></td>
<td><p class="sd-card-text">18.0.0.24491</p></td>
<td><p class="sd-card-text">18.0.0.24392</p></td>
<td><p class="sd-card-text">18.0.0.24355</p></td>
<td><p class="sd-card-text">18.0.0.24355</p></td>
<td><p class="sd-card-text">18.0.0.24232</p></td>
<td><p class="sd-card-text">17.0.0.24193</p></td>
<td><p class="sd-card-text">17.0.0.24193</p></td>
<td><p class="sd-card-text">17.0.0.24154</p></td>
<td><p class="sd-card-text">17.0.0.24103</p></td>
<td><p class="sd-card-text">17.0.0.24012</p></td>
<td><p class="sd-card-text">17.0.0.23483</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://github.com/ROCm/llvm-project/tree/amd-staging/openmp">OpenMP</a></p></th>
<td><p class="sd-card-text">19.0.0.25224</p></td>
<td><p class="sd-card-text">19.0.0.25224</p></td>
<td><p class="sd-card-text">19.0.0.25184</p></td>
<td><p class="sd-card-text">19.0.0.25133</p></td>
<td><p class="sd-card-text">18.0.0.25012</p></td>
<td><p class="sd-card-text">18.0.0.25012</p></td>
<td><p class="sd-card-text">18.0.0.24491</p></td>
<td><p class="sd-card-text">18.0.0.24491</p></td>
<td><p class="sd-card-text">18.0.0.24392</p></td>
<td><p class="sd-card-text">18.0.0.24355</p></td>
<td><p class="sd-card-text">18.0.0.24355</p></td>
<td><p class="sd-card-text">18.0.0.24232</p></td>
<td><p class="sd-card-text">17.0.0.24193</p></td>
<td><p class="sd-card-text">17.0.0.24193</p></td>
<td><p class="sd-card-text">17.0.0.24154</p></td>
<td><p class="sd-card-text">17.0.0.24103</p></td>
<td><p class="sd-card-text">17.0.0.24012</p></td>
<td><p class="sd-card-text">17.0.0.23483</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text">RUNTIMES</p></th>
<td></td>
<td id="runtime-support-compatibility-matrix-past-60"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/understand/amd_clr.html" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-doc">AMD CLR</span></a></p></th>
<td><p class="sd-card-text">6.4.43484</p></td>
<td><p class="sd-card-text">6.4.43484</p></td>
<td><p class="sd-card-text">6.4.43483</p></td>
<td><p class="sd-card-text">6.4.43482</p></td>
<td><p class="sd-card-text">6.3.42134</p></td>
<td><p class="sd-card-text">6.3.42134</p></td>
<td><p class="sd-card-text">6.3.42133</p></td>
<td><p class="sd-card-text">6.3.42131</p></td>
<td><p class="sd-card-text">6.2.41134</p></td>
<td><p class="sd-card-text">6.2.41134</p></td>
<td><p class="sd-card-text">6.2.41134</p></td>
<td><p class="sd-card-text">6.2.41133</p></td>
<td><p class="sd-card-text">6.1.40093</p></td>
<td><p class="sd-card-text">6.1.40093</p></td>
<td><p class="sd-card-text">6.1.40092</p></td>
<td><p class="sd-card-text">6.1.40091</p></td>
<td><p class="sd-card-text">6.1.32831</p></td>
<td><p class="sd-card-text">6.1.32830</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/index.html" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-doc">HIP</span></a></p></th>
<td><p class="sd-card-text">6.4.43484</p></td>
<td><p class="sd-card-text">6.4.43484</p></td>
<td><p class="sd-card-text">6.4.43483</p></td>
<td><p class="sd-card-text">6.4.43482</p></td>
<td><p class="sd-card-text">6.3.42134</p></td>
<td><p class="sd-card-text">6.3.42134</p></td>
<td><p class="sd-card-text">6.3.42133</p></td>
<td><p class="sd-card-text">6.3.42131</p></td>
<td><p class="sd-card-text">6.2.41134</p></td>
<td><p class="sd-card-text">6.2.41134</p></td>
<td><p class="sd-card-text">6.2.41134</p></td>
<td><p class="sd-card-text">6.2.41133</p></td>
<td><p class="sd-card-text">6.1.40093</p></td>
<td><p class="sd-card-text">6.1.40093</p></td>
<td><p class="sd-card-text">6.1.40092</p></td>
<td><p class="sd-card-text">6.1.40091</p></td>
<td><p class="sd-card-text">6.1.32831</p></td>
<td><p class="sd-card-text">6.1.32830</p></td>
</tr>
<tr class="row-even"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://github.com/ROCm/clr/tree/develop/opencl">OpenCL Runtime</a></p></th>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
<td><p class="sd-card-text">2.0.0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCR-Runtime/en/latest/index.html" title="(in ROCR Documentation v1.15.0)"><span class="xref std std-doc">ROCr Runtime</span></a></p></th>
<td><p class="sd-card-text">1.15.0</p></td>
<td><p class="sd-card-text">1.15.0</p></td>
<td><p class="sd-card-text">1.15.0</p></td>
<td><p class="sd-card-text">1.15.0</p></td>
<td><p class="sd-card-text">1.14.0</p></td>
<td><p class="sd-card-text">1.14.0</p></td>
<td><p class="sd-card-text">1.14.0</p></td>
<td><p class="sd-card-text">1.14.0</p></td>
<td><p class="sd-card-text">1.14.0</p></td>
<td><p class="sd-card-text">1.14.0</p></td>
<td><p class="sd-card-text">1.14.0</p></td>
<td><p class="sd-card-text">1.13.0</p></td>
<td><p class="sd-card-text">1.13.0</p></td>
<td><p class="sd-card-text">1.13.0</p></td>
<td><p class="sd-card-text">1.13.0</p></td>
<td><p class="sd-card-text">1.13.0</p></td>
<td><p class="sd-card-text">1.12.0</p></td>
<td><p class="sd-card-text">1.12.0</p></td>
</tr>
</tbody>
</table>
</div>
<p class="rubric">Footnotes</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="mi300x-past-60" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a href="#id21" role="doc-backlink">1</a>,<a href="#id22" role="doc-backlink">2</a>,<a href="#id23" role="doc-backlink">3</a>,<a href="#id24" role="doc-backlink">4</a>,<a href="#id25" role="doc-backlink">5</a>,<a href="#id26" role="doc-backlink">6</a>,<a href="#id27" role="doc-backlink">7</a>,<a href="#id28" role="doc-backlink">8</a>,<a href="#id29" role="doc-backlink">9</a>,<a href="#id30" role="doc-backlink">10</a>,<a href="#id31" role="doc-backlink">11</a>,<a href="#id32" role="doc-backlink">12</a>,<a href="#id33" role="doc-backlink">13</a>,<a href="#id34" role="doc-backlink">14</a>,<a href="#id35" role="doc-backlink">15</a>,<a href="#id43" role="doc-backlink">16</a>,<a href="#id44" role="doc-backlink">17</a>,<a href="#id45" role="doc-backlink">18</a>,<a href="#id46" role="doc-backlink">19</a>,<a href="#id47" role="doc-backlink">20</a>,<a href="#id48" role="doc-backlink">21</a>)</span>
<p class="sd-card-text">Oracle Linux and Azure Linux are supported only on AMD Instinct MI300X.</p>
</aside>
<aside class="footnote brackets" id="single-node-past-60" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a href="#id36" role="doc-backlink">1</a>,<a href="#id37" role="doc-backlink">2</a>,<a href="#id38" role="doc-backlink">3</a>,<a href="#id39" role="doc-backlink">4</a>,<a href="#id40" role="doc-backlink">5</a>,<a href="#id41" role="doc-backlink">6</a>,<a href="#id42" role="doc-backlink">7</a>)</span>
<p class="sd-card-text">Debian 12 is supported only on AMD Instinct MI300X for single-node functionality.</p>
</aside>
<aside class="footnote brackets" id="rdna-os-past-60" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a href="#id49" role="doc-backlink">1</a>,<a href="#id50" role="doc-backlink">2</a>,<a href="#id51" role="doc-backlink">3</a>,<a href="#id52" role="doc-backlink">4</a>,<a href="#id53" role="doc-backlink">5</a>,<a href="#id54" role="doc-backlink">6</a>,<a href="#id55" role="doc-backlink">7</a>,<a href="#id57" role="doc-backlink">8</a>,<a href="#id59" role="doc-backlink">9</a>)</span>
<p class="sd-card-text">Radeon AI PRO R9700, Radeon RX 9070 XT (gfx1201), Radeon RX 9060 XT (gfx1200), Radeon PRO W7700 (gfx1101), and Radeon RX 7800 XT (gfx1101) are supported only on Ubuntu 24.04.2, Ubuntu 22.04.5, RHEL 9.6, and RHEL 9.4.</p>
</aside>
<aside class="footnote brackets" id="xt-os-past-60" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a href="#id56" role="doc-backlink">1</a>,<a href="#id58" role="doc-backlink">2</a>)</span>
<p class="sd-card-text">Radeon RX 7700 XT (gfx1101) is supported only on Ubuntu 24.04.2 and RHEL 9.6.</p>
</aside>
<aside class="footnote brackets" id="mi300-624-past-60" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a href="#id60" role="doc-backlink">11</a><span class="fn-bracket">]</span></span>
<p class="sd-card-text"><strong>For ROCm 6.2.4</strong> - MI300X (gfx942) is supported on listed operating systems <em>except</em> Ubuntu 22.04.5 [6.8 HWE] and Ubuntu 22.04.4 [6.5 HWE].</p>
</aside>
<aside class="footnote brackets" id="mi300-622-past-60" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a href="#id61" role="doc-backlink">12</a><span class="fn-bracket">]</span></span>
<p class="sd-card-text"><strong>For ROCm 6.2.2</strong> - MI300X (gfx942) is supported on listed operating systems <em>except</em> Ubuntu 22.04.5 [6.8 HWE] and Ubuntu 22.04.4 [6.5 HWE].</p>
</aside>
<aside class="footnote brackets" id="mi300-621-past-60" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a href="#id62" role="doc-backlink">13</a><span class="fn-bracket">]</span></span>
<p class="sd-card-text"><strong>For ROCm 6.2.1</strong> - MI300X (gfx942) is supported on listed operating systems <em>except</em> Ubuntu 22.04.5 [6.8 HWE] and Ubuntu 22.04.4 [6.5 HWE].</p>
</aside>
<aside class="footnote brackets" id="mi300-620-past-60" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a href="#id63" role="doc-backlink">14</a><span class="fn-bracket">]</span></span>
<p class="sd-card-text"><strong>For ROCm 6.2.0</strong> - MI300X (gfx942) is supported on listed operating systems <em>except</em> Ubuntu 22.04.5 [6.8 HWE] and Ubuntu 22.04.4 [6.5 HWE].</p>
</aside>
<aside class="footnote brackets" id="mi300-612-past-60" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>15<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a href="#id64" role="doc-backlink">1</a>,<a href="#id65" role="doc-backlink">2</a>)</span>
<p class="sd-card-text"><strong>For ROCm 6.1.2</strong> - MI300A (gfx942) is supported on Ubuntu 22.04.4, RHEL 9.4, RHEL 9.3, RHEL 8.9, and SLES 15 SP5. MI300X (gfx942) is only supported on Ubuntu 22.04.4 and Oracle Linux.</p>
</aside>
<aside class="footnote brackets" id="mi300-611-past-60" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a href="#id66" role="doc-backlink">16</a><span class="fn-bracket">]</span></span>
<p class="sd-card-text"><strong>For ROCm 6.1.1</strong> - MI300A (gfx942) is supported on Ubuntu 22.04.4, RHEL 9.4, RHEL 9.3, RHEL 8.9, and SLES 15 SP5. MI300X (gfx942) is only supported on Ubuntu 22.04.4 and Oracle Linux.</p>
</aside>
<aside class="footnote brackets" id="mi300-610-past-60" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a href="#id67" role="doc-backlink">17</a><span class="fn-bracket">]</span></span>
<p class="sd-card-text"><strong>For ROCm 6.1.0</strong> - MI300A (gfx942) is supported on Ubuntu 22.04.4, RHEL 9.4, RHEL 9.3, RHEL 8.9, and SLES 15 SP5. MI300X (gfx942) is only supported on Ubuntu 22.04.4.</p>
</aside>
<aside class="footnote brackets" id="mi300-602-past-60" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a href="#id68" role="doc-backlink">18</a><span class="fn-bracket">]</span></span>
<p class="sd-card-text"><strong>For ROCm 6.0.2</strong> - MI300A (gfx942) is supported on Ubuntu 22.04.3, RHEL 8.9, and SLES 15 SP5. MI300X (gfx942) is only supported on Ubuntu 22.04.3.</p>
</aside>
<aside class="footnote brackets" id="mi300-600-past-60" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a href="#id69" role="doc-backlink">19</a><span class="fn-bracket">]</span></span>
<p class="sd-card-text"><strong>For ROCm 6.0.0</strong> - MI300A (gfx942) is supported on Ubuntu 22.04.3, RHEL 8.9, and SLES 15 SP5. MI300X (gfx942) is only supported on Ubuntu 22.04.3.</p>
</aside>
<aside class="footnote brackets" id="verl-compat" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a href="#id70" role="doc-backlink">20</a><span class="fn-bracket">]</span></span>
<p class="sd-card-text">verl is only supported on ROCm 6.2.0.</p>
</aside>
<aside class="footnote brackets" id="dgl-compat" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a href="#id71" role="doc-backlink">21</a><span class="fn-bracket">]</span></span>
<p class="sd-card-text">DGL is only supported on ROCm 6.4.0.</p>
</aside>
<aside class="footnote brackets" id="taichi-compat" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a href="#id72" role="doc-backlink">22</a><span class="fn-bracket">]</span></span>
<p class="sd-card-text">Taichi is only supported on ROCm 6.3.2.</p>
</aside>
<aside class="footnote brackets" id="kfd-support-past-60" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a href="#id76" role="doc-backlink">23</a><span class="fn-bracket">]</span></span>
<p class="sd-card-text">As of ROCm 6.4.0, forward and backward compatibility between the AMD Kernel-mode GPU Driver (KMD) and its user space software is provided up to a year apart. For earlier ROCm releases, the compatibility is provided for +/- 2 releases. The tested user space versions on this page were accurate as of the time of initial ROCm release. For the most up-to-date information, see the latest version of this information at <a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/user-kernel-space-compat-matrix.html">User and kernel-space support matrix</a>.</p>
</aside>
<aside class="footnote brackets" id="roct-rocr-past-60" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a href="#id81" role="doc-backlink">1</a>,<a href="#id82" role="doc-backlink">2</a>,<a href="#id83" role="doc-backlink">3</a>,<a href="#id84" role="doc-backlink">4</a>,<a href="#id85" role="doc-backlink">5</a>,<a href="#id86" role="doc-backlink">6</a>,<a href="#id87" role="doc-backlink">7</a>,<a href="#id88" role="doc-backlink">8</a>)</span>
<p class="sd-card-text">Starting from ROCm 6.3.0, the ROCT Thunk Interface is included as part of the ROCr runtime package.</p>
</aside>
</aside>
</div>
</details></section>
<div class="toctree-wrapper compound">
</div>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-how-to/deep-learning-rocm"></span><section id="installing-deep-learning-frameworks-for-rocm">
<h2>Installing deep learning frameworks for ROCm<a class="headerlink" href="#installing-deep-learning-frameworks-for-rocm" title="Link to this heading">#</a></h2>
<p>ROCm provides a comprehensive ecosystem for deep learning development, including
<a class="reference internal" href="#artificial-intelligence-apis"><span class="std std-ref">libraries</span></a> for optimized deep learning operations and ROCm-aware versions of popular
deep learning frameworks and libraries such as PyTorch, TensorFlow, and JAX. ROCm works closely with these
frameworks to ensure that framework-specific optimizations take advantage of AMD accelerator and GPU architectures.</p>
<p>The following guides provide information on compatibility and supported
features for these ROCm-enabled deep learning frameworks.</p>
<ul class="simple">
<li><p><a class="reference internal" href="#document-compatibility/ml-compatibility/pytorch-compatibility"><span class="doc">PyTorch compatibility</span></a></p></li>
<li><p><a class="reference internal" href="#document-compatibility/ml-compatibility/tensorflow-compatibility"><span class="doc">TensorFlow compatibility</span></a></p></li>
<li><p><a class="reference internal" href="#document-compatibility/ml-compatibility/jax-compatibility"><span class="doc">JAX compatibility</span></a></p></li>
<li><p><a class="reference internal" href="#document-compatibility/ml-compatibility/verl-compatibility"><span class="doc">verl compatibility</span></a></p></li>
<li><p><a class="reference internal" href="#document-compatibility/ml-compatibility/stanford-megatron-lm-compatibility"><span class="doc">Stanford Megatron-LM compatibility</span></a></p></li>
<li><p><a class="reference internal" href="#document-compatibility/ml-compatibility/dgl-compatibility"><span class="doc">DGL compatibility</span></a></p></li>
<li><p><a class="reference internal" href="#document-compatibility/ml-compatibility/megablocks-compatibility"><span class="doc">Megablocks compatibility</span></a></p></li>
<li><p><a class="reference internal" href="#document-compatibility/ml-compatibility/taichi-compatibility"><span class="doc">Taichi compatibility</span></a></p></li>
</ul>
<p>This chart steps through typical installation workflows for installing deep learning frameworks for ROCm.</p>
<img alt="Flowchart for installing ROCm-aware machine learning frameworks" class="align-center" src="_images/framework_install_2024_07_04.png"/>
<p>See the installation instructions to get started.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/pytorch-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">PyTorch for ROCm</span></a></p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/tensorflow-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">TensorFlow for ROCm</span></a></p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/jax-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">JAX for ROCm</span></a></p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/verl-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">verl for ROCm</span></a></p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/stanford-megatron-lm-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">Stanford Megatron-LM for ROCm</span></a></p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/dgl-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">DGL for ROCm</span></a></p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/megablocks-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">Megablocks for ROCm</span></a></p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/taichi-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">Taichi for ROCm</span></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For guidance on installing ROCm itself, refer to <a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/index.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">ROCm installation for Linux</span></a>.</p>
</div>
<p>Learn how to use your ROCm deep learning environment for training, fine-tuning, inference, and performance optimization
through the following guides.</p>
<ul class="simple">
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/index"><span class="doc">Use ROCm for AI</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/training/index"><span class="doc">Training</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/fine-tuning/index"><span class="doc">Fine-tuning LLMs</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/inference/index"><span class="doc">Inference</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/inference-optimization/index"><span class="doc">Inference optimization</span></a></p></li>
</ul>
</section>
<span id="document-how-to/build-rocm"></span><section id="build-rocm-from-source">
<span id="building-rocm"></span><h2>Build ROCm from source<a class="headerlink" href="#build-rocm-from-source" title="Link to this heading">#</a></h2>
<p>ROCm is an open-source stack from which you can build from source code. The source code is available from <a class="github reference external" href="https://github.com/ROCm/ROCm">ROCm/ROCm</a>.</p>
<p>The general steps to build ROCm are:</p>
<ol class="arabic simple">
<li><p>Clone the ROCm source code</p></li>
<li><p>Prepare the build environment</p></li>
<li><p>Run the build command</p></li>
</ol>
<p>Because the ROCm stack is constantly evolving, the most current instructions are stored with the source code in GitHub.
For detailed build instructions, see <cite>Getting and Building ROCm from Source &lt;https://github.com/ROCm/ROCm?tab=readme-ov-file#getting-and-building-rocm-from-source&gt;</cite>.</p>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-how-to/rocm-for-ai/index"></span><section id="use-rocm-for-ai">
<h2>Use ROCm for AI<a class="headerlink" href="#use-rocm-for-ai" title="Link to this heading">#</a></h2>
<p>ROCm is an open-source software platform that enables high-performance computing and machine learning applications. It features the ability to accelerate training, fine-tuning, and inference for AI application development. With ROCm, you can access the full power of AMD GPUs, which can significantly improve the performance and efficiency of AI workloads.</p>
<p>You can use ROCm to perform distributed training, which enables you to train models across multiple GPUs or nodes simultaneously. Additionally, ROCm supports mixed-precision training, which can help reduce the memory and compute requirements of training workloads. For fine-tuning, ROCm provides access to various algorithms and optimization techniques. In terms of inference, ROCm provides several techniques that can help you optimize your models for deployment, such as quantization, GEMM tuning, and optimization with composable kernel.</p>
<p>Overall, ROCm can be used to improve the performance and efficiency of your AI applications. With its training, fine-tuning, and inference support, ROCm provides a complete solution for optimizing AI workflows and achieving the optimum results possible on AMD GPUs.</p>
<p>The AI Developer Hub contains <a class="reference external" href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/">AMD ROCm tutorials</a> for
training, fine-tuning, and inference. It leverages popular machine learning frameworks on AMD GPUs.</p>
<p>In this guide, you’ll learn how to use ROCm for AI:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/training/index"><span class="doc">Training</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/fine-tuning/index"><span class="doc">Fine-tuning LLMs</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/inference/index"><span class="doc">Inference</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/inference-optimization/index"><span class="doc">Inference optimization</span></a></p></li>
</ul>
<p>To learn about ROCm for HPC applications and scientific computing, see
<a class="reference internal" href="#document-how-to/rocm-for-hpc/index"><span class="doc">Using ROCm for HPC</span></a>.</p>
<div class="toctree-wrapper compound">
<span id="document-how-to/rocm-for-ai/install"></span><section id="installing-rocm-and-machine-learning-frameworks">
<span id="rocm-for-ai-install"></span><h3>Installing ROCm and machine learning frameworks<a class="headerlink" href="#installing-rocm-and-machine-learning-frameworks" title="Link to this heading">#</a></h3>
<p>Before getting started, install ROCm and supported machine learning frameworks.</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-1 sd-row-cols-xs-1 sd-row-cols-sm-1 sd-row-cols-md-1 sd-row-cols-lg-1 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
Pre-install</div>
<p class="sd-card-text">Each release of ROCm supports specific hardware and software configurations. Before installing, consult the
<a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">System requirements</span></a> and
<a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/prerequisites.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">Installation prerequisites</span></a> guides.</p>
</div>
</div>
</div>
</div>
</div>
<p>If you’re new to ROCm, refer to the <a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">ROCm quick start install guide for Linux</span></a>.</p>
<p>If you’re using a Radeon GPU for graphics-accelerated applications, refer to the
<a class="reference external" href="https://rocm.docs.amd.com/projects/radeon/en/docs-6.1.3/docs/install/native_linux/install-radeon.html">Radeon installation instructions</a>.</p>
<p>You can install ROCm on <span class="xref std std-ref">compatible systems</span> via your Linux
distribution’s package manager. See the following documentation resources to get started:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/install-overview.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">ROCm installation overview</span></a></p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/install-methods/package-manager-index.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">Using your Linux distribution’s package manager</span></a></p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/install-methods/multi-version-install-index.html#installation-types" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-ref">Multi-version installation</span></a></p></li>
</ul>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-1 sd-row-cols-xs-1 sd-row-cols-sm-1 sd-row-cols-md-1 sd-row-cols-lg-1 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
Post-install</div>
<p class="sd-card-text">Follow the <a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/post-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">post-installation instructions</span></a> to
configure your system linker, PATH, and verify the installation.</p>
<p class="sd-card-text">If you encounter any issues during installation, refer to the
<a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/install-faq.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">Installation troubleshooting</span></a> guide.</p>
</div>
</div>
</div>
</div>
</div>
<section id="machine-learning-frameworks">
<h4>Machine learning frameworks<a class="headerlink" href="#machine-learning-frameworks" title="Link to this heading">#</a></h4>
<p>ROCm supports popular machine learning frameworks and libraries including <a class="reference external" href="https://pytorch.org/blog/pytorch-for-amd-rocm-platform-now-available-as-python-package">PyTorch</a>, <a class="reference external" href="https://tensorflow.org">TensorFlow</a>, <a class="reference external" href="https://jax.readthedocs.io/en/latest">JAX</a>, and <a class="reference external" href="https://cloudblogs.microsoft.com/opensource/2022/03/21/supporting-efficient-large-model-training-on-amd-instinct-gpus-with-deepspeed/">DeepSpeed</a>.</p>
<p>Review the framework installation documentation. For ease-of-use, it’s recommended to use official ROCm prebuilt Docker
images with the framework pre-installed.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/pytorch-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">PyTorch for ROCm</span></a></p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/tensorflow-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">TensorFlow for ROCm</span></a></p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/jax-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">JAX for ROCm</span></a></p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/verl-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">verl for ROCm</span></a></p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/jax-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">Stanford Megatron-LM for ROCm</span></a></p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/jax-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">DGL for ROCm</span></a></p></li>
</ul>
</section>
<section id="next-steps">
<h4>Next steps<a class="headerlink" href="#next-steps" title="Link to this heading">#</a></h4>
<p>After installing ROCm and your desired ML libraries – and before running AI workloads – conduct system health benchmarks
to test the optimal performance of your AMD hardware. See <a class="reference internal" href="#document-how-to/rocm-for-ai/system-health-check"><span class="doc">System health benchmarks</span></a> to get started.</p>
</section>
</section>
<span id="document-how-to/rocm-for-ai/system-health-check"></span><section id="system-health-benchmarks">
<span id="rocm-for-ai-system-health-bench"></span><h3>System health benchmarks<a class="headerlink" href="#system-health-benchmarks" title="Link to this heading">#</a></h3>
<p>Before running AI workloads, it is important to validate that your AMD hardware is configured correctly and is performing optimally. This topic outlines several system health benchmarks you can use to test key aspects like GPU compute capabilities (FLOPS), memory bandwidth, and interconnect performance. Many of these tests are part of the ROCm Validation Suite (RVS).</p>
<section id="rocm-validation-suite-rvs-tests">
<h4>ROCm Validation Suite (RVS) tests<a class="headerlink" href="#rocm-validation-suite-rvs-tests" title="Link to this heading">#</a></h4>
<p>RVS provides a collection of tests, benchmarks, and qualification tools, each
targeting a specific subsystem of the system under test. It includes tests for
GPU stress and memory bandwidth.</p>
<section id="install-rocm-validation-suite">
<span id="healthcheck-install-rvs"></span><h5>Install ROCm Validation Suite<a class="headerlink" href="#install-rocm-validation-suite" title="Link to this heading">#</a></h5>
<p>To get started, install RVS. For example, on an Ubuntu system with ROCm already
installed, run the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>apt<span class="w"> </span>update
sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>rocm-validation-suite
</pre></div>
</div>
<p>See the <a class="reference external" href="https://rocm.docs.amd.com/projects/ROCmValidationSuite/en/latest/install/installation.html">ROCm Validation Suite installation instructions</a>,
and <a class="reference external" href="https://instinct.docs.amd.com/projects/system-acceptance/en/latest/mi300x/system-validation.html#system-validation-tests">System validation tests</a>
in the Instinct documentation for more detailed instructions.</p>
</section>
<section id="benchmark-stress-and-qualification-tests">
<h5>Benchmark, stress, and qualification tests<a class="headerlink" href="#benchmark-stress-and-qualification-tests" title="Link to this heading">#</a></h5>
<p>The GPU stress test runs various GEMM computations as workloads to stress the GPU FLOPS performance and check whether it
meets the configured target GFLOPS.</p>
<p>Run the benchmark, stress, and qualification tests included with RVS. See the <a class="reference external" href="https://instinct.docs.amd.com/projects/system-acceptance/en/latest/mi300x/system-validation.html#benchmark-stress-qualification">Benchmark, stress, qualification</a>
section of the Instinct documentation for usage instructions.</p>
</section>
<section id="babelstream-test">
<h5>BabelStream test<a class="headerlink" href="#babelstream-test" title="Link to this heading">#</a></h5>
<p>BabelStream is a synthetic GPU benchmark based on the STREAM benchmark for
CPUs, measuring memory transfer rates to and from global device memory.
BabelStream tests are included with the RVS package as part of the <a class="reference external" href="https://rocm.docs.amd.com/projects/ROCmValidationSuite/en/latest/conceptual/rvs-modules.html#babel-benchmark-test-babel-module">BABEL module</a>.</p>
<p>For more information, see <a class="reference external" href="https://instinct.docs.amd.com/projects/system-acceptance/en/latest/mi300x/performance-bench.html#babelstream-benchmarking-results">Performance benchmarking</a>
in the Instinct documentation.</p>
</section>
</section>
<section id="rccl-tests">
<h4>RCCL tests<a class="headerlink" href="#rccl-tests" title="Link to this heading">#</a></h4>
<p>The ROCm Communication Collectives Library (RCCL) enables efficient multi-GPU
communication. The <a class="github reference external" href="https://github.com/ROCm/rccl-tests">ROCm/rccl-tests</a> suite benchmarks
the performance and verifies the correctness of these collective operations.
This helps ensure optimal scaling for multi-accelerator tasks.</p>
<ol class="arabic">
<li><p>To get started, build RCCL-tests using the official instructions in the README at
<a class="github reference external" href="https://github.com/ROCm/rccl-tests?tab=readme-ov-file#build">ROCm/rccl-tests</a> or use the
following commands:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/rccl-tests.git
<span class="nb">cd</span><span class="w"> </span>rccl-tests
make
</pre></div>
</div>
</li>
<li><p>Run the suggested RCCL tests – see <a class="reference external" href="https://instinct.docs.amd.com/projects/system-acceptance/en/latest/mi300x/performance-bench.html#rccl-benchmarking-results">RCCL benchmarking</a>
in the Instinct performance benchmarking documentation for instructions.</p></li>
</ol>
</section>
<section id="transferbench-test">
<h4>TransferBench test<a class="headerlink" href="#transferbench-test" title="Link to this heading">#</a></h4>
<p>TransferBench is a standalone utility for benchmarking simultaneous data
transfer performance between various devices in the system, including
CPU-to-GPU and GPU-to-GPU (peer-to-peer). This helps identify potential
bottlenecks in data movement between the host system and the GPUs, or between
GPUs, which can impact end-to-end latency.</p>
<ol class="arabic" id="healthcheck-install-transferbench">
<li><p>To get started, use the instructions in the <a class="reference external" href="https://rocm.docs.amd.com/projects/TransferBench/en/latest/install/install.html#install-transferbench">TransferBench documentation</a>
or use the following commands:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/TransferBench.git
<span class="nb">cd</span><span class="w"> </span>TransferBench
<span class="nv">CC</span><span class="o">=</span>hipcc<span class="w"> </span>make
</pre></div>
</div>
</li>
<li><p>Run the suggested TransferBench tests – see <a class="reference external" href="https://instinct.docs.amd.com/projects/system-acceptance/en/latest/mi300x/performance-bench.html#transferbench-benchmarking-results">TransferBench benchmarking</a>
in the Instinct performance benchmarking documentation for instructions.</p></li>
</ol>
</section>
</section>
<span id="document-how-to/rocm-for-ai/training/index"></span><section id="use-rocm-for-training">
<h3>Use ROCm for training<a class="headerlink" href="#use-rocm-for-training" title="Link to this heading">#</a></h3>
<p>Training models is the process of teaching a computer program to recognize patterns in data. This involves providing the computer with large amounts of labeled data and allowing it to learn from that data, adjusting the model’s parameters.</p>
<p>The process of training models is computationally intensive, requiring specialized hardware like GPUs to accelerate computations and reduce training time. Training models on AMD GPUs involves leveraging the parallel processing capabilities of these GPUs to significantly speed up the model training process in machine learning and deep learning tasks.</p>
<p>Training models on AMD GPUs with the ROCm™ software platform allows you to use the powerful parallel processing capabilities and efficient compute resource management, significantly improving training time and overall performance in machine learning applications.</p>
<p>The ROCm software platform makes it easier to train models on AMD GPUs while maintaining compatibility with existing code and tools. The platform also provides features like multi-GPU support, allowing for scaling and parallelization of model training across multiple GPUs to enhance performance.</p>
<p>The AI Developer Hub contains <a class="reference external" href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/">AMD ROCm tutorials</a> for
training, fine-tuning, and inference. It leverages popular machine learning frameworks on AMD GPUs.</p>
<p>In this guide, you’ll learn about:</p>
<ul class="simple">
<li><p>Training a model</p>
<ul>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/training/benchmark-docker/megatron-lm"><span class="doc">With Megatron-LM</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/training/benchmark-docker/pytorch-training"><span class="doc">With PyTorch</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/training/benchmark-docker/jax-maxtext"><span class="doc">With JAX MaxText</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/training/benchmark-docker/mpt-llm-foundry"><span class="doc">With LLM Foundry</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/training/scale-model-training"><span class="doc">Scaling model training</span></a></p></li>
</ul>
<div class="toctree-wrapper compound">
<span id="document-how-to/rocm-for-ai/training/benchmark-docker/megatron-lm"></span><section id="training-a-model-with-megatron-lm-for-rocm">
<h4>Training a model with Megatron-LM for ROCm<a class="headerlink" href="#training-a-model-with-megatron-lm-for-rocm" title="Link to this heading">#</a></h4>
<p>The <a class="reference external" href="https://github.com/ROCm/Megatron-LM">Megatron-LM framework for ROCm</a> is
a specialized fork of the robust Megatron-LM, designed to enable efficient
training of large-scale language models on AMD GPUs. By leveraging AMD
Instinct™ MI300X series accelerators, Megatron-LM delivers enhanced
scalability, performance, and resource utilization for AI workloads. It is
purpose-built to support models like Llama, DeepSeek, and Mixtral,
enabling developers to train next-generation AI models more
efficiently.</p>
<p>AMD provides ready-to-use Docker images for MI300X series accelerators containing
essential components, including PyTorch, ROCm libraries, and Megatron-LM
utilities. It contains the following software components to accelerate training
workloads:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="rocm/megatron-lm:v25.6_py312" for="sd-tab-item-0">
<code class="docutils literal notranslate"><span class="pre">rocm/megatron-lm:v25.6_py312</span></code></label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Software component</p></th>
<th class="head"><p>Version</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ROCm</p></td>
<td><p>6.4.1</p></td>
</tr>
<tr class="row-odd"><td><p>PyTorch</p></td>
<td><p>2.8.0a0+git7d205b2</p></td>
</tr>
<tr class="row-even"><td><p>Python</p></td>
<td><p>3.12</p></td>
</tr>
<tr class="row-odd"><td><p>Transformer Engine</p></td>
<td><p>2.1.0.dev0+8c4a512</p></td>
</tr>
<tr class="row-even"><td><p>hipBLASLt</p></td>
<td><p>393e413</p></td>
</tr>
<tr class="row-odd"><td><p>Triton</p></td>
<td><p>3.3.0</p></td>
</tr>
<tr class="row-even"><td><p>RCCL</p></td>
<td><p>2.23.4.7a84c5d</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="rocm/megatron-lm:v25.6_py310" for="sd-tab-item-1">
<code class="docutils literal notranslate"><span class="pre">rocm/megatron-lm:v25.6_py310</span></code></label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Software component</p></th>
<th class="head"><p>Version</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ROCm</p></td>
<td><p>6.4.1</p></td>
</tr>
<tr class="row-odd"><td><p>PyTorch</p></td>
<td><p>2.8.0a0+git7d205b2</p></td>
</tr>
<tr class="row-even"><td><p>Python</p></td>
<td><p>3.10</p></td>
</tr>
<tr class="row-odd"><td><p>Transformer Engine</p></td>
<td><p>2.1.0.dev0+8c4a512</p></td>
</tr>
<tr class="row-even"><td><p>hipBLASLt</p></td>
<td><p>393e413</p></td>
</tr>
<tr class="row-odd"><td><p>Triton</p></td>
<td><p>3.3.0</p></td>
</tr>
<tr class="row-even"><td><p>RCCL</p></td>
<td><p>2.23.4.7a84c5d</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p id="amd-megatron-lm-model-support">The following models are pre-optimized for performance on AMD Instinct MI300X series accelerators.</p>
<section id="supported-models">
<h5>Supported models<a class="headerlink" href="#supported-models" title="Link to this heading">#</a></h5>
<p>The following models are supported for training performance benchmarking with Megatron-LM and ROCm.
Some instructions, commands, and training recommendations in this documentation might
vary by model – select one to get started.</p>
<div class="container-fluid" id="vllm-benchmark-ud-params-picker">
<div class="row">
<div class="col-2 me-2 model-param-head">Model</div>
<div class="row col-10">
<div class="col-3 model-param" data-param-k="model-group" data-param-v="llama" tabindex="0">Meta Llama</div>
<div class="col-3 model-param" data-param-k="model-group" data-param-v="deepseek" tabindex="0">DeepSeek</div>
<div class="col-3 model-param" data-param-k="model-group" data-param-v="mistral" tabindex="0">Mistral AI</div>
<div class="col-3 model-param" data-param-k="model-group" data-param-v="qwen" tabindex="0">Qwen</div>
</div>
</div>
<div class="row mt-1">
<div class="col-2 me-2 model-param-head">Model variant</div>
<div class="row col-10">
<div class="col-4 model-param" data-param-group="llama" data-param-k="model" data-param-v="pyt_megatron_lm_train_llama-3.3-70b" tabindex="0">Llama 3.3 70B</div>
<div class="col-4 model-param" data-param-group="llama" data-param-k="model" data-param-v="pyt_megatron_lm_train_llama-3.1-8b" tabindex="0">Llama 3.1 8B</div>
<div class="col-4 model-param" data-param-group="llama" data-param-k="model" data-param-v="pyt_megatron_lm_train_llama-3.1-70b" tabindex="0">Llama 3.1 70B</div>
<div class="col-4 model-param" data-param-group="llama" data-param-k="model" data-param-v="pyt_megatron_lm_train_llama-3.1-70b-proxy" tabindex="0">Llama 3.1 70B (proxy)</div>
<div class="col-4 model-param" data-param-group="llama" data-param-k="model" data-param-v="pyt_megatron_lm_train_llama-2-7b" tabindex="0">Llama 2 7B</div>
<div class="col-4 model-param" data-param-group="llama" data-param-k="model" data-param-v="pyt_megatron_lm_train_llama-2-70b" tabindex="0">Llama 2 70B</div>
<div class="col-6 model-param" data-param-group="deepseek" data-param-k="model" data-param-v="pyt_megatron_lm_train_deepseek-v3-proxy" tabindex="0">DeepSeek-V3 (proxy)</div>
<div class="col-6 model-param" data-param-group="deepseek" data-param-k="model" data-param-v="pyt_megatron_lm_train_deepseek-v2-lite-16b" tabindex="0">DeepSeek-V2-Lite</div>
<div class="col-6 model-param" data-param-group="mistral" data-param-k="model" data-param-v="pyt_megatron_lm_train_mixtral-8x7b" tabindex="0">Mixtral 8x7B</div>
<div class="col-6 model-param" data-param-group="mistral" data-param-k="model" data-param-v="pyt_megatron_lm_train_mixtral-8x22b-proxy" tabindex="0">Mixtral 8x22B (proxy)</div>
<div class="col-6 model-param" data-param-group="qwen" data-param-k="model" data-param-v="pyt_megatron_lm_train_qwen2.5-7b" tabindex="0">Qwen 2.5 7B</div>
<div class="col-6 model-param" data-param-group="qwen" data-param-k="model" data-param-v="pyt_megatron_lm_train_qwen2.5-72b" tabindex="0">Qwen 2.5 72B</div>
</div>
</div>
</div></section>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some models, such as Llama, require an external license agreement through
a third party (for example, Meta).</p>
</div>
<section id="performance-measurements">
<span id="amd-megatron-lm-performance-measurements"></span><h5>Performance measurements<a class="headerlink" href="#performance-measurements" title="Link to this heading">#</a></h5>
<p>To evaluate performance, the
<a class="reference external" href="https://www.amd.com/en/developer/resources/rocm-hub/dev-ai/performance-results.html#tabs-a8deaeb413-item-21cea50186-tab">Performance results with AMD ROCm software</a>
page provides reference throughput and latency measurements for training
popular AI models.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The performance data presented in
<a class="reference external" href="https://www.amd.com/en/developer/resources/rocm-hub/dev-ai/performance-results.html">Performance results with AMD ROCm software</a>
only reflects the latest version of this training benchmarking environment.
The listed measurements should not be interpreted as the peak performance achievable by AMD Instinct MI325X and MI300X accelerators or ROCm software.</p>
</div>
</section>
<section id="system-validation">
<h5>System validation<a class="headerlink" href="#system-validation" title="Link to this heading">#</a></h5>
<p>Before running AI workloads, it’s important to validate that your AMD hardware is configured
correctly and performing optimally.</p>
<p>If you have already validated your system settings, including aspects like NUMA auto-balancing, you
can skip this step. Otherwise, complete the procedures in the <a class="reference internal" href="#rocm-for-ai-system-optimization"><span class="std std-ref">System validation and
optimization</span></a> guide to properly configure your system settings
before starting training.</p>
<p>To test for optimal performance, consult the recommended <a class="reference internal" href="#rocm-for-ai-system-health-bench"><span class="std std-ref">System health benchmarks</span></a>. This suite of tests will help you verify and fine-tune your
system’s configuration.</p>
</section>
<section id="environment-setup">
<span id="mi300x-amd-megatron-lm-training"></span><h5>Environment setup<a class="headerlink" href="#environment-setup" title="Link to this heading">#</a></h5>
<p>Use the following instructions to set up the environment, configure the script to train models, and
reproduce the benchmark results on MI300X series accelerators with the AMD Megatron-LM Docker
image.</p>
<section id="download-the-docker-image">
<span id="amd-megatron-lm-requirements"></span><h6>Download the Docker image<a class="headerlink" href="#download-the-docker-image" title="Link to this heading">#</a></h6>
<ol class="arabic">
<li><p>Use the following command to pull the Docker image from Docker Hub.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-2" name="sd-tab-set-1" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="rocm/megatron-lm:v25.6_py312" for="sd-tab-item-2">
Ubuntu 24.04 + Python 3.12</label><div class="sd-tab-content docutils">
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/megatron-lm:v25.6_py312
</pre></div>
</div>
</div>
<input id="sd-tab-item-3" name="sd-tab-set-1" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="rocm/megatron-lm:v25.6_py310" for="sd-tab-item-3">
Ubuntu 22.04 + Python 3.10</label><div class="sd-tab-content docutils">
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/megatron-lm:v25.6_py310
</pre></div>
</div>
</div>
</div>
</li>
<li><p>Launch the Docker container.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-4" name="sd-tab-set-2" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="rocm/megatron-lm:v25.6_py312" for="sd-tab-item-4">
Ubuntu 24.04 + Python 3.12</label><div class="sd-tab-content docutils">
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>/dev/infiniband<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--network<span class="w"> </span>host<span class="w"> </span>--ipc<span class="w"> </span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="w"> </span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--privileged<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="nv">$HOME</span>:<span class="nv">$HOME</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="nv">$HOME</span>/.ssh:/root/.ssh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>128G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span>megatron_training_env<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/megatron-lm:v25.6_py312
</pre></div>
</div>
</div>
<input id="sd-tab-item-5" name="sd-tab-set-2" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="rocm/megatron-lm:v25.6_py310" for="sd-tab-item-5">
Ubuntu 22.04 + Python 3.10</label><div class="sd-tab-content docutils">
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>/dev/infiniband<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--network<span class="w"> </span>host<span class="w"> </span>--ipc<span class="w"> </span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="w"> </span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--privileged<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="nv">$HOME</span>:<span class="nv">$HOME</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="nv">$HOME</span>/.ssh:/root/.ssh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>128G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span>megatron_training_env<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/megatron-lm:v25.6_py310
</pre></div>
</div>
</div>
</div>
</li>
</ol>
<ol class="arabic" start="3">
<li><p>Use these commands if you exit the <code class="docutils literal notranslate"><span class="pre">megatron_training_env</span></code> container and need to return to it.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>start<span class="w"> </span>megatron_training_env
docker<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span>megatron_training_env<span class="w"> </span>bash
</pre></div>
</div>
</li>
</ol>
<p>The Docker container includes a pre-installed, verified version of the ROCm
Megatron-LM development branch
<a class="github reference external" href="https://github.com/ROCm/Megatron-LM/tree/rocm_dev">ROCm/Megatron-LM</a>, including necessary
training scripts.</p>
</section>
</section>
<section id="configuration">
<span id="amd-megatron-lm-environment-setup"></span><h5>Configuration<a class="headerlink" href="#configuration" title="Link to this heading">#</a></h5>
<div class="model-doc pyt-megatron-lm-train-llama-3-3-70b pyt-megatron-lm-train-llama-3-1-8b pyt-megatron-lm-train-llama-3-1-70b docutils container">
<p>Update the <code class="docutils literal notranslate"><span class="pre">train_llama3.sh</span></code> configuration script in the <code class="docutils literal notranslate"><span class="pre">examples/llama</span></code>
directory of
<a class="github reference external" href="https://github.com/ROCm/Megatron-LM/tree/rocm_dev/examples/llama">ROCm/Megatron-LM</a> to configure your training run.
Options can also be passed as command line arguments as described in <a class="reference internal" href="#amd-megatron-lm-run-training"><span class="std std-ref">Run training</span></a>.</p>
</div>
<div class="model-doc pyt-megatron-lm-train-llama-2-7b pyt-megatron-lm-train-llama-2-70b docutils container">
<p>Update the <code class="docutils literal notranslate"><span class="pre">train_llama2.sh</span></code> configuration script in the <code class="docutils literal notranslate"><span class="pre">examples/llama</span></code>
directory of
<a class="github reference external" href="https://github.com/ROCm/Megatron-LM/tree/rocm_dev/examples/llama">ROCm/Megatron-LM</a> to configure your training run.
Options can also be passed as command line arguments as described in <a class="reference internal" href="#amd-megatron-lm-run-training"><span class="std std-ref">Run training</span></a>.</p>
</div>
<div class="model-doc pyt-megatron-lm-train-deepseek-v3-proxy docutils container">
<p>Update the <code class="docutils literal notranslate"><span class="pre">train_deepseekv3.sh</span></code> configuration script in the <code class="docutils literal notranslate"><span class="pre">examples/deepseek_v3</span></code>
directory of
<a class="github reference external" href="https://github.com/ROCm/Megatron-LM/tree/rocm_dev/examples/deepseek_v3">ROCm/Megatron-LM</a> to configure your training run.
Options can also be passed as command line arguments as described in <a class="reference internal" href="#amd-megatron-lm-run-training"><span class="std std-ref">Run training</span></a>.</p>
</div>
<div class="model-doc pyt-megatron-lm-train-deepseek-v2-lite-16b docutils container">
<p>Update the <code class="docutils literal notranslate"><span class="pre">train_deepseekv2.sh</span></code> configuration script in the <code class="docutils literal notranslate"><span class="pre">examples/deepseek_v2</span></code>
directory of
<a class="github reference external" href="https://github.com/ROCm/Megatron-LM/tree/rocm_dev/examples/deepseek_v2">ROCm/Megatron-LM</a> to configure your training run.
Options can also be passed as command line arguments as described in <a class="reference internal" href="#amd-megatron-lm-run-training"><span class="std std-ref">Run training</span></a>.</p>
</div>
<div class="model-doc pyt-megatron-lm-train-mixtral-8x7b pyt-megatron-lm-train-mixtral-8x22b-proxy docutils container">
<p>Update the <code class="docutils literal notranslate"><span class="pre">train_mixtral_moe.sh</span></code> configuration script in the <code class="docutils literal notranslate"><span class="pre">examples/mixtral</span></code>
directory of
<a class="github reference external" href="https://github.com/ROCm/Megatron-LM/tree/rocm_dev/examples/mixtral">ROCm/Megatron-LM</a> to configure your training run.
Options can also be passed as command line arguments as described in <a class="reference internal" href="#amd-megatron-lm-run-training"><span class="std std-ref">Run training</span></a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference internal" href="#amd-megatron-lm-benchmark-test-vars"><span class="std std-ref">Key options</span></a> for more information on configuration options.</p>
</div>
<section id="network-interface">
<h6>Network interface<a class="headerlink" href="#network-interface" title="Link to this heading">#</a></h6>
<p>Update the network interface in the script to match your system’s network interface. To
find your network interface, run the following (outside of any Docker container):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ip<span class="w"> </span>a
</pre></div>
</div>
<p>Look for an active interface that has an IP address in the same subnet as
your other nodes. Then, update the following variables in the script, for
example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_SOCKET_IFNAME</span><span class="o">=</span>ens50f0np0

<span class="nb">export</span><span class="w"> </span><span class="nv">GLOO_SOCKET_IFNAME</span><span class="o">=</span>ens50f0np0
</pre></div>
</div>
</section>
<section id="tokenizer">
<span id="amd-megatron-lm-tokenizer"></span><h6>Tokenizer<a class="headerlink" href="#tokenizer" title="Link to this heading">#</a></h6>
<p>You can assign the path of an existing tokenizer to the <code class="docutils literal notranslate"><span class="pre">TOKENIZER_MODEL</span></code> as shown in the following examples.
If the tokenizer is not found, it’ll be downloaded if publicly available.</p>
<div class="model-doc pyt-megatron-lm-train-llama-3-3-70b docutils container">
<p>If you do not have Llama 3.3 tokenizer locally, you need to use your
personal Hugging Face access token <code class="docutils literal notranslate"><span class="pre">HF_TOKEN</span></code> to download the tokenizer.
See <a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct">Llama-3.3-70B-Instruct</a>. After you are
authorized, use your <code class="docutils literal notranslate"><span class="pre">HF_TOKEN</span></code> to download the tokenizer and set the
variable <code class="docutils literal notranslate"><span class="pre">TOKENIZER_MODEL</span></code> to the tokenizer path.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">HF_TOKEN</span><span class="o">=</span>&lt;Your<span class="w"> </span>personal<span class="w"> </span>Hugging<span class="w"> </span>Face<span class="w"> </span>access<span class="w"> </span>token&gt;
</pre></div>
</div>
<p>The training script uses the <code class="docutils literal notranslate"><span class="pre">HuggingFaceTokenizer</span></code>. Set <code class="docutils literal notranslate"><span class="pre">TOKENIZER_MODEL</span></code> to the appropriate Hugging Face model path.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">TOKENIZER_MODEL</span><span class="o">=</span><span class="s2">"meta-llama/Llama-3.3-70B-Instruct"</span>
</pre></div>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-llama-3-1-8b docutils container">
<p>The training script uses the <code class="docutils literal notranslate"><span class="pre">HuggingFaceTokenizer</span></code>. Set <code class="docutils literal notranslate"><span class="pre">TOKENIZER_MODEL</span></code> to the appropriate Hugging Face model path.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">TOKENIZER_MODEL</span><span class="o">=</span><span class="s2">"meta-llama/Llama-3.1-8B"</span>
</pre></div>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-llama-3-1-70b docutils container">
<p>The training script uses the <code class="docutils literal notranslate"><span class="pre">HuggingFaceTokenizer</span></code>. Set <code class="docutils literal notranslate"><span class="pre">TOKENIZER_MODEL</span></code> to the appropriate Hugging Face model path.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">TOKENIZER_MODEL</span><span class="o">=</span><span class="s2">"meta-llama/Llama-3.1-70B"</span>
</pre></div>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-llama-2-7b pyt-megatron-lm-train-llama-2-70b docutils container">
<p>The training script uses either the <code class="docutils literal notranslate"><span class="pre">Llama2Tokenizer</span></code> or <code class="docutils literal notranslate"><span class="pre">HuggingFaceTokenizer</span></code> by default.</p>
</div>
<div class="model-doc pyt-megatron-lm-train-deepseek-v3-proxy docutils container">
<p>The training script uses the <code class="docutils literal notranslate"><span class="pre">HuggingFaceTokenizer</span></code>. Set <code class="docutils literal notranslate"><span class="pre">TOKENIZER_MODEL</span></code> to the appropriate Hugging Face model path.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">TOKENIZER_MODEL</span><span class="o">=</span><span class="s2">"deepseek-ai/DeepSeek-V3"</span>
</pre></div>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-deepseek-v2-lite-16b docutils container">
<p>The training script uses the <code class="docutils literal notranslate"><span class="pre">HuggingFaceTokenizer</span></code>. Set <code class="docutils literal notranslate"><span class="pre">TOKENIZER_MODEL</span></code> to the appropriate Hugging Face model path.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">TOKENIZER_MODEL</span><span class="o">=</span><span class="s2">"deepseek-ai/DeepSeek-V2-Lite"</span>
</pre></div>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-mixtral-8x7b pyt-megatron-lm-train-mixtral-8x22b-proxy docutils container">
<p>Download the Mixtral tokenizer.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>tokenizer
<span class="nb">cd</span><span class="w"> </span>tokenizer
<span class="nb">export</span><span class="w"> </span><span class="nv">HF_TOKEN</span><span class="o">=</span>&lt;Your<span class="w"> </span>personal<span class="w"> </span>Hugging<span class="w"> </span>Face<span class="w"> </span>access<span class="w"> </span>token&gt;
wget<span class="w"> </span>--header<span class="o">=</span><span class="s2">"Authorization: Bearer </span><span class="nv">$HF_TOKEN</span><span class="s2">"</span><span class="w"> </span>-O<span class="w"> </span>./tokenizer.model<span class="w"> </span>https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/tokenizer.model
</pre></div>
</div>
<p>Use the <code class="docutils literal notranslate"><span class="pre">HuggingFaceTokenizer</span></code>. Set <code class="docutils literal notranslate"><span class="pre">TOKENIZER_MODEL</span></code> to the appropriate Hugging Face model path.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">TOKENIZER_MODEL</span><span class="o">=</span>tokenizer/tokenizer.model
</pre></div>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-qwen2-5-7b docutils container">
<p>The training script uses the <code class="docutils literal notranslate"><span class="pre">HuggingFaceTokenizer</span></code>. Set <code class="docutils literal notranslate"><span class="pre">TOKENIZER_MODEL</span></code> to the appropriate Hugging Face model path.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">TOKENIZER_MODEL</span><span class="o">=</span><span class="s2">"Qwen/Qwen2.5-7B"</span>
</pre></div>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-qwen2-5-72b docutils container">
<p>The training script uses the <code class="docutils literal notranslate"><span class="pre">HuggingFaceTokenizer</span></code>. Set <code class="docutils literal notranslate"><span class="pre">TOKENIZER_MODEL</span></code> to the appropriate Hugging Face model path.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">TOKENIZER_MODEL</span><span class="o">=</span><span class="s2">"Qwen/Qwen2.5-72B"</span>
</pre></div>
</div>
</div>
</section>
<section id="dataset-options">
<h6>Dataset options<a class="headerlink" href="#dataset-options" title="Link to this heading">#</a></h6>
<p>You can use either mock data or real data for training.</p>
<ul>
<li><p>Mock data can be useful for testing and validation. Use the <code class="docutils literal notranslate"><span class="pre">MOCK_DATA</span></code> variable to toggle between mock and real data. The default
value is <code class="docutils literal notranslate"><span class="pre">1</span></code> for enabled.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">MOCK_DATA</span><span class="o">=</span><span class="m">1</span>
</pre></div>
</div>
</li>
<li><p>If you’re using a real dataset, update the <code class="docutils literal notranslate"><span class="pre">DATA_PATH</span></code> variable to point to the location of your dataset.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">MOCK_DATA</span><span class="o">=</span><span class="m">0</span>

<span class="nv">DATA_PATH</span><span class="o">=</span><span class="s2">"/data/bookcorpus_text_sentence"</span><span class="w">  </span><span class="c1"># Change to where your dataset is stored</span>
</pre></div>
</div>
<p>Ensure that the files are accessible inside the Docker container.</p>
</li>
</ul>
<section id="download-the-dataset">
<h6 aria-level="7">Download the dataset<a class="headerlink" href="#download-the-dataset" title="Link to this heading">#</a></h6>
<div class="model-doc pyt-megatron-lm-train-llama-3-3-70b pyt-megatron-lm-train-llama-3-1-8b pyt-megatron-lm-train-llama-3-1-70b pyt-megatron-lm-train-llama-2-7b pyt-megatron-lm-train-llama-2-70b pyt-megatron-lm-train-llama-3-1-70b-proxy docutils container">
<p>For Llama models, use the <a class="reference external" href="https://github.com/ROCm/Megatron-LM/tree/rocm_dev/examples/llama">prepare_dataset.sh</a> script
to prepare your dataset.
To download the dataset, set the <code class="docutils literal notranslate"><span class="pre">DATASET</span></code> variable to the dataset you’d
like to use. Three datasets are supported: <code class="docutils literal notranslate"><span class="pre">DATASET=wiki</span></code>, <code class="docutils literal notranslate"><span class="pre">DATASET=fineweb</span></code>, and
<code class="docutils literal notranslate"><span class="pre">DATASET=bookcorpus</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">DATASET</span><span class="o">=</span>wiki<span class="w"> </span><span class="nv">TOKENIZER_MODEL</span><span class="o">=</span>NousResearch/Llama-2-7b-chat-hf<span class="w"> </span>bash<span class="w"> </span>examples/llama/prepare_dataset.sh<span class="w"> </span><span class="c1">#for wiki-en dataset</span>
<span class="nv">DATASET</span><span class="o">=</span>bookcorpus<span class="w"> </span><span class="nv">TOKENIZER_MODEL</span><span class="o">=</span>NousResearch/Llama-2-7b-chat-hf<span class="w"> </span>bash<span class="w"> </span>examples/llama/prepare_dataset.sh<span class="w"> </span><span class="c1">#for bookcorpus dataset</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">TOKENIZER_MODEL</span></code> can be any accessible Hugging Face tokenizer.
Remember to either pre-download the tokenizer or setup Hugging Face access
otherwise when needed – see the <a class="reference internal" href="#amd-megatron-lm-tokenizer"><span class="std std-ref">Tokenizer</span></a> section.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When training set <code class="docutils literal notranslate"><span class="pre">DATA_PATH</span></code> to the specific file name prefix pointing to the <code class="docutils literal notranslate"><span class="pre">.bin</span></code> or <code class="docutils literal notranslate"><span class="pre">.idx</span></code>
as in the following example:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="s2">"data/bookcorpus_text_sentence"</span><span class="w"> </span><span class="c1"># Change to where your dataset is stored.</span>
</pre></div>
</div>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-deepseek-v3-proxy docutils container">
<p>If you don’t already have the dataset, download the DeepSeek dataset using the following
commands:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>deepseek-datasets
<span class="nb">cd</span><span class="w"> </span>deepseek-datasets
wget<span class="w"> </span>https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/deepseek-datasets/SlimPajama.json
wget<span class="w"> </span>https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/deepseek-datasets/alpaca_zh-train.json
wget<span class="w"> </span>https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/deepseek-datasets/alpaca_zh-valid.json
<span class="nb">cd</span><span class="w"> </span>..
bash<span class="w"> </span>tools/run_make_pretraining_dataset_megatron.sh<span class="w"> </span>deepseek-datasets/SlimPajama.json<span class="w"> </span>DeepSeekV3Tokenizer<span class="w"> </span>text<span class="w"> </span>deepseek-datasets<span class="w"> </span>deepseek-ai/DeepSeek-V3
</pre></div>
</div>
<p>To train on this data, update the <code class="docutils literal notranslate"><span class="pre">DATA_DIR</span></code> variable to point to the location of your dataset.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">MOCK_DATA</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="c1"># Train on real data</span>

<span class="nv">DATA_DIR</span><span class="o">=</span><span class="s2">"&lt;path-to&gt;/deepseek-datasets"</span><span class="w">  </span><span class="c1"># Change to where your dataset is stored</span>

Ensure<span class="w"> </span>that<span class="w"> </span>the<span class="w"> </span>files<span class="w"> </span>are<span class="w"> </span>accessible<span class="w"> </span>inside<span class="w"> </span>the<span class="w"> </span>Docker<span class="w"> </span>container.
</pre></div>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-deepseek-v2-lite-16b docutils container">
<p>If you don’t already have the dataset, download the DeepSeek dataset using the following
commands:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>deepseek-datasets
<span class="nb">cd</span><span class="w"> </span>deepseek-datasets
wget<span class="w"> </span>https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/deepseek-datasets/SlimPajama.json
wget<span class="w"> </span>https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/deepseek-datasets/alpaca_zh-train.json
wget<span class="w"> </span>https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/deepseek-datasets/alpaca_zh-valid.json
<span class="nb">cd</span><span class="w"> </span>..
bash<span class="w"> </span>tools/run_make_pretraining_dataset_megatron.sh<span class="w"> </span>deepseek-datasets/SlimPajama.json<span class="w"> </span>DeepSeekV3Tokenizer<span class="w"> </span>text<span class="w"> </span>deepseek-datasets<span class="w"> </span>deepseek-ai/DeepSeek-V3
</pre></div>
</div>
<p>To train on this data, update the <code class="docutils literal notranslate"><span class="pre">DATA_DIR</span></code> variable to point to the location of your dataset.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">MOCK_DATA</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="c1"># Train on real data</span>

<span class="nv">DATA_DIR</span><span class="o">=</span><span class="s2">"&lt;path-to&gt;/deepseek-datasets"</span><span class="w">  </span><span class="c1"># Change to where your dataset is stored</span>
</pre></div>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-mixtral-8x7b pyt-megatron-lm-train-mixtral-8x22b-proxy docutils container">
<p>If you don’t already have the dataset, download the Mixtral dataset using the following
commands:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>mixtral-datasets
<span class="nb">cd</span><span class="w"> </span>mixtral-datasets
wget<span class="w"> </span>https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/mistral-datasets/wudao_mistralbpe_content_document.bin
wget<span class="w"> </span>https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/mistral-datasets/wudao_mistralbpe_content_document.idx
</pre></div>
</div>
<p>To train on this data, update the <code class="docutils literal notranslate"><span class="pre">DATA_DIR</span></code> variable to point to the location of your dataset.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">MOCK_DATA</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="c1"># Train on real data</span>

<span class="nv">DATA_DIR</span><span class="o">=</span><span class="s2">"&lt;path-to&gt;/mixtral-datasets"</span><span class="w">  </span><span class="c1"># Change to where your dataset is stored</span>
</pre></div>
</div>
<p>Ensure that the files are accessible inside the Docker container.</p>
</div>
<div class="model-doc pyt-megatron-lm-train-qwen2-5-7b pyt-megatron-lm-train-qwen2-5-72b docutils container">
<p>If you don’t already have the dataset, download the Mixtral dataset using the following
commands:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>temp/qwen-datasets
wget<span class="w"> </span>https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/qwen-datasets/wudao_qwenbpe_text_document.bin
wget<span class="w"> </span>https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/qwen-datasets/wudao_qwenbpe_text_document.idx
</pre></div>
</div>
<p>To train on this data, update the <code class="docutils literal notranslate"><span class="pre">DATA_DIR</span></code> variable to point to the location of your dataset.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">MOCK_DATA</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="c1"># Train on real data</span>

<span class="nv">DATA_DIR</span><span class="o">=</span><span class="s2">"&lt;path-to&gt;/qwen-datasets"</span><span class="w">  </span><span class="c1"># Change to where your dataset is stored</span>
</pre></div>
</div>
<p>Ensure that the files are accessible inside the Docker container.</p>
</div>
</section>
</section>
<section id="multi-node-configuration">
<h6>Multi-node configuration<a class="headerlink" href="#multi-node-configuration" title="Link to this heading">#</a></h6>
<p>If you’re running multi-node training, update the following environment variables. They can
also be passed as command line arguments. Refer to the following example configurations.</p>
<ul>
<li><p>Change <code class="docutils literal notranslate"><span class="pre">localhost</span></code> to the master node’s hostname:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">MASTER_ADDR</span><span class="o">=</span><span class="s2">"</span><span class="si">${</span><span class="nv">MASTER_ADDR</span><span class="k">:-</span><span class="nv">localhost</span><span class="si">}</span><span class="s2">"</span>
</pre></div>
</div>
</li>
<li><p>Set the number of nodes you want to train on (for instance, <code class="docutils literal notranslate"><span class="pre">2</span></code>, <code class="docutils literal notranslate"><span class="pre">4</span></code>, <code class="docutils literal notranslate"><span class="pre">8</span></code>):</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">NNODES</span><span class="o">=</span><span class="s2">"</span><span class="si">${</span><span class="nv">NNODES</span><span class="k">:-</span><span class="nv">1</span><span class="si">}</span><span class="s2">"</span>
</pre></div>
</div>
</li>
<li><p>Set the rank of each node (0 for master, 1 for the first worker node, and so on):</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">NODE_RANK</span><span class="o">=</span><span class="s2">"</span><span class="si">${</span><span class="nv">NODE_RANK</span><span class="k">:-</span><span class="nv">0</span><span class="si">}</span><span class="s2">"</span>
</pre></div>
</div>
</li>
<li><p>Set <code class="docutils literal notranslate"><span class="pre">DATA_CACHE_PATH</span></code> to a common directory accessible by all the nodes (for example, an
NFS directory) for multi-node runs:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">DATA_CACHE_PATH</span><span class="o">=</span>/root/cache<span class="w"> </span><span class="c1"># Set to a common directory for multi-node runs</span>
</pre></div>
</div>
</li>
<li><p>For multi-node runs, make sure the correct network drivers are installed on the nodes. If
inside a Docker container, either install the drivers inside the Docker container or pass the network
drivers from the host while creating the Docker container.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Specify which RDMA interfaces to use for communication</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_IB_HCA</span><span class="o">=</span>rdma0,rdma1,rdma2,rdma3,rdma4,rdma5,rdma6,rdma7
</pre></div>
</div>
</li>
</ul>
</section>
</section>
<section id="run-training">
<span id="amd-megatron-lm-run-training"></span><h5>Run training<a class="headerlink" href="#run-training" title="Link to this heading">#</a></h5>
<p>Use the following example commands to set up the environment, configure
<a class="reference internal" href="#amd-megatron-lm-benchmark-test-vars"><span class="std std-ref">key options</span></a>, and run training on
MI300X series accelerators with the AMD Megatron-LM environment.</p>
<section id="single-node-training">
<h6>Single node training<a class="headerlink" href="#single-node-training" title="Link to this heading">#</a></h6>
<div class="model-doc pyt-megatron-lm-train-llama-3-3-70b docutils container">
<p>To run the training on a single node for Llama 3.3 70B BF16 with FSDP-v2 enabled, add the <code class="docutils literal notranslate"><span class="pre">FSDP=1</span></code> argument.
For example, use the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">TOKENIZER_MODEL</span><span class="o">=</span>meta-llama/Llama-3.3-70B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="nv">CKPT_FORMAT</span><span class="o">=</span>torch_dist<span class="w"> </span><span class="se">\</span>
<span class="nv">TEE_OUTPUT</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">RECOMPUTE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">SEQ_LENGTH</span><span class="o">=</span><span class="m">8192</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MBS</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="nv">BS</span><span class="o">=</span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TE_FP8</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">PP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">FSDP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MODEL_SIZE</span><span class="o">=</span><span class="m">70</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TOTAL_ITERS</span><span class="o">=</span><span class="m">50</span><span class="w"> </span><span class="se">\</span>
bash<span class="w"> </span>examples/llama/train_llama3.sh
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is suggested to use <code class="docutils literal notranslate"><span class="pre">TP=1</span></code> when FSDP is enabled for higher
throughput. FSDP-v2 is not supported with pipeline parallelism, expert
parallelism, MCore’s distributed optimizer, gradient accumulation fusion,
or FP16.</p>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-llama-3-1-8b docutils container">
<p>To run training on a single node for Llama 3.1 8B FP8, navigate to the Megatron-LM folder and use the
following command.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">TEE_OUTPUT</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MBS</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="nv">BS</span><span class="o">=</span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TE_FP8</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">SEQ_LENGTH</span><span class="o">=</span><span class="m">8192</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MODEL_SIZE</span><span class="o">=</span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TOTAL_ITERS</span><span class="o">=</span><span class="m">50</span><span class="w"> </span><span class="se">\</span>
bash<span class="w"> </span>examples/llama/train_llama3.sh
</pre></div>
</div>
<p>For Llama 3.1 8B BF16, use the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">TEE_OUTPUT</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MBS</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="nv">BS</span><span class="o">=</span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TE_FP8</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="nv">SEQ_LENGTH</span><span class="o">=</span><span class="m">8192</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MODEL_SIZE</span><span class="o">=</span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TOTAL_ITERS</span><span class="o">=</span><span class="m">50</span><span class="w"> </span><span class="se">\</span>
bash<span class="w"> </span>examples/llama/train_llama3.sh
</pre></div>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-llama-3-1-70b docutils container">
<p>To run the training on a single node for Llama 3.1 70B BF16 with FSDP-v2 enabled, add the <code class="docutils literal notranslate"><span class="pre">FSDP=1</span></code> argument.
For example, use the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CKPT_FORMAT</span><span class="o">=</span>torch_dist<span class="w"> </span><span class="se">\</span>
<span class="nv">TEE_OUTPUT</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MBS</span><span class="o">=</span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="nv">BS</span><span class="o">=</span><span class="m">24</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TE_FP8</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="nv">FSDP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">RECOMPUTE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">SEQ_LENGTH</span><span class="o">=</span><span class="m">8192</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MODEL_SIZE</span><span class="o">=</span><span class="m">70</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TOTAL_ITERS</span><span class="o">=</span><span class="m">50</span><span class="w"> </span><span class="se">\</span>
bash<span class="w"> </span>examples/llama/train_llama3.sh
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is suggested to use <code class="docutils literal notranslate"><span class="pre">TP=1</span></code> when FSDP is enabled for higher
throughput. FSDP-v2 is not supported with pipeline parallelism, expert
parallelism, MCore’s distributed optimizer, gradient accumulation fusion,
or FP16.</p>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-llama-3-1-70b-proxy docutils container">
<p>To run the training on a single node for Llama 3.1 70B with proxy, use the following command.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CKPT_FORMAT</span><span class="o">=</span>torch_dist<span class="w"> </span><span class="se">\</span>
<span class="nv">TEE_OUTPUT</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">RECOMPUTE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MBS</span><span class="o">=</span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="nv">BS</span><span class="o">=</span><span class="m">24</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TE_FP8</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">SEQ_LENGTH</span><span class="o">=</span><span class="m">8192</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MODEL_SIZE</span><span class="o">=</span><span class="m">70</span><span class="w"> </span><span class="se">\</span>
<span class="nv">FSDP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TOTAL_ITERS</span><span class="o">=</span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="nv">NUM_LAYERS</span><span class="o">=</span><span class="m">40</span><span class="w"> </span><span class="se">\</span>
bash<span class="w"> </span>examples/llama/train_llama3.sh
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Use two or more nodes to run the <em>full</em> Llama 70B model with FP8 precision.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is suggested to use <code class="docutils literal notranslate"><span class="pre">TP=1</span></code> when FSDP is enabled for higher
throughput. FSDP-v2 is not supported with pipeline parallelism, expert
parallelism, MCore’s distributed optimizer, gradient accumulation fusion,
or FP16.</p>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-llama-2-7b docutils container">
<p>To run training on a single node for Llama 2 7B FP8, navigate to the Megatron-LM folder and use the
following command.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">TEE_OUTPUT</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MBS</span><span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="nv">BS</span><span class="o">=</span><span class="m">256</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TE_FP8</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">SEQ_LENGTH</span><span class="o">=</span><span class="m">4096</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MODEL_SIZE</span><span class="o">=</span><span class="m">7</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TOTAL_ITERS</span><span class="o">=</span><span class="m">50</span><span class="w"> </span><span class="se">\</span>
bash<span class="w"> </span>examples/llama/train_llama2.sh
</pre></div>
</div>
<p>For Llama 2 7B BF16, use the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">TEE_OUTPUT</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MBS</span><span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="nv">BS</span><span class="o">=</span><span class="m">256</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TE_FP8</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="nv">SEQ_LENGTH</span><span class="o">=</span><span class="m">4096</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MODEL_SIZE</span><span class="o">=</span><span class="m">7</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TOTAL_ITERS</span><span class="o">=</span><span class="m">50</span><span class="w"> </span><span class="se">\</span>
bash<span class="w"> </span>examples/llama/train_llama2.sh
</pre></div>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-llama-2-70b docutils container">
<p>To run the training on a single node for Llama 2 70B BF16 with FSDP-v2 enabled, add the <code class="docutils literal notranslate"><span class="pre">FSDP=1</span></code> argument.
For example, use the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CKPT_FORMAT</span><span class="o">=</span>torch_dist<span class="w"> </span><span class="se">\</span>
<span class="nv">TEE_OUTPUT</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MBS</span><span class="o">=</span><span class="m">7</span><span class="w"> </span><span class="se">\</span>
<span class="nv">BS</span><span class="o">=</span><span class="m">56</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TE_FP8</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="nv">FSDP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">RECOMPUTE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">SEQ_LENGTH</span><span class="o">=</span><span class="m">4096</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MODEL_SIZE</span><span class="o">=</span><span class="m">70</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TOTAL_ITERS</span><span class="o">=</span><span class="m">50</span><span class="w"> </span><span class="se">\</span>
bash<span class="w"> </span>examples/llama/train_llama2.sh
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is suggested to use <code class="docutils literal notranslate"><span class="pre">TP=1</span></code> when FSDP is enabled for higher
throughput. FSDP-v2 is not supported with pipeline parallelism, expert
parallelism, MCore’s distributed optimizer, gradient accumulation fusion,
or FP16.</p>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-deepseek-v3-proxy docutils container">
<p>To run training on a single node for DeepSeek-V3 (MoE with expert parallel) with 3-layer proxy,
navigate to the Megatron-LM folder and use the following command.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">NVTE_FUSED_ATTN_CK</span><span class="o">=</span><span class="m">0</span>
<span class="nv">FORCE_BALANCE</span><span class="o">=</span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="nv">RUN_ENV</span><span class="o">=</span>cluster<span class="w"> </span><span class="se">\</span>
<span class="nv">MODEL_SIZE</span><span class="o">=</span>671B<span class="w"> </span><span class="se">\</span>
<span class="nv">TRAIN_ITERS</span><span class="o">=</span><span class="m">50</span><span class="w"> </span><span class="se">\</span>
<span class="nv">SEQ_LEN</span><span class="o">=</span><span class="m">4096</span><span class="w"> </span><span class="se">\</span>
<span class="nv">NUM_LAYERS</span><span class="o">=</span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MICRO_BATCH_SIZE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">GLOBAL_BATCH_SIZE</span><span class="o">=</span><span class="m">32</span><span class="w"> </span><span class="se">\</span>
<span class="nv">PR</span><span class="o">=</span>bf16<span class="w"> </span><span class="se">\</span>
<span class="nv">TP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">PP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">ETP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">EP</span><span class="o">=</span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="nv">GEMM_TUNING</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">NVTE_CK_USES_BWD_V3</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">USE_GROUPED_GEMM</span><span class="o">=</span><span class="nb">true</span><span class="w"> </span><span class="nv">MOE_USE_LEGACY_GROUPED_GEMM</span><span class="o">=</span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="nv">GPT_LAYER_IN_TE</span><span class="o">=</span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
bash<span class="w"> </span>examples/deepseek_v3/train_deepseekv3.sh
</pre></div>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-deepseek-v2-lite-16b docutils container">
<p>To run training on a single node for DeepSeek-V2-Lite (MoE with expert parallel),
navigate to the Megatron-LM folder and use the following command.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">NVTE_FUSED_ATTN_CK</span><span class="o">=</span><span class="m">0</span>
<span class="nv">GEMM_TUNING</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">PR</span><span class="o">=</span>bf16<span class="w"> </span><span class="se">\</span>
<span class="nv">MBS</span><span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="nv">AC</span><span class="o">=</span>none<span class="w"> </span><span class="se">\</span>
<span class="nv">SEQ_LEN</span><span class="o">=</span><span class="m">4096</span><span class="w"> </span><span class="se">\</span>
<span class="nv">PAD_LEN</span><span class="o">=</span><span class="m">4096</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TRAIN_ITERS</span><span class="o">=</span><span class="m">50</span><span class="w"> </span><span class="se">\</span>
bash<span class="w"> </span>examples/deepseek_v2/train_deepseekv2.sh
</pre></div>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-mixtral-8x7b docutils container">
<p>To run training on a single node for Mixtral 8x7B (MoE with expert parallel),
navigate to the Megatron-LM folder and use the following command.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">TOKENIZER_MODEL</span><span class="o">=</span>&lt;path/to/tokenizer/model&gt;
<span class="nv">RECOMPUTE_NUM_LAYERS</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TEE_OUTPUT</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MBS</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">GBS</span><span class="o">=</span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TP_SIZE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">PP_SIZE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">AC</span><span class="o">=</span>none<span class="w"> </span><span class="se">\</span>
<span class="nv">PR</span><span class="o">=</span>bf16<span class="w"> </span><span class="se">\</span>
<span class="nv">EP_SIZE</span><span class="o">=</span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="nv">ETP_SIZE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">SEQLEN</span><span class="o">=</span><span class="m">4096</span><span class="w"> </span><span class="se">\</span>
<span class="nv">FORCE_BALANCE</span><span class="o">=</span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MOCK_DATA</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">RUN_ENV</span><span class="o">=</span>cluster<span class="w"> </span><span class="se">\</span>
<span class="nv">MODEL_SIZE</span><span class="o">=</span>8x7B<span class="w"> </span><span class="se">\</span>
<span class="nv">TRAIN_ITERS</span><span class="o">=</span><span class="m">50</span><span class="w"> </span><span class="se">\</span>
bash<span class="w"> </span>examples/mixtral/train_mixtral_moe.sh
</pre></div>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-mixtral-8x22b-proxy docutils container">
<p>To run training on a single node for Mixtral 8x7B (MoE with expert parallel) with 4-layer proxy,
navigate to the Megatron-LM folder and use the following command.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">TOKENIZER_MODEL</span><span class="o">=</span>&lt;path/to/tokenizer/model&gt;
<span class="nv">RECOMPUTE_NUM_LAYERS</span><span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TEE_OUTPUT</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MBS</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">GBS</span><span class="o">=</span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TP_SIZE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">PP_SIZE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">AC</span><span class="o">=</span>full<span class="w"> </span><span class="se">\</span>
<span class="nv">NUM_LAYERS</span><span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="nv">PR</span><span class="o">=</span>bf16<span class="w"> </span><span class="se">\</span>
<span class="nv">EP_SIZE</span><span class="o">=</span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="nv">ETP_SIZE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">SEQLEN</span><span class="o">=</span><span class="m">8192</span><span class="w"> </span><span class="se">\</span>
<span class="nv">FORCE_BALANCE</span><span class="o">=</span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MOCK_DATA</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">RUN_ENV</span><span class="o">=</span>cluster<span class="w"> </span><span class="se">\</span>
<span class="nv">MODEL_SIZE</span><span class="o">=</span>8x22B<span class="w"> </span><span class="se">\</span>
<span class="nv">TRAIN_ITERS</span><span class="o">=</span><span class="m">50</span><span class="w"> </span><span class="se">\</span>
bash<span class="w"> </span>examples/mixtral/train_mixtral_moe.sh
</pre></div>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-qwen2-5-7b docutils container">
<p>To run training on a single node for Qwen 2.5 7B BF16, use the following
command.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>examples/qwen/train_qwen2.sh<span class="w"> </span><span class="nv">TP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">CP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">PP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">MBS</span><span class="o">=</span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">BS</span><span class="o">=</span><span class="m">640</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">TE_FP8</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">MODEL_SIZE</span><span class="o">=</span><span class="m">7</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">SEQ_LENGTH</span><span class="o">=</span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">TOTAL_ITERS</span><span class="o">=</span><span class="m">50</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">MOCK_DATA</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">TOKENIZER_MODEL</span><span class="o">=</span>Qwen/Qwen2.5-7B
</pre></div>
</div>
<p>For FP8, use the following command.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>examples/qwen/train_qwen2.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">TP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">CP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">PP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">MBS</span><span class="o">=</span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">BS</span><span class="o">=</span><span class="m">640</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">TE_FP8</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">MODEL_SIZE</span><span class="o">=</span><span class="m">7</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">SEQ_LENGTH</span><span class="o">=</span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">TOTAL_ITERS</span><span class="o">=</span><span class="m">50</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">MOCK_DATA</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">TOKENIZER_MODEL</span><span class="o">=</span>Qwen/Qwen2.5-7B
</pre></div>
</div>
</div>
<div class="model-doc pyt-megatron-lm-train-qwen2-5-72b docutils container">
<p>To run the training on a single node for Qwen 2.5 72B BF16, use the following command.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>examples/qwen/train_qwen2.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">FSDP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">CP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">PP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">MBS</span><span class="o">=</span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">BS</span><span class="o">=</span><span class="m">24</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">TE_FP8</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">MODEL_SIZE</span><span class="o">=</span><span class="m">72</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">SEQ_LENGTH</span><span class="o">=</span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">TOTAL_ITERS</span><span class="o">=</span><span class="m">50</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">MOCK_DATA</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">TOKENIZER_MODEL</span><span class="o">=</span>Qwen/Qwen2.5-72B<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">RECOMPUTE_ACTIVATIONS</span><span class="o">=</span>full<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">CKPT_FORMAT</span><span class="o">=</span>torch_dist
</pre></div>
</div>
</div>
</section>
<section id="multi-node-training-examples">
<h6>Multi-node training examples<a class="headerlink" href="#multi-node-training-examples" title="Link to this heading">#</a></h6>
<p>To run training on multiple nodes, launch the Docker container on each node.
For example, for Llama 3 using a two node setup (<code class="docutils literal notranslate"><span class="pre">NODE0</span></code> as the master node),
use these commands.</p>
<ul>
<li><p>On the master node <code class="docutils literal notranslate"><span class="pre">NODE0</span></code>:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">TEE_OUTPUT</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MBS</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="nv">BS</span><span class="o">=</span><span class="m">256</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TE_FP8</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">SEQ_LENGTH</span><span class="o">=</span><span class="m">8192</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MODEL_SIZE</span><span class="o">=</span><span class="m">8</span><span class="w">  </span><span class="se">\</span>
<span class="nv">MASTER_ADDR</span><span class="o">=</span>IP_NODE0<span class="w"> </span><span class="se">\</span>
<span class="nv">NNODES</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="nv">NODE_RANK</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
bash<span class="w"> </span>examples/llama/train_llama3.sh
</pre></div>
</div>
</li>
<li><p>On the worker node <code class="docutils literal notranslate"><span class="pre">NODE1</span></code>:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">TEE_OUTPUT</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MBS</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="nv">BS</span><span class="o">=</span><span class="m">256</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TP</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">TE_FP8</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="nv">SEQ_LENGTH</span><span class="o">=</span><span class="m">8192</span><span class="w"> </span><span class="se">\</span>
<span class="nv">MODEL_SIZE</span><span class="o">=</span><span class="m">8</span><span class="w">  </span><span class="se">\</span>
<span class="nv">MASTER_ADDR</span><span class="o">=</span>IP_NODE0<span class="w"> </span><span class="se">\</span>
<span class="nv">NNODES</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="nv">NODE_RANK</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
bash<span class="w"> </span>examples/llama/train_llama3.sh
</pre></div>
</div>
</li>
</ul>
<p>Or, for DeepSeek-V3, an example script <code class="docutils literal notranslate"><span class="pre">train_deepseek_v3_slurm.sh</span></code> is
provided in
<a class="github reference external" href="https://github.com/ROCm/Megatron-LM/tree/rocm_dev/examples/deepseek_v3">ROCm/Megatron-LM</a> to
enable training at scale under a SLURM environment. For example, to run
training on 16 nodes, try the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sbatch<span class="w"> </span>examples/deepseek_v3/train_deepseek_v3_slurm.sh
</pre></div>
</div>
</section>
<section id="key-options">
<span id="amd-megatron-lm-benchmark-test-vars"></span><h6>Key options<a class="headerlink" href="#key-options" title="Link to this heading">#</a></h6>
<p>The benchmark tests support the following sets of variables.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">TEE_OUTPUT</span></code></dt><dd><p><code class="docutils literal notranslate"><span class="pre">1</span></code> to enable training logs or <code class="docutils literal notranslate"><span class="pre">0</span></code> to disable.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">TE_FP8</span></code></dt><dd><p><code class="docutils literal notranslate"><span class="pre">0</span></code> for B16 or <code class="docutils literal notranslate"><span class="pre">1</span></code> for FP8 – <code class="docutils literal notranslate"><span class="pre">0</span></code> by default.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">GEMM_TUNING</span></code></dt><dd><p><code class="docutils literal notranslate"><span class="pre">1</span></code> to enable GEMM tuning, which boosts performance by using the best GEMM kernels.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">USE_FLASH_ATTN</span></code></dt><dd><p><code class="docutils literal notranslate"><span class="pre">1</span></code> to enable Flash Attention.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">FSDP</span></code></dt><dd><p><code class="docutils literal notranslate"><span class="pre">1</span></code> to enable PyTorch FSDP2. If FSDP is enabled, <code class="docutils literal notranslate"><span class="pre">--use-distributed-optimizer</span></code>,
<code class="docutils literal notranslate"><span class="pre">--overlap-param-gather</span></code>, and <code class="docutils literal notranslate"><span class="pre">--sequence-parallel</span></code> are automatically disabled.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">ENABLE_PROFILING</span></code></dt><dd><p><code class="docutils literal notranslate"><span class="pre">1</span></code> to enable PyTorch profiling for performance analysis.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">transformer-impl</span></code></dt><dd><p><code class="docutils literal notranslate"><span class="pre">transformer_engine</span></code> to use the Transformer Engine (TE) or <code class="docutils literal notranslate"><span class="pre">local</span></code> to disable TE.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">MODEL_SIZE</span></code></dt><dd><p><code class="docutils literal notranslate"><span class="pre">8B</span></code> or <code class="docutils literal notranslate"><span class="pre">70B</span></code> for Llama 3 and 3.1. <code class="docutils literal notranslate"><span class="pre">7B</span></code> or <code class="docutils literal notranslate"><span class="pre">70B</span></code> for Llama 2, for example.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">TOTAL_ITERS</span></code></dt><dd><p>The total number of iterations – <code class="docutils literal notranslate"><span class="pre">10</span></code> by default.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">MOCK_DATA</span></code></dt><dd><p><code class="docutils literal notranslate"><span class="pre">1</span></code> to use mock data or <code class="docutils literal notranslate"><span class="pre">0</span></code> to use real data you provide.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">MBS</span></code></dt><dd><p>Micro batch size.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">BS</span></code></dt><dd><p>Global batch size.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">TP</span></code> / <code class="docutils literal notranslate"><span class="pre">TP_SIZE</span></code></dt><dd><p>Tensor parallel (<code class="docutils literal notranslate"><span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">2</span></code>, <code class="docutils literal notranslate"><span class="pre">4</span></code>, <code class="docutils literal notranslate"><span class="pre">8</span></code>). <code class="docutils literal notranslate"><span class="pre">TP</span></code> is disabled when <code class="docutils literal notranslate"><span class="pre">FSDP</span></code> is turned on.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">EP</span></code> / <code class="docutils literal notranslate"><span class="pre">EP_SIZE</span></code></dt><dd><p>Expert parallel for MoE models.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">SEQ_LENGTH</span></code></dt><dd><p>Input sequence length.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">PR</span></code></dt><dd><p>Precision for training. <code class="docutils literal notranslate"><span class="pre">bf16</span></code> for BF16 (default) or <code class="docutils literal notranslate"><span class="pre">fp8</span></code> for FP8 GEMMs.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">AC</span></code></dt><dd><p>Activation checkpointing (<code class="docutils literal notranslate"><span class="pre">none</span></code>, <code class="docutils literal notranslate"><span class="pre">sel</span></code>, or <code class="docutils literal notranslate"><span class="pre">full</span></code>) – <code class="docutils literal notranslate"><span class="pre">sel</span></code> by default.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">NUM_LAYERS</span></code></dt><dd><p>Use reduced number of layers as a proxy model.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">RECOMPUTE_NUM_LAYERS</span></code></dt><dd><p>Number of layers used for checkpointing recompute.</p>
</dd>
</dl>
</section>
</section>
<section id="previous-versions">
<h5>Previous versions<a class="headerlink" href="#previous-versions" title="Link to this heading">#</a></h5>
<p>See <a class="reference internal" href="#document-how-to/rocm-for-ai/training/benchmark-docker/previous-versions/megatron-lm-history"><span class="doc">Megatron-LM training performance testing version history</span></a> to find documentation for previous releases
of the <code class="docutils literal notranslate"><span class="pre">ROCm/megatron-lm</span></code> Docker image.</p>
</section>
</section>
<span id="document-how-to/rocm-for-ai/training/benchmark-docker/pytorch-training"></span><section id="training-a-model-with-pytorch-for-rocm">
<h4>Training a model with PyTorch for ROCm<a class="headerlink" href="#training-a-model-with-pytorch-for-rocm" title="Link to this heading">#</a></h4>
<p>PyTorch is an open-source machine learning framework that is widely used for
model training with GPU-optimized components for transformer-based models.</p>
<p>The <a class="reference external" href="https://hub.docker.com/r/rocm/pytorch-training/tags">PyTorch for ROCm training Docker</a>
(<code class="docutils literal notranslate"><span class="pre">rocm/pytorch-training:v25.6</span></code>) image provides a prebuilt optimized environment for fine-tuning and pretraining a
model on AMD Instinct MI325X and MI300X accelerators. It includes the following software components to accelerate
training workloads:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Software component</p></th>
<th class="head"><p>Version</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ROCm</p></td>
<td><p>6.3.4</p></td>
</tr>
<tr class="row-odd"><td><p>PyTorch</p></td>
<td><p>2.8.0a0+git7d205b2</p></td>
</tr>
<tr class="row-even"><td><p>Python</p></td>
<td><p>3.10.17</p></td>
</tr>
<tr class="row-odd"><td><p>Transformer Engine</p></td>
<td><p>1.14.0+2f85f5f2</p></td>
</tr>
<tr class="row-even"><td><p>Flash Attention</p></td>
<td><p>3.0.0.post1</p></td>
</tr>
<tr class="row-odd"><td><p>hipBLASLt</p></td>
<td><p>0.15.0-8c6919d</p></td>
</tr>
<tr class="row-even"><td><p>Triton</p></td>
<td><p>3.3.0</p></td>
</tr>
</tbody>
</table>
</div>
<section id="supported-models">
<span id="amd-pytorch-training-model-support"></span><h5>Supported models<a class="headerlink" href="#supported-models" title="Link to this heading">#</a></h5>
<p>The following models are pre-optimized for performance on the AMD Instinct MI325X and MI300X accelerators.</p>
<div class="container-fluid" id="vllm-benchmark-ud-params-picker">
<div class="row">
<div class="col-2 me-2 model-param-head">Workload</div>
<div class="row col-10">
<div class="col-6 model-param" data-param-k="model-group" data-param-v="pre-training" tabindex="0">Pre-training</div>
<div class="col-6 model-param" data-param-k="model-group" data-param-v="fine-tuning" tabindex="0">Fine-tuning</div>
</div>
</div>
<div class="row mt-1">
<div class="col-2 me-2 model-param-head">Model</div>
<div class="row col-10">
<div class="col-4 model-param" data-param-group="pre-training" data-param-k="model" data-param-v="pyt_train_llama-3.1-8b" tabindex="0">Llama 3.1 8B</div>
<div class="col-4 model-param" data-param-group="pre-training" data-param-k="model" data-param-v="pyt_train_llama-3.1-70b" tabindex="0">Llama 3.1 70B</div>
<div class="col-4 model-param" data-param-group="pre-training" data-param-k="model" data-param-v="pyt_train_flux" tabindex="0">FLUX.1-dev</div>
<div class="col-6 model-param" data-param-group="fine-tuning" data-param-k="model" data-param-v="pyt_train_llama-4-scout-17b-16e" tabindex="0">Llama 4 Scout 17B-16E</div>
<div class="col-6 model-param" data-param-group="fine-tuning" data-param-k="model" data-param-v="pyt_train_llama-3.3-70b" tabindex="0">Llama 3.3 70B</div>
<div class="col-6 model-param" data-param-group="fine-tuning" data-param-k="model" data-param-v="pyt_train_llama-3.2-1b" tabindex="0">Llama 3.2 1B</div>
<div class="col-6 model-param" data-param-group="fine-tuning" data-param-k="model" data-param-v="pyt_train_llama-3.2-3b" tabindex="0">Llama 3.2 3B</div>
<div class="col-6 model-param" data-param-group="fine-tuning" data-param-k="model" data-param-v="pyt_train_llama-3.2-vision-11b" tabindex="0">Llama 3.2 Vision 11B</div>
<div class="col-6 model-param" data-param-group="fine-tuning" data-param-k="model" data-param-v="pyt_train_llama-3.2-vision-90b" tabindex="0">Llama 3.2 Vision 90B</div>
<div class="col-6 model-param" data-param-group="fine-tuning" data-param-k="model" data-param-v="pyt_train_llama-3.1-8b" tabindex="0">Llama 3.1 8B</div>
<div class="col-6 model-param" data-param-group="fine-tuning" data-param-k="model" data-param-v="pyt_train_llama-3.1-70b" tabindex="0">Llama 3.1 70B</div>
<div class="col-6 model-param" data-param-group="fine-tuning" data-param-k="model" data-param-v="pyt_train_llama-3.1-405b" tabindex="0">Llama 3.1 405B</div>
<div class="col-6 model-param" data-param-group="fine-tuning" data-param-k="model" data-param-v="pyt_train_llama-3-8b" tabindex="0">Llama 3 8B</div>
<div class="col-6 model-param" data-param-group="fine-tuning" data-param-k="model" data-param-v="pyt_train_llama-3-70b" tabindex="0">Llama 3 70B</div>
<div class="col-6 model-param" data-param-group="fine-tuning" data-param-k="model" data-param-v="pyt_train_llama-2-7b" tabindex="0">Llama 2 7B</div>
<div class="col-6 model-param" data-param-group="fine-tuning" data-param-k="model" data-param-v="pyt_train_llama-2-13b" tabindex="0">Llama 2 13B</div>
<div class="col-6 model-param" data-param-group="fine-tuning" data-param-k="model" data-param-v="pyt_train_llama-2-70b" tabindex="0">Llama 2 70B</div>
</div>
</div>
</div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some models require an external license agreement through a third party (for example, Meta).</p>
</div>
<section id="performance-measurements">
<span id="amd-pytorch-training-performance-measurements"></span><h6>Performance measurements<a class="headerlink" href="#performance-measurements" title="Link to this heading">#</a></h6>
<p>To evaluate performance, the
<a class="reference external" href="https://www.amd.com/en/developer/resources/rocm-hub/dev-ai/performance-results.html#tabs-a8deaeb413-item-21cea50186-tab">Performance results with AMD ROCm software</a>
page provides reference throughput and latency measurements for training
popular AI models.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The performance data presented in
<a class="reference external" href="https://www.amd.com/en/developer/resources/rocm-hub/dev-ai/performance-results.html#tabs-a8deaeb413-item-21cea50186-tab">Performance results with AMD ROCm software</a>
should not be interpreted as the peak performance achievable by AMD
Instinct MI325X and MI300X accelerators or ROCm software.</p>
</div>
</section>
<section id="system-validation">
<h6>System validation<a class="headerlink" href="#system-validation" title="Link to this heading">#</a></h6>
<p>Before running AI workloads, it’s important to validate that your AMD hardware is configured
correctly and performing optimally.</p>
<p>If you have already validated your system settings, including aspects like NUMA auto-balancing, you
can skip this step. Otherwise, complete the procedures in the <a class="reference internal" href="#rocm-for-ai-system-optimization"><span class="std std-ref">System validation and
optimization</span></a> guide to properly configure your system settings
before starting training.</p>
<p>To test for optimal performance, consult the recommended <a class="reference internal" href="#rocm-for-ai-system-health-bench"><span class="std std-ref">System health benchmarks</span></a>. This suite of tests will help you verify and fine-tune your
system’s configuration.</p>
<p>This Docker image is optimized for specific model configurations outlined
below. Performance can vary for other training workloads, as AMD
doesn’t validate configurations and run conditions outside those described.</p>
</section>
<section id="benchmarking">
<h6>Benchmarking<a class="headerlink" href="#benchmarking" title="Link to this heading">#</a></h6>
<p>Once the setup is complete, choose between two options to start benchmarking:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-6" name="sd-tab-set-3" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-6">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
<div class="model-doc pyt-train-llama-3-1-8b docutils container">
<p>For example, use this command to run the performance benchmark test on the Llama 3.1 8B model
using one GPU with the BF16 data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_train_llama-3.1-8b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_train_llama-3.1-8b</span></code>, for example. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/perf.csv</span></code>.</p>
</div>
<div class="model-doc pyt-train-llama-3-1-70b docutils container">
<p>For example, use this command to run the performance benchmark test on the Llama 3.1 70B model
using one GPU with the BF16 data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_train_llama-3.1-70b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_train_llama-3.1-70b</span></code>, for example. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/perf.csv</span></code>.</p>
</div>
<div class="model-doc pyt-train-flux docutils container">
<p>For example, use this command to run the performance benchmark test on the FLUX.1-dev model
using one GPU with the BF16 data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_train_flux<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_train_flux</span></code>, for example. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/perf.csv</span></code>.</p>
</div>
<div class="model-doc pyt-train-llama-4-scout-17b-16e docutils container">
<p>For example, use this command to run the performance benchmark test on the Llama 4 Scout 17B-16E model
using one GPU with the BF16 data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_train_llama-4-scout-17b-16e<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_train_llama-4-scout-17b-16e</span></code>, for example. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/perf.csv</span></code>.</p>
</div>
<div class="model-doc pyt-train-llama-3-3-70b docutils container">
<p>For example, use this command to run the performance benchmark test on the Llama 3.3 70B model
using one GPU with the BF16 data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_train_llama-3.3-70b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_train_llama-3.3-70b</span></code>, for example. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/perf.csv</span></code>.</p>
</div>
<div class="model-doc pyt-train-llama-3-2-1b docutils container">
<p>For example, use this command to run the performance benchmark test on the Llama 3.2 1B model
using one GPU with the BF16 data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_train_llama-3.2-1b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_train_llama-3.2-1b</span></code>, for example. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/perf.csv</span></code>.</p>
</div>
<div class="model-doc pyt-train-llama-3-2-3b docutils container">
<p>For example, use this command to run the performance benchmark test on the Llama 3.2 3B model
using one GPU with the BF16 data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_train_llama-3.2-3b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_train_llama-3.2-3b</span></code>, for example. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/perf.csv</span></code>.</p>
</div>
<div class="model-doc pyt-train-llama-3-2-vision-11b docutils container">
<p>For example, use this command to run the performance benchmark test on the Llama 3.2 Vision 11B model
using one GPU with the BF16 data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_train_llama-3.2-vision-11b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_train_llama-3.2-vision-11b</span></code>, for example. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/perf.csv</span></code>.</p>
</div>
<div class="model-doc pyt-train-llama-3-2-vision-90b docutils container">
<p>For example, use this command to run the performance benchmark test on the Llama 3.2 Vision 90B model
using one GPU with the BF16 data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_train_llama-3.2-vision-90b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_train_llama-3.2-vision-90b</span></code>, for example. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/perf.csv</span></code>.</p>
</div>
<div class="model-doc pyt-train-llama-3-1-8b docutils container">
<p>For example, use this command to run the performance benchmark test on the Llama 3.1 8B model
using one GPU with the BF16 data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_train_llama-3.1-8b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_train_llama-3.1-8b</span></code>, for example. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/perf.csv</span></code>.</p>
</div>
<div class="model-doc pyt-train-llama-3-1-70b docutils container">
<p>For example, use this command to run the performance benchmark test on the Llama 3.1 70B model
using one GPU with the BF16 data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_train_llama-3.1-70b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_train_llama-3.1-70b</span></code>, for example. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/perf.csv</span></code>.</p>
</div>
<div class="model-doc pyt-train-llama-3-1-405b docutils container">
<p>For example, use this command to run the performance benchmark test on the Llama 3.1 405B model
using one GPU with the BF16 data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_train_llama-3.1-405b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_train_llama-3.1-405b</span></code>, for example. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/perf.csv</span></code>.</p>
</div>
<div class="model-doc pyt-train-llama-3-8b docutils container">
<p>For example, use this command to run the performance benchmark test on the Llama 3 8B model
using one GPU with the BF16 data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_train_llama-3-8b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_train_llama-3-8b</span></code>, for example. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/perf.csv</span></code>.</p>
</div>
<div class="model-doc pyt-train-llama-3-70b docutils container">
<p>For example, use this command to run the performance benchmark test on the Llama 3 70B model
using one GPU with the BF16 data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_train_llama-3-70b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_train_llama-3-70b</span></code>, for example. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/perf.csv</span></code>.</p>
</div>
<div class="model-doc pyt-train-llama-2-7b docutils container">
<p>For example, use this command to run the performance benchmark test on the Llama 2 7B model
using one GPU with the BF16 data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_train_llama-2-7b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_train_llama-2-7b</span></code>, for example. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/perf.csv</span></code>.</p>
</div>
<div class="model-doc pyt-train-llama-2-13b docutils container">
<p>For example, use this command to run the performance benchmark test on the Llama 2 13B model
using one GPU with the BF16 data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_train_llama-2-13b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_train_llama-2-13b</span></code>, for example. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/perf.csv</span></code>.</p>
</div>
<div class="model-doc pyt-train-llama-2-70b docutils container">
<p>For example, use this command to run the performance benchmark test on the Llama 2 70B model
using one GPU with the BF16 data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_train_llama-2-70b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_train_llama-2-70b</span></code>, for example. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/perf.csv</span></code>.</p>
</div>
</div>
<input id="sd-tab-item-7" name="sd-tab-set-3" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-7">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required packages</p>
<p>Use the following command to pull the Docker image from Docker Hub.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/pytorch-training:v25.6
</pre></div>
</div>
<p>Run the Docker container.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>--device<span class="w"> </span>/dev/dri<span class="w"> </span>--device<span class="w"> </span>/dev/kfd<span class="w"> </span>--network<span class="w"> </span>host<span class="w"> </span>--ipc<span class="w"> </span>host<span class="w"> </span>--group-add<span class="w"> </span>video<span class="w"> </span>--cap-add<span class="w"> </span>SYS_PTRACE<span class="w"> </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span>--privileged<span class="w"> </span>-v<span class="w"> </span><span class="nv">$HOME</span>:<span class="nv">$HOME</span><span class="w"> </span>-v<span class="w">  </span><span class="nv">$HOME</span>/.ssh:/root/.ssh<span class="w"> </span>--shm-size<span class="w"> </span>64G<span class="w"> </span>--name<span class="w"> </span>training_env<span class="w"> </span>rocm/pytorch-training:v25.6
</pre></div>
</div>
<p>Use these commands if you exit the <code class="docutils literal notranslate"><span class="pre">training_env</span></code> container and need to return to it.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>start<span class="w"> </span>training_env
docker<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span>training_env<span class="w"> </span>bash
</pre></div>
</div>
<p>In the Docker container, clone the <a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>
repository and navigate to the benchmark scripts directory
<code class="docutils literal notranslate"><span class="pre">/workspace/MAD/scripts/pytorch_train</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/pytorch_train
</pre></div>
</div>
<p class="rubric">Prepare training datasets and dependencies</p>
<p>The following benchmarking examples require downloading models and datasets
from Hugging Face. To ensure successful access to gated repos, set your
<code class="docutils literal notranslate"><span class="pre">HF_TOKEN</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">HF_TOKEN</span><span class="o">=</span><span class="nv">$your_personal_hugging_face_access_token</span>
</pre></div>
</div>
<p>Run the setup script to install libraries and datasets needed for benchmarking.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./pytorch_benchmark_setup.sh
</pre></div>
</div>
<div class="model-doc pyt-train-llama-3-1-8b docutils container">
<p><code class="docutils literal notranslate"><span class="pre">pytorch_benchmark_setup.sh</span></code> installs the following libraries for Llama 3.1 8B:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Library</p></th>
<th class="head"><p>Reference</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">accelerate</span></code></p></td>
<td><p><a class="reference external" href="https://huggingface.co/docs/accelerate/en/index">Hugging Face Accelerate</a></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">datasets</span></code></p></td>
<td><p><a class="reference external" href="https://huggingface.co/docs/datasets/v3.2.0/en/index">Hugging Face Datasets</a> 3.2.0</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="model-doc pyt-train-llama-3-1-70b docutils container">
<p><code class="docutils literal notranslate"><span class="pre">pytorch_benchmark_setup.sh</span></code> installs the following libraries for Llama 3.1 70B:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Library</p></th>
<th class="head"><p>Reference</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">datasets</span></code></p></td>
<td><p><a class="reference external" href="https://huggingface.co/docs/datasets/v3.2.0/en/index">Hugging Face Datasets</a> 3.2.0</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">torchdata</span></code></p></td>
<td><p><a class="reference external" href="https://pytorch.org/data/beta/index.html">TorchData</a></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">tomli</span></code></p></td>
<td><p><a class="reference external" href="https://pypi.org/project/tomli/">Tomli</a></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">tiktoken</span></code></p></td>
<td><p><a class="reference external" href="https://github.com/openai/tiktoken">tiktoken</a></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">blobfile</span></code></p></td>
<td><p><a class="reference external" href="https://pypi.org/project/blobfile/">blobfile</a></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">tabulate</span></code></p></td>
<td><p><a class="reference external" href="https://pypi.org/project/tabulate/">tabulate</a></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">wandb</span></code></p></td>
<td><p><a class="reference external" href="https://github.com/wandb/wandb">Weights &amp; Biases</a></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">sentencepiece</span></code></p></td>
<td><p><a class="reference external" href="https://github.com/google/sentencepiece">SentencePiece</a> 0.2.0</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">tensorboard</span></code></p></td>
<td><p><a class="reference external" href="https://www.tensorflow.org/tensorboard">TensorBoard</a> 2.18.0</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="model-doc pyt-train-flux docutils container">
<p><code class="docutils literal notranslate"><span class="pre">pytorch_benchmark_setup.sh</span></code> installs the following libraries for FLUX:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Library</p></th>
<th class="head"><p>Reference</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">accelerate</span></code></p></td>
<td><p><a class="reference external" href="https://huggingface.co/docs/accelerate/en/index">Hugging Face Accelerate</a></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">datasets</span></code></p></td>
<td><p><a class="reference external" href="https://huggingface.co/docs/datasets/v3.2.0/en/index">Hugging Face Datasets</a> 3.2.0</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">sentencepiece</span></code></p></td>
<td><p><a class="reference external" href="https://github.com/google/sentencepiece">SentencePiece</a> 0.2.0</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">tensorboard</span></code></p></td>
<td><p><a class="reference external" href="https://www.tensorflow.org/tensorboard">TensorBoard</a> 2.18.0</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">csvkit</span></code></p></td>
<td><p><a class="reference external" href="https://csvkit.readthedocs.io/en/latest/">csvkit</a> 2.0.1</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">deepspeed</span></code></p></td>
<td><p><a class="reference external" href="https://github.com/deepspeedai/DeepSpeed">DeepSpeed</a> 0.16.2</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">diffusers</span></code></p></td>
<td><p><a class="reference external" href="https://huggingface.co/docs/diffusers/en/index">Hugging Face Diffusers</a> 0.31.0</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">GitPython</span></code></p></td>
<td><p><a class="reference external" href="https://github.com/gitpython-developers/GitPython">GitPython</a> 3.1.44</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">opencv-python-headless</span></code></p></td>
<td><p><a class="reference external" href="https://pypi.org/project/opencv-python-headless/">opencv-python-headless</a> 4.10.0.84</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">peft</span></code></p></td>
<td><p><a class="reference external" href="https://huggingface.co/docs/peft/en/index">PEFT</a> 0.14.0</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">protobuf</span></code></p></td>
<td><p><a class="reference external" href="https://github.com/protocolbuffers/protobuf">Protocol Buffers</a> 5.29.2</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">pytest</span></code></p></td>
<td><p><a class="reference external" href="https://docs.pytest.org/en/stable/">PyTest</a> 8.3.4</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">python-dotenv</span></code></p></td>
<td><p><a class="reference external" href="https://pypi.org/project/python-dotenv/">python-dotenv</a> 1.0.1</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">seaborn</span></code></p></td>
<td><p><a class="reference external" href="https://seaborn.pydata.org/">Seaborn</a> 0.13.2</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">transformers</span></code></p></td>
<td><p><a class="reference external" href="https://huggingface.co/docs/transformers/en/index">Transformers</a> 4.47.0</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">pytorch_benchmark_setup.sh</span></code> downloads the following datasets from Hugging Face:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/datasets/bghira/pseudo-camera-10k">bghira/pseudo-camera-10k</a></p></li>
</ul>
<div class="model-doc pyt-train-llama-3-1-8b docutils container">
<p class="rubric">Pretraining</p>
<p>To start the pre-training benchmark, use the following command with the
appropriate options. See the following list of options and their descriptions.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./pytorch_benchmark_report.sh<span class="w"> </span>-t<span class="w"> </span>pretrain<span class="w"> </span>-m<span class="w"> </span>Llama-3.1-8B<span class="w"> </span>-p<span class="w"> </span><span class="nv">$datatype</span><span class="w"> </span>-s<span class="w"> </span><span class="nv">$sequence_length</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Options</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BF16</span></code> or <code class="docutils literal notranslate"><span class="pre">FP8</span></code></p></td>
<td><p>Only Llama 3.1 8B supports FP8 precision.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$sequence_length</span></code></p></td>
<td><p>Sequence length for the language model.</p></td>
<td><p>Between 2048 and 8192. 8192 by default.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="model-doc pyt-train-llama-3-1-70b docutils container">
<p class="rubric">Pretraining</p>
<p>To start the pre-training benchmark, use the following command with the
appropriate options. See the following list of options and their descriptions.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./pytorch_benchmark_report.sh<span class="w"> </span>-t<span class="w"> </span>pretrain<span class="w"> </span>-m<span class="w"> </span>Llama-3.1-70B<span class="w"> </span>-p<span class="w"> </span><span class="nv">$datatype</span><span class="w"> </span>-s<span class="w"> </span><span class="nv">$sequence_length</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Options</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BF16</span></code></p></td>
<td><p>Only Llama 3.1 8B supports FP8 precision.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$sequence_length</span></code></p></td>
<td><p>Sequence length for the language model.</p></td>
<td><p>Between 2048 and 8192. 8192 by default.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="model-doc pyt-train-flux docutils container">
<p class="rubric">Pretraining</p>
<p>To start the pre-training benchmark, use the following command with the
appropriate options. See the following list of options and their descriptions.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./pytorch_benchmark_report.sh<span class="w"> </span>-t<span class="w"> </span>pretrain<span class="w"> </span>-m<span class="w"> </span>Flux<span class="w"> </span>-p<span class="w"> </span><span class="nv">$datatype</span><span class="w"> </span>-s<span class="w"> </span><span class="nv">$sequence_length</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Options</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BF16</span></code></p></td>
<td><p>Only Llama 3.1 8B supports FP8 precision.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$sequence_length</span></code></p></td>
<td><p>Sequence length for the language model.</p></td>
<td><p>Between 2048 and 8192. 8192 by default.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="model-doc pyt-train-flux docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Occasionally, downloading the Flux dataset might fail. In the event of this
error, manually download it from Hugging Face at
<a class="reference external" href="https://huggingface.co/black-forest-labs/FLUX.1-dev">black-forest-labs/FLUX.1-dev</a>
and save it to <cite>/workspace/FluxBenchmark</cite>. This ensures that the test script can access
the required dataset.</p>
</div>
</div>
</div>
<div class="model-doc pyt-train-llama-4-scout-17b-16e docutils container">
<p class="rubric">Fine-tuning</p>
<p>To start the fine-tuning benchmark, use the following command with the
appropriate options. See the following list of options and their descriptions.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./pytorch_benchmark_report.sh<span class="w"> </span>-t<span class="w"> </span><span class="nv">$training_mode</span><span class="w"> </span>-m<span class="w"> </span>Llama-4-17B_16E<span class="w"> </span>-p<span class="w"> </span>BF16<span class="w"> </span>-s<span class="w"> </span><span class="nv">$sequence_length</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Options</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$training_mode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></td>
<td><p>Full weight fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_qlora</span></code></p></td>
<td><p>QLoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">HF_finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning with Hugging Face PEFT</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BF16</span></code></p></td>
<td><p>All models support BF16.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$sequence_length</span></code></p></td>
<td><p>Between 2048 and 16384.</p></td>
<td><p>Sequence length for the language model.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Llama 4 Scout 17B-16E currently supports the following fine-tuning methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></li>
</ul>
<p>The upstream <a class="reference external" href="https://github.com/pytorch/torchtune">torchtune</a> repository
does not currently provide YAML configuration files for other combinations of
model to fine-tuning method
However, you can still configure your own YAML files to enable support for
fine-tuning methods not listed here by following existing patterns in the
<code class="docutils literal notranslate"><span class="pre">/workspace/torchtune/recipes/configs</span></code> directory.</p>
</div>
</div>
<div class="model-doc pyt-train-llama-3-3-70b docutils container">
<p class="rubric">Fine-tuning</p>
<p>To start the fine-tuning benchmark, use the following command with the
appropriate options. See the following list of options and their descriptions.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./pytorch_benchmark_report.sh<span class="w"> </span>-t<span class="w"> </span><span class="nv">$training_mode</span><span class="w"> </span>-m<span class="w"> </span>Llama-3.3-70B<span class="w"> </span>-p<span class="w"> </span>BF16<span class="w"> </span>-s<span class="w"> </span><span class="nv">$sequence_length</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Options</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$training_mode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></td>
<td><p>Full weight fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_qlora</span></code></p></td>
<td><p>QLoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">HF_finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning with Hugging Face PEFT</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BF16</span></code></p></td>
<td><p>All models support BF16.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$sequence_length</span></code></p></td>
<td><p>Between 2048 and 16384.</p></td>
<td><p>Sequence length for the language model.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Llama 3.3 70B currently supports the following fine-tuning methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_qlora</span></code></p></li>
</ul>
<p>The upstream <a class="reference external" href="https://github.com/pytorch/torchtune">torchtune</a> repository
does not currently provide YAML configuration files for other combinations of
model to fine-tuning method
However, you can still configure your own YAML files to enable support for
fine-tuning methods not listed here by following existing patterns in the
<code class="docutils literal notranslate"><span class="pre">/workspace/torchtune/recipes/configs</span></code> directory.</p>
</div>
</div>
<div class="model-doc pyt-train-llama-3-2-1b docutils container">
<p class="rubric">Fine-tuning</p>
<p>To start the fine-tuning benchmark, use the following command with the
appropriate options. See the following list of options and their descriptions.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./pytorch_benchmark_report.sh<span class="w"> </span>-t<span class="w"> </span><span class="nv">$training_mode</span><span class="w"> </span>-m<span class="w"> </span>Llama-3.2-1B<span class="w"> </span>-p<span class="w"> </span>BF16<span class="w"> </span>-s<span class="w"> </span><span class="nv">$sequence_length</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Options</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$training_mode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></td>
<td><p>Full weight fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_qlora</span></code></p></td>
<td><p>QLoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">HF_finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning with Hugging Face PEFT</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BF16</span></code></p></td>
<td><p>All models support BF16.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$sequence_length</span></code></p></td>
<td><p>Between 2048 and 16384.</p></td>
<td><p>Sequence length for the language model.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Llama 3.2 1B currently supports the following fine-tuning methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></li>
</ul>
<p>The upstream <a class="reference external" href="https://github.com/pytorch/torchtune">torchtune</a> repository
does not currently provide YAML configuration files for other combinations of
model to fine-tuning method
However, you can still configure your own YAML files to enable support for
fine-tuning methods not listed here by following existing patterns in the
<code class="docutils literal notranslate"><span class="pre">/workspace/torchtune/recipes/configs</span></code> directory.</p>
</div>
</div>
<div class="model-doc pyt-train-llama-3-2-3b docutils container">
<p class="rubric">Fine-tuning</p>
<p>To start the fine-tuning benchmark, use the following command with the
appropriate options. See the following list of options and their descriptions.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./pytorch_benchmark_report.sh<span class="w"> </span>-t<span class="w"> </span><span class="nv">$training_mode</span><span class="w"> </span>-m<span class="w"> </span>Llama-3.2-3B<span class="w"> </span>-p<span class="w"> </span>BF16<span class="w"> </span>-s<span class="w"> </span><span class="nv">$sequence_length</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Options</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$training_mode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></td>
<td><p>Full weight fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_qlora</span></code></p></td>
<td><p>QLoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">HF_finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning with Hugging Face PEFT</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BF16</span></code></p></td>
<td><p>All models support BF16.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$sequence_length</span></code></p></td>
<td><p>Between 2048 and 16384.</p></td>
<td><p>Sequence length for the language model.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Llama 3.2 3B currently supports the following fine-tuning methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></li>
</ul>
<p>The upstream <a class="reference external" href="https://github.com/pytorch/torchtune">torchtune</a> repository
does not currently provide YAML configuration files for other combinations of
model to fine-tuning method
However, you can still configure your own YAML files to enable support for
fine-tuning methods not listed here by following existing patterns in the
<code class="docutils literal notranslate"><span class="pre">/workspace/torchtune/recipes/configs</span></code> directory.</p>
</div>
</div>
<div class="model-doc pyt-train-llama-3-2-vision-11b docutils container">
<p class="rubric">Fine-tuning</p>
<p>To start the fine-tuning benchmark, use the following command with the
appropriate options. See the following list of options and their descriptions.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./pytorch_benchmark_report.sh<span class="w"> </span>-t<span class="w"> </span><span class="nv">$training_mode</span><span class="w"> </span>-m<span class="w"> </span>Llama-3.2-Vision-11B<span class="w"> </span>-p<span class="w"> </span>BF16<span class="w"> </span>-s<span class="w"> </span><span class="nv">$sequence_length</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Options</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$training_mode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></td>
<td><p>Full weight fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_qlora</span></code></p></td>
<td><p>QLoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">HF_finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning with Hugging Face PEFT</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BF16</span></code></p></td>
<td><p>All models support BF16.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$sequence_length</span></code></p></td>
<td><p>Between 2048 and 16384.</p></td>
<td><p>Sequence length for the language model.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Llama 3.2 Vision 11B currently supports the following fine-tuning methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></li>
</ul>
<p>The upstream <a class="reference external" href="https://github.com/pytorch/torchtune">torchtune</a> repository
does not currently provide YAML configuration files for other combinations of
model to fine-tuning method
However, you can still configure your own YAML files to enable support for
fine-tuning methods not listed here by following existing patterns in the
<code class="docutils literal notranslate"><span class="pre">/workspace/torchtune/recipes/configs</span></code> directory.</p>
</div>
</div>
<div class="model-doc pyt-train-llama-3-2-vision-90b docutils container">
<p class="rubric">Fine-tuning</p>
<p>To start the fine-tuning benchmark, use the following command with the
appropriate options. See the following list of options and their descriptions.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./pytorch_benchmark_report.sh<span class="w"> </span>-t<span class="w"> </span><span class="nv">$training_mode</span><span class="w"> </span>-m<span class="w"> </span>Llama-3.2-Vision-90B<span class="w"> </span>-p<span class="w"> </span>BF16<span class="w"> </span>-s<span class="w"> </span><span class="nv">$sequence_length</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Options</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$training_mode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></td>
<td><p>Full weight fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_qlora</span></code></p></td>
<td><p>QLoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">HF_finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning with Hugging Face PEFT</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BF16</span></code></p></td>
<td><p>All models support BF16.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$sequence_length</span></code></p></td>
<td><p>Between 2048 and 16384.</p></td>
<td><p>Sequence length for the language model.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Llama 3.2 Vision 90B currently supports the following fine-tuning methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></li>
</ul>
<p>The upstream <a class="reference external" href="https://github.com/pytorch/torchtune">torchtune</a> repository
does not currently provide YAML configuration files for other combinations of
model to fine-tuning method
However, you can still configure your own YAML files to enable support for
fine-tuning methods not listed here by following existing patterns in the
<code class="docutils literal notranslate"><span class="pre">/workspace/torchtune/recipes/configs</span></code> directory.</p>
</div>
</div>
<div class="model-doc pyt-train-llama-3-1-8b docutils container">
<p class="rubric">Fine-tuning</p>
<p>To start the fine-tuning benchmark, use the following command with the
appropriate options. See the following list of options and their descriptions.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./pytorch_benchmark_report.sh<span class="w"> </span>-t<span class="w"> </span><span class="nv">$training_mode</span><span class="w"> </span>-m<span class="w"> </span>Llama-3.1-8B<span class="w"> </span>-p<span class="w"> </span>BF16<span class="w"> </span>-s<span class="w"> </span><span class="nv">$sequence_length</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Options</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$training_mode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></td>
<td><p>Full weight fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_qlora</span></code></p></td>
<td><p>QLoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">HF_finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning with Hugging Face PEFT</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BF16</span></code></p></td>
<td><p>All models support BF16.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$sequence_length</span></code></p></td>
<td><p>Between 2048 and 16384.</p></td>
<td><p>Sequence length for the language model.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Llama 3.1 8B currently supports the following fine-tuning methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></li>
</ul>
<p>The upstream <a class="reference external" href="https://github.com/pytorch/torchtune">torchtune</a> repository
does not currently provide YAML configuration files for other combinations of
model to fine-tuning method
However, you can still configure your own YAML files to enable support for
fine-tuning methods not listed here by following existing patterns in the
<code class="docutils literal notranslate"><span class="pre">/workspace/torchtune/recipes/configs</span></code> directory.</p>
</div>
</div>
<div class="model-doc pyt-train-llama-3-1-70b docutils container">
<p class="rubric">Fine-tuning</p>
<p>To start the fine-tuning benchmark, use the following command with the
appropriate options. See the following list of options and their descriptions.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./pytorch_benchmark_report.sh<span class="w"> </span>-t<span class="w"> </span><span class="nv">$training_mode</span><span class="w"> </span>-m<span class="w"> </span>Llama-3.1-70B<span class="w"> </span>-p<span class="w"> </span>BF16<span class="w"> </span>-s<span class="w"> </span><span class="nv">$sequence_length</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Options</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$training_mode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></td>
<td><p>Full weight fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_qlora</span></code></p></td>
<td><p>QLoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">HF_finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning with Hugging Face PEFT</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BF16</span></code></p></td>
<td><p>All models support BF16.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$sequence_length</span></code></p></td>
<td><p>Between 2048 and 16384.</p></td>
<td><p>Sequence length for the language model.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Llama 3.1 70B currently supports the following fine-tuning methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_qlora</span></code></p></li>
</ul>
<p>The upstream <a class="reference external" href="https://github.com/pytorch/torchtune">torchtune</a> repository
does not currently provide YAML configuration files for other combinations of
model to fine-tuning method
However, you can still configure your own YAML files to enable support for
fine-tuning methods not listed here by following existing patterns in the
<code class="docutils literal notranslate"><span class="pre">/workspace/torchtune/recipes/configs</span></code> directory.</p>
</div>
</div>
<div class="model-doc pyt-train-llama-3-1-405b docutils container">
<p class="rubric">Fine-tuning</p>
<p>To start the fine-tuning benchmark, use the following command with the
appropriate options. See the following list of options and their descriptions.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./pytorch_benchmark_report.sh<span class="w"> </span>-t<span class="w"> </span><span class="nv">$training_mode</span><span class="w"> </span>-m<span class="w"> </span>Llama-3.1-405B<span class="w"> </span>-p<span class="w"> </span>BF16<span class="w"> </span>-s<span class="w"> </span><span class="nv">$sequence_length</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Options</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$training_mode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></td>
<td><p>Full weight fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_qlora</span></code></p></td>
<td><p>QLoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">HF_finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning with Hugging Face PEFT</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BF16</span></code></p></td>
<td><p>All models support BF16.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$sequence_length</span></code></p></td>
<td><p>Between 2048 and 16384.</p></td>
<td><p>Sequence length for the language model.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Llama 3.1 405B currently supports the following fine-tuning methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_qlora</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HF_finetune_lora</span></code></p></li>
</ul>
<p>The upstream <a class="reference external" href="https://github.com/pytorch/torchtune">torchtune</a> repository
does not currently provide YAML configuration files for other combinations of
model to fine-tuning method
However, you can still configure your own YAML files to enable support for
fine-tuning methods not listed here by following existing patterns in the
<code class="docutils literal notranslate"><span class="pre">/workspace/torchtune/recipes/configs</span></code> directory.</p>
</div>
</div>
<div class="model-doc pyt-train-llama-3-8b docutils container">
<p class="rubric">Fine-tuning</p>
<p>To start the fine-tuning benchmark, use the following command with the
appropriate options. See the following list of options and their descriptions.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./pytorch_benchmark_report.sh<span class="w"> </span>-t<span class="w"> </span><span class="nv">$training_mode</span><span class="w"> </span>-m<span class="w"> </span>Llama-3-8B<span class="w"> </span>-p<span class="w"> </span>BF16<span class="w"> </span>-s<span class="w"> </span><span class="nv">$sequence_length</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Options</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$training_mode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></td>
<td><p>Full weight fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_qlora</span></code></p></td>
<td><p>QLoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">HF_finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning with Hugging Face PEFT</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BF16</span></code></p></td>
<td><p>All models support BF16.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$sequence_length</span></code></p></td>
<td><p>Between 2048 and 16384.</p></td>
<td><p>Sequence length for the language model.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Llama 3 8B currently supports the following fine-tuning methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></li>
</ul>
<p>The upstream <a class="reference external" href="https://github.com/pytorch/torchtune">torchtune</a> repository
does not currently provide YAML configuration files for other combinations of
model to fine-tuning method
However, you can still configure your own YAML files to enable support for
fine-tuning methods not listed here by following existing patterns in the
<code class="docutils literal notranslate"><span class="pre">/workspace/torchtune/recipes/configs</span></code> directory.</p>
</div>
</div>
<div class="model-doc pyt-train-llama-3-70b docutils container">
<p class="rubric">Fine-tuning</p>
<p>To start the fine-tuning benchmark, use the following command with the
appropriate options. See the following list of options and their descriptions.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./pytorch_benchmark_report.sh<span class="w"> </span>-t<span class="w"> </span><span class="nv">$training_mode</span><span class="w"> </span>-m<span class="w"> </span>Llama-3-70B<span class="w"> </span>-p<span class="w"> </span>BF16<span class="w"> </span>-s<span class="w"> </span><span class="nv">$sequence_length</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Options</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$training_mode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></td>
<td><p>Full weight fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_qlora</span></code></p></td>
<td><p>QLoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">HF_finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning with Hugging Face PEFT</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BF16</span></code></p></td>
<td><p>All models support BF16.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$sequence_length</span></code></p></td>
<td><p>Between 2048 and 16384.</p></td>
<td><p>Sequence length for the language model.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Llama 3 70B currently supports the following fine-tuning methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></li>
</ul>
<p>The upstream <a class="reference external" href="https://github.com/pytorch/torchtune">torchtune</a> repository
does not currently provide YAML configuration files for other combinations of
model to fine-tuning method
However, you can still configure your own YAML files to enable support for
fine-tuning methods not listed here by following existing patterns in the
<code class="docutils literal notranslate"><span class="pre">/workspace/torchtune/recipes/configs</span></code> directory.</p>
</div>
</div>
<div class="model-doc pyt-train-llama-2-7b docutils container">
<p class="rubric">Fine-tuning</p>
<p>To start the fine-tuning benchmark, use the following command with the
appropriate options. See the following list of options and their descriptions.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./pytorch_benchmark_report.sh<span class="w"> </span>-t<span class="w"> </span><span class="nv">$training_mode</span><span class="w"> </span>-m<span class="w"> </span>Llama-2-7B<span class="w"> </span>-p<span class="w"> </span>BF16<span class="w"> </span>-s<span class="w"> </span><span class="nv">$sequence_length</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Options</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$training_mode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></td>
<td><p>Full weight fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_qlora</span></code></p></td>
<td><p>QLoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">HF_finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning with Hugging Face PEFT</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BF16</span></code></p></td>
<td><p>All models support BF16.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$sequence_length</span></code></p></td>
<td><p>Between 2048 and 16384.</p></td>
<td><p>Sequence length for the language model.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Llama 2 7B currently supports the following fine-tuning methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_qlora</span></code></p></li>
</ul>
<p>The upstream <a class="reference external" href="https://github.com/pytorch/torchtune">torchtune</a> repository
does not currently provide YAML configuration files for other combinations of
model to fine-tuning method
However, you can still configure your own YAML files to enable support for
fine-tuning methods not listed here by following existing patterns in the
<code class="docutils literal notranslate"><span class="pre">/workspace/torchtune/recipes/configs</span></code> directory.</p>
</div>
</div>
<div class="model-doc pyt-train-llama-2-13b docutils container">
<p class="rubric">Fine-tuning</p>
<p>To start the fine-tuning benchmark, use the following command with the
appropriate options. See the following list of options and their descriptions.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./pytorch_benchmark_report.sh<span class="w"> </span>-t<span class="w"> </span><span class="nv">$training_mode</span><span class="w"> </span>-m<span class="w"> </span>Llama-2-13B<span class="w"> </span>-p<span class="w"> </span>BF16<span class="w"> </span>-s<span class="w"> </span><span class="nv">$sequence_length</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Options</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$training_mode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></td>
<td><p>Full weight fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_qlora</span></code></p></td>
<td><p>QLoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">HF_finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning with Hugging Face PEFT</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BF16</span></code></p></td>
<td><p>All models support BF16.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$sequence_length</span></code></p></td>
<td><p>Between 2048 and 16384.</p></td>
<td><p>Sequence length for the language model.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Llama 2 13B currently supports the following fine-tuning methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></li>
</ul>
<p>The upstream <a class="reference external" href="https://github.com/pytorch/torchtune">torchtune</a> repository
does not currently provide YAML configuration files for other combinations of
model to fine-tuning method
However, you can still configure your own YAML files to enable support for
fine-tuning methods not listed here by following existing patterns in the
<code class="docutils literal notranslate"><span class="pre">/workspace/torchtune/recipes/configs</span></code> directory.</p>
</div>
</div>
<div class="model-doc pyt-train-llama-2-70b docutils container">
<p class="rubric">Fine-tuning</p>
<p>To start the fine-tuning benchmark, use the following command with the
appropriate options. See the following list of options and their descriptions.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./pytorch_benchmark_report.sh<span class="w"> </span>-t<span class="w"> </span><span class="nv">$training_mode</span><span class="w"> </span>-m<span class="w"> </span>Llama-2-70B<span class="w"> </span>-p<span class="w"> </span>BF16<span class="w"> </span>-s<span class="w"> </span><span class="nv">$sequence_length</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Options</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$training_mode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_fw</span></code></p></td>
<td><p>Full weight fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">finetune_qlora</span></code></p></td>
<td><p>QLoRA fine-tuning (BF16 supported)</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">HF_finetune_lora</span></code></p></td>
<td><p>LoRA fine-tuning with Hugging Face PEFT</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BF16</span></code></p></td>
<td><p>All models support BF16.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$sequence_length</span></code></p></td>
<td><p>Between 2048 and 16384.</p></td>
<td><p>Sequence length for the language model.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Llama 2 70B currently supports the following fine-tuning methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_lora</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">finetune_qlora</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HF_finetune_lora</span></code></p></li>
</ul>
<p>The upstream <a class="reference external" href="https://github.com/pytorch/torchtune">torchtune</a> repository
does not currently provide YAML configuration files for other combinations of
model to fine-tuning method
However, you can still configure your own YAML files to enable support for
fine-tuning methods not listed here by following existing patterns in the
<code class="docutils literal notranslate"><span class="pre">/workspace/torchtune/recipes/configs</span></code> directory.</p>
<p class="rubric">Benchmarking examples</p>
<p>For examples of benchmarking commands, see <a class="github reference external" href="https://github.com/ROCm/MAD/tree/develop/benchmark/pytorch_train#benchmarking-examples">ROCm/MAD</a>.</p>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="further-reading">
<h5>Further reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p>To learn more about MAD and the <code class="docutils literal notranslate"><span class="pre">madengine</span></code> CLI, see the <a class="reference external" href="https://github.com/ROCm/MAD?tab=readme-ov-file#usage-guide">MAD usage guide</a>.</p></li>
<li><p>To learn more about system settings and management practices to configure your system for
AMD Instinct MI300X series accelerators, see <a class="reference external" href="https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/system-optimization/mi300x.html">AMD Instinct MI300X system optimization</a>.</p></li>
<li><p>For a list of other ready-made Docker images for AI with ROCm, see
<a class="reference external" href="https://www.amd.com/en/developer/resources/infinity-hub.html#f-amd_hub_category=AI%20%26%20ML%20Models">AMD Infinity Hub</a>.</p></li>
</ul>
</section>
<section id="previous-versions">
<h5>Previous versions<a class="headerlink" href="#previous-versions" title="Link to this heading">#</a></h5>
<p>See <a class="reference internal" href="#document-how-to/rocm-for-ai/training/benchmark-docker/previous-versions/pytorch-training-history"><span class="doc">PyTorch training performance testing version history</span></a> to find documentation for previous releases
of the <code class="docutils literal notranslate"><span class="pre">ROCm/pytorch-training</span></code> Docker image.</p>
</section>
</section>
<span id="document-how-to/rocm-for-ai/training/benchmark-docker/jax-maxtext"></span><section id="training-a-model-with-maxtext-for-rocm">
<h4>Training a model with MaxText for ROCm<a class="headerlink" href="#training-a-model-with-maxtext-for-rocm" title="Link to this heading">#</a></h4>
<p>MaxText is a high-performance, open-source framework built on the Google JAX
machine learning library to train LLMs at scale. The MaxText framework for
ROCm is an optimized fork of the upstream
<a class="github reference external" href="https://github.com/AI-Hypercomputer/maxtext">AI-Hypercomputer/maxtext</a> enabling efficient AI workloads
on AMD MI300X series accelerators.</p>
<p>The MaxText for ROCm training Docker (<code class="docutils literal notranslate"><span class="pre">rocm/jax-training:maxtext-v25.5</span></code>) image
provides a prebuilt environment for training on AMD Instinct MI300X and MI325X accelerators,
including essential components like JAX, XLA, ROCm libraries, and MaxText utilities.
It includes the following software components:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Software component</p></th>
<th class="head"><p>Version</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ROCm</p></td>
<td><p>6.3.4</p></td>
</tr>
<tr class="row-odd"><td><p>JAX</p></td>
<td><p>0.4.35</p></td>
</tr>
<tr class="row-even"><td><p>Python</p></td>
<td><p>3.10.12</p></td>
</tr>
<tr class="row-odd"><td><p>Transformer Engine</p></td>
<td><p>1.12.0.dev0+b8b92dc</p></td>
</tr>
<tr class="row-even"><td><p>hipBLASLt</p></td>
<td><p>0.13.0-ae9c477a</p></td>
</tr>
</tbody>
</table>
</div>
<section id="supported-features-and-models">
<h5>Supported features and models<a class="headerlink" href="#supported-features-and-models" title="Link to this heading">#</a></h5>
<p>MaxText provides the following key features to train large language models efficiently:</p>
<ul class="simple">
<li><p>Transformer Engine (TE)</p></li>
<li><p>Flash Attention (FA) 3</p></li>
<li><p>GEMM tuning</p></li>
<li><p>Multi-node support</p></li>
</ul>
<p id="amd-maxtext-model-support">The following models are pre-optimized for performance on AMD Instinct MI300X series accelerators.</p>
<ul class="simple">
<li><p>Llama 3.3 70B</p></li>
<li><p>Llama 3.1 8B</p></li>
<li><p>Llama 3.1 70B</p></li>
<li><p>Llama 3 8B</p></li>
<li><p>Llama 3 70B</p></li>
<li><p>Llama 2 7B</p></li>
<li><p>Llama 2 70B</p></li>
<li><p>DeepSeek-V2-Lite</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some models, such as Llama 3, require an external license agreement through
a third party (for example, Meta).</p>
</div>
<section id="unsupported-features">
<h6>Unsupported features<a class="headerlink" href="#unsupported-features" title="Link to this heading">#</a></h6>
<p>Currently, MaxText’s default packed input format is not supported. Using this format
with the current Docker image results in incorrect attention calculations
across different input sequences. Support for packed input format is planned for a future release.</p>
</section>
</section>
<section id="system-validation">
<h5>System validation<a class="headerlink" href="#system-validation" title="Link to this heading">#</a></h5>
<p>Before running AI workloads, it’s important to validate that your AMD hardware is configured
correctly and performing optimally.</p>
<p>If you have already validated your system settings, including aspects like NUMA auto-balancing, you
can skip this step. Otherwise, complete the procedures in the <a class="reference internal" href="#rocm-for-ai-system-optimization"><span class="std std-ref">System validation and
optimization</span></a> guide to properly configure your system settings
before starting training.</p>
<p>To test for optimal performance, consult the recommended <a class="reference internal" href="#rocm-for-ai-system-health-bench"><span class="std std-ref">System health benchmarks</span></a>. This suite of tests will help you verify and fine-tune your
system’s configuration.</p>
</section>
<section id="environment-setup">
<h5>Environment setup<a class="headerlink" href="#environment-setup" title="Link to this heading">#</a></h5>
<p>This Docker image is optimized for specific model configurations outlined
as follows. Performance can vary for other training workloads, as AMD
doesn’t validate configurations and run conditions outside those described.</p>
<section id="multi-node-setup">
<span id="amd-maxtext-multi-node-setup"></span><h6>Multi-node setup<a class="headerlink" href="#multi-node-setup" title="Link to this heading">#</a></h6>
<p>For multi-node environments, ensure you have all the necessary packages for
your network device, such as, RDMA. If you’re not using a multi-node setup
with RDMA, skip ahead to <a class="reference internal" href="#amd-maxtext-download-docker"><span class="std std-ref">Pull the Docker image</span></a>.</p>
<ol class="arabic">
<li><p>Install the following packages to build and install the RDMA driver.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>iproute2<span class="w"> </span>-y
sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>linux-headers-<span class="s2">"</span><span class="k">$(</span>uname-r<span class="k">)</span><span class="s2">"</span><span class="w"> </span>libelf-dev
sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>gcc<span class="w"> </span>make<span class="w"> </span>libtool<span class="w"> </span>autoconf<span class="w"> </span>librdmacm-dev<span class="w"> </span>rdmacm-utils<span class="w"> </span>infiniband-diags<span class="w"> </span>ibverbs-utils<span class="w"> </span>perftest<span class="w"> </span>ethtool<span class="w"> </span>libibverbs-dev<span class="w"> </span>rdma-core<span class="w"> </span>strace<span class="w"> </span>libibmad5<span class="w"> </span>libibnetdisc5<span class="w"> </span>ibverbs-providers<span class="w"> </span>libibumad-dev<span class="w"> </span>libibumad3<span class="w"> </span>libibverbs1<span class="w"> </span>libnl-3-dev<span class="w"> </span>libnl-route-3-dev
</pre></div>
</div>
<p>Refer to your NIC manufacturer’s documentation for further steps on
compiling and installing the RoCE driver. For example, for Broadcom,
see <a class="reference external" href="https://docs.broadcom.com/doc/957608-AN2XX#G3.484341">Compiling Broadcom NIC software from source</a>
in <a class="reference external" href="https://docs.broadcom.com/doc/957608-AN2XX">Ethernet networking guide for AMD Instinct MI300X GPU clusters</a>.</p>
</li>
<li><p>Set the following environment variables.</p>
<ol class="loweralpha">
<li><p>Master address</p>
<p>Change <code class="docutils literal notranslate"><span class="pre">localhost</span></code> to the master node’s resolvable hostname or IP address:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MASTER_ADDR</span><span class="o">=</span><span class="s2">"</span><span class="si">${</span><span class="nv">MASTER_ADDR</span><span class="k">:-</span><span class="nv">localhost</span><span class="si">}</span><span class="s2">"</span>
</pre></div>
</div>
</li>
<li><p>Number of nodes</p>
<p>Set the number of nodes you want to train on (for example, <code class="docutils literal notranslate"><span class="pre">2</span></code>, <code class="docutils literal notranslate"><span class="pre">4</span></code>, or <code class="docutils literal notranslate"><span class="pre">8</span></code>):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">NNODES</span><span class="o">=</span><span class="s2">"</span><span class="si">${</span><span class="nv">NNODES</span><span class="k">:-</span><span class="nv">1</span><span class="si">}</span><span class="s2">"</span>
</pre></div>
</div>
</li>
<li><p>Node ranks</p>
<p>Set the rank of each node (<code class="docutils literal notranslate"><span class="pre">0</span></code> for master, <code class="docutils literal notranslate"><span class="pre">1</span></code> for the first worker node, and so on)
Node ranks should be unique across all nodes in the cluster.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">NODE_RANK</span><span class="o">=</span><span class="s2">"</span><span class="si">${</span><span class="nv">NODE_RANK</span><span class="k">:-</span><span class="nv">0</span><span class="si">}</span><span class="s2">"</span>
</pre></div>
</div>
</li>
<li><p>Network interface</p>
<p>Update the network interface in the script to match your system’s network interface. To
find your network interface, run the following (outside of any Docker container):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ip<span class="w"> </span>a
</pre></div>
</div>
<p>Look for an active interface with an IP address in the same subnet as
your other nodes. Then, update the following variable in the script, for
example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_SOCKET_IFNAME</span><span class="o">=</span>ens50f0np0
</pre></div>
</div>
<p>This variable specifies which network interface to use for inter-node communication.
Setting this variable to the incorrect interface can result in communication failures
or significantly reduced performance.</p>
</li>
<li><p>RDMA interface</p>
<p>Ensure the <a class="reference internal" href="#amd-maxtext-multi-node-setup"><span class="std std-ref">required packages</span></a> are installed on all nodes.
Then, set the RDMA interfaces to use for communication.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># If using Broadcom NIC</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_IB_HCA</span><span class="o">=</span>rdma0,rdma1,rdma2,rdma3,rdma4,rdma5,rdma6,rdma7
<span class="c1"># If using Mellanox NIC</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_IB_HCA</span><span class="o">=</span>mlx5_0,mlx5_1,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_8,mlx5_9
</pre></div>
</div>
</li>
</ol>
</li>
</ol>
</section>
<section id="pull-the-docker-image">
<span id="amd-maxtext-download-docker"></span><h6>Pull the Docker image<a class="headerlink" href="#pull-the-docker-image" title="Link to this heading">#</a></h6>
<ol class="arabic">
<li><p>Use the following command to pull the Docker image from Docker Hub.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/jax-training:maxtext-v25.5
</pre></div>
</div>
</li>
<li><p>Use the following command to launch the Docker container. Note that the benchmarking scripts
used in the <a class="reference internal" href="#amd-maxtext-get-started"><span class="std std-ref">following section</span></a> automatically launch the Docker container
and execute the benchmark.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>--device<span class="w"> </span>/dev/dri<span class="w"> </span>--device<span class="w"> </span>/dev/kfd<span class="w"> </span>--network<span class="w"> </span>host<span class="w"> </span>--ipc<span class="w"> </span>host<span class="w"> </span>--group-add<span class="w"> </span>video<span class="w"> </span>--cap-add<span class="w"> </span>SYS_PTRACE<span class="w"> </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span>--privileged<span class="w"> </span>-v<span class="w"> </span><span class="nv">$HOME</span>/.ssh:/root/.ssh<span class="w"> </span>--shm-size<span class="w"> </span>128G<span class="w"> </span>--name<span class="w"> </span>maxtext_training<span class="w"> </span>rocm/jax-training:maxtext-v25.5
</pre></div>
</div>
</li>
</ol>
</section>
</section>
<section id="getting-started">
<span id="amd-maxtext-get-started"></span><h5>Getting started<a class="headerlink" href="#getting-started" title="Link to this heading">#</a></h5>
<p>The following examples demonstrate how to get started with single node
and multi-node training using the benchmarking scripts provided at
<a class="github reference external" href="https://github.com/ROCm/maxtext/blob/main/benchmarks/gpu-rocm/">ROCm/maxtext</a>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The provided scripts launch a Docker container and execute a benchmark. Ensure you run these commands outside of any existing Docker container.</p>
</div>
<p>Before running any benchmarks, ensure the <code class="docutils literal notranslate"><span class="pre">$HF_HOME</span></code> environment variable is
set correctly and points to your Hugging Face cache directory. Refer to the
README at <a class="github reference external" href="https://github.com/ROCm/maxtext/blob/main/benchmarks/gpu-rocm/">ROCm/maxtext</a>
for more detailed instructions.</p>
<section id="single-node-training-benchmarking-examples">
<h6>Single node training benchmarking examples<a class="headerlink" href="#single-node-training-benchmarking-examples" title="Link to this heading">#</a></h6>
<ul>
<li><p>Example 1: Single node training with Llama 2 7B</p>
<p>Download the benchmarking script:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>https://raw.githubusercontent.com/ROCm/maxtext/refs/heads/main/benchmarks/gpu-rocm/llama2_7b.sh
</pre></div>
</div>
<p>Run the single node training benchmark:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">IMAGE</span><span class="o">=</span><span class="s2">"rocm/jax-training:maxtext-v25.5"</span><span class="w"> </span>bash<span class="w"> </span>./llama2_7b.sh
</pre></div>
</div>
</li>
<li><p>Example 2: Single node training with Llama 2 70B</p>
<p>Download the benchmarking script:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>https://raw.githubusercontent.com/ROCm/maxtext/refs/heads/main/benchmarks/gpu-rocm/llama2_70b.sh
</pre></div>
</div>
<p>Run the single node training benchmark:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">IMAGE</span><span class="o">=</span><span class="s2">"rocm/jax-training:maxtext-v25.5"</span><span class="w"> </span>bash<span class="w"> </span>./llama2_70b.sh
</pre></div>
</div>
</li>
<li><p>Example 3: Single node training with Llama 3 8B</p>
<p>Download the benchmarking script:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>https://raw.githubusercontent.com/ROCm/maxtext/refs/heads/main/benchmarks/gpu-rocm/llama3_8b.sh
</pre></div>
</div>
<p>Run the single node training benchmark:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">IMAGE</span><span class="o">=</span><span class="s2">"rocm/jax-training:maxtext-v25.5"</span><span class="w"> </span>bash<span class="w"> </span>./llama3_8b.sh
</pre></div>
</div>
</li>
<li><p>Example 4: Single node training with Llama 3 70B</p>
<p>Download the benchmarking script:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>https://raw.githubusercontent.com/ROCm/maxtext/refs/heads/main/benchmarks/gpu-rocm/llama3_70b.sh
</pre></div>
</div>
<p>Run the single node training benchmark:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">IMAGE</span><span class="o">=</span><span class="s2">"rocm/jax-training:maxtext-v25.5"</span><span class="w"> </span>bash<span class="w"> </span>./llama3_70b.sh
</pre></div>
</div>
</li>
<li><p>Example 5: Single node training with Llama 3.3 70B</p>
<p>Download the benchmarking script:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>https://raw.githubusercontent.com/ROCm/maxtext/refs/heads/main/benchmarks/gpu-rocm/llama3.3_70b.sh
</pre></div>
</div>
<p>Run the single node training benchmark:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">IMAGE</span><span class="o">=</span><span class="s2">"rocm/jax-training:maxtext-v25.5"</span><span class="w"> </span>bash<span class="w"> </span>./llama3.3_70b.sh
</pre></div>
</div>
</li>
<li><p>Example 6: Single node training with DeepSeek V2 16B</p>
<p>Download the benchmarking script:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>https://raw.githubusercontent.com/ROCm/maxtext/refs/heads/main/benchmarks/gpu-rocm/deepseek_v2_16b.sh
</pre></div>
</div>
<p>Run the single node training benchmark:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">IMAGE</span><span class="o">=</span><span class="s2">"rocm/jax-training:maxtext-v25.5"</span><span class="w"> </span>bash<span class="w"> </span>./deepseek_v2_16b.sh
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The reported TFLOP/s by MaxText for DeepSeek is not accurate. Use
the tokens/s as a performance indicator.</p>
</div>
</li>
</ul>
</section>
<section id="multi-node-training-benchmarking-examples">
<h6>Multi-node training benchmarking examples<a class="headerlink" href="#multi-node-training-benchmarking-examples" title="Link to this heading">#</a></h6>
<p>The following examples use SLURM for running on multiple nodes – the commands might need to be adjusted for your
own cluster setup.</p>
<ul>
<li><p>Example 1: Multi-node training with Llama 2 7B</p>
<p>Download the benchmarking script:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>https://raw.githubusercontent.com/ROCm/maxtext/refs/heads/main/benchmarks/gpu-rocm/llama2_7b_multinode.sh
</pre></div>
</div>
<p>Run the multi-node training benchmark. For example:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sbatch<span class="w"> </span>-N<span class="w"> </span>&lt;num_nodes&gt;<span class="w"> </span>llama2_7b_multinode.sh
</pre></div>
</div>
</li>
<li><p>Example 2: Multi-node training with Llama 2 70B</p>
<p>Download the benchmarking script:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>https://raw.githubusercontent.com/ROCm/maxtext/refs/heads/main/benchmarks/gpu-rocm/llama2_70b_multinode.sh
</pre></div>
</div>
<p>Run the multi-node training benchmark. For example:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sbatch<span class="w"> </span>-N<span class="w"> </span>&lt;num_nodes&gt;<span class="w"> </span>llama2_70b_multinode.sh
</pre></div>
</div>
</li>
<li><p>Example 3: Multi-node training with Llama 3 8B model</p>
<p>Download the benchmarking script:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>https://raw.githubusercontent.com/ROCm/maxtext/refs/heads/main/benchmarks/gpu-rocm/llama3_8b_multinode.sh
</pre></div>
</div>
<p>Run the multi-node training benchmark. For example:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sbatch<span class="w"> </span>-N<span class="w"> </span>&lt;num_nodes&gt;<span class="w"> </span>llama3_8b_multinode.sh
</pre></div>
</div>
</li>
<li><p>Example 4: Multi-node training with Llama 3 70B model</p>
<p>Download the benchmarking script:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>https://raw.githubusercontent.com/ROCm/maxtext/refs/heads/main/benchmarks/gpu-rocm/llama3_70b_multinode.sh
</pre></div>
</div>
<p>Run the multi-node training benchmark. For example:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sbatch<span class="w"> </span>-N<span class="w"> </span>&lt;num_nodes&gt;<span class="w"> </span>llama3_70b_multinode.sh
</pre></div>
</div>
</li>
</ul>
</section>
</section>
<section id="previous-versions">
<h5>Previous versions<a class="headerlink" href="#previous-versions" title="Link to this heading">#</a></h5>
<p>See <a class="reference internal" href="#document-how-to/rocm-for-ai/training/benchmark-docker/previous-versions/jax-maxtext-history"><span class="doc">JAX MaxText training performance testing version history</span></a> to find documentation for previous releases
of the <code class="docutils literal notranslate"><span class="pre">ROCm/jax-training</span></code> Docker image.</p>
</section>
</section>
<span id="document-how-to/rocm-for-ai/training/benchmark-docker/mpt-llm-foundry"></span><section id="training-mpt-30b-with-llm-foundry-and-rocm">
<h4>Training MPT-30B with LLM Foundry and ROCm<a class="headerlink" href="#training-mpt-30b-with-llm-foundry-and-rocm" title="Link to this heading">#</a></h4>
<p>MPT-30B is a 30-billion parameter decoder-style transformer-based model from
the Mosaic Pretrained Transformer (MPT) family – learn more about it in
MosaicML’s research blog <a class="reference external" href="https://www.databricks.com/blog/mpt-30b">MPT-30B: Raising the bar for open-source foundation
models</a>.</p>
<p>ROCm and <a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a> provide a pre-configured training
environment for the MPT-30B model using the <code class="docutils literal notranslate"><span class="pre">rocm/pytorch-training:v25.5</span></code>
base <a class="reference external" href="https://hub.docker.com/layers/rocm/pytorch-training/v25.5/images/sha256-d47850a9b25b4a7151f796a8d24d55ea17bba545573f0d50d54d3852f96ecde5">Docker image</a>
and the <a class="reference external" href="https://github.com/mosaicml/llm-foundry">LLM Foundry</a> framework.
This environment packages the following software components to train
on AMD Instinct MI300X series accelerators:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Software component</p></th>
<th class="head"><p>Version</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ROCm</p></td>
<td><p>6.3.4</p></td>
</tr>
<tr class="row-odd"><td><p>PyTorch</p></td>
<td><p>2.7.0a0+git6374332</p></td>
</tr>
<tr class="row-even"><td><p>Flash Attention</p></td>
<td><p>3.0.0.post1</p></td>
</tr>
</tbody>
</table>
</div>
<p>Using this image, you can build, run, and test the training process
for MPT-30B with access to detailed logs and performance metrics.</p>
<section id="system-validation">
<h5>System validation<a class="headerlink" href="#system-validation" title="Link to this heading">#</a></h5>
<p>Before running AI workloads, it’s important to validate that your AMD hardware is configured
correctly and performing optimally.</p>
<p>If you have already validated your system settings, including aspects like NUMA auto-balancing, you
can skip this step. Otherwise, complete the procedures in the <a class="reference internal" href="#rocm-for-ai-system-optimization"><span class="std std-ref">System validation and
optimization</span></a> guide to properly configure your system settings
before starting training.</p>
<p>To test for optimal performance, consult the recommended <a class="reference internal" href="#rocm-for-ai-system-health-bench"><span class="std std-ref">System health benchmarks</span></a>. This suite of tests will help you verify and fine-tune your
system’s configuration.</p>
</section>
<section id="getting-started">
<h5>Getting started<a class="headerlink" href="#getting-started" title="Link to this heading">#</a></h5>
<p>The following procedures help you set up the training environment in a
reproducible Docker container. This training environment is tailored for
training MPT-30B using LLM Foundry and the specific model configurations outlined.
Other configurations and run conditions outside those described in this
document are not validated.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-8" name="sd-tab-set-4" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-8">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<p>On your host machine, clone the ROCm Model Automation and Dashboarding
(<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local directory and
install the required packages.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
<p>Use this command to initiate the MPT-30B training benchmark.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_mpt30b_training<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--clean-docker-cache
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you experience data download failures, set the
<code class="docutils literal notranslate"><span class="pre">MAD_SECRETS_HFTOKEN</span></code> variable to your Hugging Face access token. See
<a class="reference external" href="https://huggingface.co/docs/hub/security-tokens">User access tokens</a>
for details.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
</pre></div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For improved performance (training throughput), consider enabling TunableOp.
By default, <code class="docutils literal notranslate"><span class="pre">pyt_mpt30b_training</span></code> runs with TunableOp disabled. To enable it,
run <code class="docutils literal notranslate"><span class="pre">madengine</span> <span class="pre">run</span></code> with the <code class="docutils literal notranslate"><span class="pre">--tunableop</span> <span class="pre">on</span></code> argument or edit the
<code class="docutils literal notranslate"><span class="pre">models.json</span></code> configuration before running training.</p>
<p>Although this might increase the initial training time, it can result in a performance gain.</p>
</div>
</div>
<input id="sd-tab-item-9" name="sd-tab-set-4" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-9">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p>To set up the training environment, clone the
<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a> repo and build the Docker image. In
this snippet, the image is named <code class="docutils literal notranslate"><span class="pre">mosaic_mpt30_image</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD

docker<span class="w"> </span>build<span class="w"> </span>--build-arg<span class="w"> </span><span class="nv">MAD_SYSTEM_GPU_ARCHITECTURE</span><span class="o">=</span>gfx942<span class="w"> </span>-f<span class="w"> </span>docker/pyt_mpt30b_training.ubuntu.amd.Dockerfile<span class="w"> </span>-t<span class="w"> </span>mosaic_mpt30_image<span class="w"> </span>.
</pre></div>
</div>
<p>Start a <code class="docutils literal notranslate"><span class="pre">mosaic_mpt30_image</span></code> container using the following command.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span>--group-add<span class="o">=</span>video<span class="w"> </span>--ipc<span class="o">=</span>host<span class="w"> </span>--shm-size<span class="o">=</span>8G<span class="w"> </span>mosaic_mpt30_image
</pre></div>
</div>
<p>In the Docker container, clone the <a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>
repository and navigate to the benchmark scripts directory at
<code class="docutils literal notranslate"><span class="pre">/workspace/MAD/scripts/pyt_mpt30b_training</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/pyt_mpt30b_training
</pre></div>
</div>
<p>To initiate the training process, use the following command. This script uses the hyperparameters defined in
<code class="docutils literal notranslate"><span class="pre">mpt-30b-instruct.yaml</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">source</span><span class="w"> </span>run.sh
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For improved performance (training throughput), consider enabling TunableOp.
To enable it, add the <code class="docutils literal notranslate"><span class="pre">--tunableop</span> <span class="pre">on</span></code> flag.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">source</span><span class="w"> </span>run.sh<span class="w"> </span>--tunableop<span class="w"> </span>on
</pre></div>
</div>
<p>Although this might increase the initial training time, it can result in a performance gain.</p>
</div>
</div>
</div>
</section>
<section id="interpreting-the-output">
<h5>Interpreting the output<a class="headerlink" href="#interpreting-the-output" title="Link to this heading">#</a></h5>
<p>The training output will be displayed in the terminal and simultaneously saved
to the <code class="docutils literal notranslate"><span class="pre">output.txt</span></code> file in the current directory. Key performance metrics will
also be extracted and appended to the <code class="docutils literal notranslate"><span class="pre">perf_pyt_mpt30b_training.csv</span></code> file.</p>
<p>Key performance metrics include:</p>
<ul>
<li><p>Training logs: Real-time display of loss metrics, accuracy, and training progress.</p></li>
<li><p>Model checkpoints: Periodically saved model snapshots for potential resume or evaluation.</p></li>
<li><p>Performance metrics: Detailed summaries of training speed and training loss metrics.</p>
<ul>
<li><p>Performance (throughput/samples_per_sec)</p>
<p>Overall throughput, measuring the total samples processed per second. Higher values indicate better hardware utilization.</p>
</li>
<li><p>Performance per device (throughput/samples_per_sec)</p>
<p>Throughput on a per-device basis, showing how each GPU or CPU is performing.</p>
</li>
<li><p>Language Cross Entropy (metrics/train/LanguageCrossEntropy)</p>
<p>Measures prediction accuracy. Lower cross entropy suggests the model’s output is closer to the expected distribution.</p>
</li>
<li><p>Training loss (loss/train/total)</p>
<p>Overall training loss. A decreasing trend indicates the model is learning effectively.</p>
</li>
</ul>
</li>
</ul>
</section>
<section id="further-reading">
<h5>Further reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p>To learn more about MAD and the <code class="docutils literal notranslate"><span class="pre">madengine</span></code> CLI, see the <a class="reference external" href="https://github.com/ROCm/MAD?tab=readme-ov-file#usage-guide">MAD usage guide</a>.</p></li>
<li><p>To learn more about system settings and management practices to configure your system for
AMD Instinct MI300X series accelerators, see <a class="reference external" href="https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/system-optimization/mi300x.html">AMD Instinct MI300X system optimization</a>.</p></li>
<li><p>For a list of other ready-made Docker images for AI with ROCm, see
<a class="reference external" href="https://www.amd.com/en/developer/resources/infinity-hub.html#f-amd_hub_category=AI%20%26%20ML%20Models">AMD Infinity Hub</a>.</p></li>
</ul>
</section>
</section>
<span id="document-how-to/rocm-for-ai/training/scale-model-training"></span><section id="scaling-model-training">
<h4>Scaling model training<a class="headerlink" href="#scaling-model-training" title="Link to this heading">#</a></h4>
<p>To train a large-scale model like OpenAI GPT-2 or Meta Llama 2 70B, a single accelerator or GPU cannot store all the
model parameters required for training. This immense scale presents a fundamental challenge: no single GPU or
accelerator can simultaneously store and process the entire model’s parameters during training. PyTorch
provides an answer to this computational constraint through its distributed training frameworks.</p>
<section id="pytorch-distributed">
<span id="rocm-for-ai-pytorch-distributed"></span><h5>PyTorch distributed<a class="headerlink" href="#pytorch-distributed" title="Link to this heading">#</a></h5>
<p>Features in <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> are categorized into three main components:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html">Distributed data-parallel training</a> (DDP)</p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/rpc.html">RPC-Based distributed training</a> (RPC)</p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/distributed.html">Collective communication</a></p></li>
</ul>
<p>In this topic, the focus is on the distributed data-parallelism strategy as it’s the most popular. To get started with DDP,
you need to first understand how to coordinate the model and its training data across multiple accelerators or GPUs.</p>
<p>The DDP workflow on multiple accelerators or GPUs is as follows:</p>
<ol class="arabic simple">
<li><p>Split the current global training batch into small local batches on each GPU. For instance, if you have 8 GPUs and
the global batch is set at 32 samples, each of the 8 GPUs will have a local batch size of 4 samples.</p></li>
<li><p>Copy the model to every device so each can process its local batches independently.</p></li>
<li><p>Run a forward pass, then a backward pass, and output the gradient of the weights with respect to the loss of the
model for that local batch. This happens in parallel on multiple devices.</p></li>
<li><p>Synchronize the local gradients computed by each device and combine them to update the model weights. The updated
weights are then redistributed to each device.</p></li>
</ol>
<p>In DDP training, each process or worker owns a replica of the model and processes a batch of data, and then the reducer uses
<code class="docutils literal notranslate"><span class="pre">allreduce</span></code> to sum up gradients over different workers.</p>
<p>See the following developer blogs for more in-depth explanations and examples.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/tutorials/beginner/ddp_series_multigpu.html">Multi GPU training with DDP — PyTorch Tutorials</a></p></li>
<li><p><a class="reference external" href="https://rocm.blogs.amd.com/artificial-intelligence/decoder-transformer/README.html#distributed-training-on-multiple-gpus">Building a decoder transformer model on AMD GPUs — ROCm Blogs</a></p></li>
</ul>
<section id="pytorch-fsdp">
<span id="rocm-for-ai-pytorch-fsdp"></span><h6>PyTorch FSDP<a class="headerlink" href="#pytorch-fsdp" title="Link to this heading">#</a></h6>
<p>As noted in <a class="reference internal" href="#rocm-for-ai-pytorch-distributed"><span class="std std-ref">PyTorch distributed</span></a>, DDP model weights and optimizer states
are evenly replicated across all workers. Fully Sharded Data Parallel (FSDP) is a type of data parallelism that shards
model parameters, optimizer states, and gradients across DDP ranks.</p>
<p>When training with FSDP, the GPU memory footprint is smaller than when training with DDP across all workers. This makes
training some very large models feasible by allowing larger models or batch sizes to fit on-device. However, this
comes with the cost of increased communication volume. The communication overhead is reduced by internal optimizations
like overlapping communication and computation.</p>
<p>For a high-level overview of how FSDP works, review <a class="reference external" href="https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html#how-fsdp-works">Getting started with Fully Sharded Data Parallel</a>.</p>
<p>For detailed training steps, see <a class="reference external" href="https://github.com/pytorch/examples/tree/main/distributed/FSDP">PyTorch FSDP examples</a>.</p>
</section>
<section id="deepspeed">
<span id="rocm-for-ai-deepspeed"></span><h6>DeepSpeed<a class="headerlink" href="#deepspeed" title="Link to this heading">#</a></h6>
<p><a class="reference external" href="https://deepspeed.ai">DeepSpeed</a> offers system innovations that make large-scale deep learning training effective,
efficient, and easy to use. Innovations such as ZeRO, 3D-Parallelism, DeepSpeed-MoE, ZeRO-Infinity, and so on fall under
the training pillar.</p>
<p>See <a class="reference external" href="https://rocm.blogs.amd.com/artificial-intelligence/megatron-deepspeed-pretrain/README.html">Pre-training a large language model with Megatron-DeepSpeed on multiple AMD GPUs</a> for a detailed example of
training with DeepSpeed on an AMD accelerator or GPU.</p>
</section>
<section id="automatic-mixed-precision-amp">
<span id="rocm-for-ai-automatic-mixed-precision"></span><h6>Automatic mixed precision (AMP)<a class="headerlink" href="#automatic-mixed-precision-amp" title="Link to this heading">#</a></h6>
<p>As models increase in size, so do the time and memory needed to train them; their cost also increases. Any measure we
can take to reduce training time and memory usage through <a class="reference external" href="https://pytorch.org/docs/stable/amp.html">automatic mixed precision</a> (AMP) is highly beneficial for most use cases.</p>
<p>See <a class="reference external" href="https://rocm.blogs.amd.com/artificial-intelligence/automatic-mixed-precision/README.html#automatic-mixed-precision-in-pytorch-using-amd-gpus">Automatic mixed precision in PyTorch using AMD GPUs — ROCm Blogs</a>
for more information about running AMP on an AMD accelerator.</p>
</section>
</section>
<section id="fine-tuning-your-model">
<span id="rocm-for-ai-fine-tune"></span><h5>Fine-tuning your model<a class="headerlink" href="#fine-tuning-your-model" title="Link to this heading">#</a></h5>
<p>ROCm supports multiple techniques for <a class="reference internal" href="#fine-tuning-llms-concept-optimizations"><span class="std std-ref">optimizing fine-tuning</span></a>, for
example, LoRA, QLoRA, PEFT, and FSDP.</p>
<p>Learn more about challenges and solutions for model fine-tuning in <a class="reference internal" href="#document-how-to/rocm-for-ai/fine-tuning/index"><span class="doc">Use ROCm for fine-tuning LLMs</span></a>.</p>
<p>The following developer blogs showcase examples of fine-tuning a model on an AMD accelerator or GPU.</p>
<ul class="simple">
<li><p>Fine-tuning Llama2 with LoRA</p>
<ul>
<li><p><a class="reference external" href="https://rocm.blogs.amd.com/artificial-intelligence/llama2-lora/README.html">Fine-tune Llama 2 with LoRA: Customizing a large language model for question-answering</a></p></li>
</ul>
</li>
<li><p>Fine-tuning Llama2 with QLoRA</p>
<ul>
<li><p><a class="reference external" href="https://rocm.blogs.amd.com/artificial-intelligence/llama2-Qlora/README.html">Enhancing LLM accessibility: A deep dive into QLoRA through fine-tuning Llama 2 on a single AMD GPU</a></p></li>
</ul>
</li>
<li><p>Fine-tuning a BERT-based LLM for a text classification task using JAX</p>
<ul>
<li><p><a class="reference external" href="https://rocm.blogs.amd.com/artificial-intelligence/distributed-sft-jax/README.html">LLM distributed supervised fine-tuning with JAX</a></p></li>
</ul>
</li>
<li><p>Fine-tuning StarCoder using PEFT</p>
<ul>
<li><p><a class="reference external" href="https://rocm.blogs.amd.com/artificial-intelligence/starcoder-fine-tune/README.html">Instruction fine-tuning of StarCoder with PEFT on multiple AMD GPUs</a></p></li>
</ul>
</li>
<li><p>Recipes for fine-tuning Llama2 and 3 with <code class="docutils literal notranslate"><span class="pre">llama-recipes</span></code></p>
<ul>
<li><p><a class="reference external" href="https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/finetuning">meta-llama/llama-recipes: Scripts for fine-tuning Meta Llama3 with composable FSDP &amp; PEFT methods to cover
single/multi-node GPUs</a></p></li>
</ul>
</li>
</ul>
</section>
</section>
</div>
</section>
<span id="document-how-to/rocm-for-ai/fine-tuning/index"></span><section id="use-rocm-for-fine-tuning-llms">
<h3>Use ROCm for fine-tuning LLMs<a class="headerlink" href="#use-rocm-for-fine-tuning-llms" title="Link to this heading">#</a></h3>
<p>Fine-tuning is an essential technique in machine learning, where a pre-trained model, typically trained on a large-scale dataset, is further refined to achieve better performance and adapt to a particular task or dataset of interest.</p>
<p>With AMD GPUs, the fine-tuning process benefits from the parallel processing capabilities and efficient resource management, ultimately leading to improved performance and faster model adaptation to the target domain.</p>
<p>The ROCm™ software platform helps you optimize this fine-tuning process by supporting various optimization techniques tailored for AMD GPUs. It empowers the fine-tuning of large language models, making them accessible and efficient for specialized tasks. ROCm supports the broader AI ecosystem to ensure seamless integration with open frameworks, models, and tools.</p>
<p>Throughout the following topics, this guide discusses the goals and <a class="reference internal" href="#fine-tuning-llms-concept-challenge"><span class="std std-ref">challenges of fine-tuning a large language
model</span></a> like Llama 2. In the
sections that follow, you’ll find practical guides on libraries and tools to accelerate your fine-tuning.</p>
<p>The AI Developer Hub contains <a class="reference external" href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/">AMD ROCm tutorials</a> for
training, fine-tuning, and inference. It leverages popular machine learning frameworks on AMD GPUs.</p>
<ul class="simple">
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/fine-tuning/overview"><span class="doc">Conceptual overview of fine-tuning LLMs</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/fine-tuning/fine-tuning-and-inference"><span class="doc">Fine-tuning and inference</span></a> using a
<a class="reference internal" href="#document-how-to/rocm-for-ai/fine-tuning/single-gpu-fine-tuning-and-inference"><span class="doc">single-accelerator</span></a> or
<a class="reference internal" href="#document-how-to/rocm-for-ai/fine-tuning/multi-gpu-fine-tuning-and-inference"><span class="doc">multi-accelerator</span></a> system.</p></li>
</ul>
<div class="toctree-wrapper compound">
<span id="document-how-to/rocm-for-ai/fine-tuning/overview"></span><section id="conceptual-overview-of-fine-tuning-llms">
<h4>Conceptual overview of fine-tuning LLMs<a class="headerlink" href="#conceptual-overview-of-fine-tuning-llms" title="Link to this heading">#</a></h4>
<p>Large language models (LLMs) are trained on massive amounts of text data to generate coherent and fluent text. The
underlying <em>transformer</em> architecture is the fundamental building block of all LLMs. Transformers
enable LLMs to understand and generate text by capturing contextual relationships and long-range dependencies. To better
understand the philosophy of the transformer architecture, review the foundational
<a class="reference external" href="https://arxiv.org/pdf/1706.03762.pdf">Attention is all you need</a> paper.</p>
<p>By further training pre-trained LLMs, the fine-tuned model can gain knowledge related to specific fields or tasks,
thereby significantly improving its performance in that field or task. The core idea of fine-tuning is to use the
parameters of the pre-trained model as the starting point for new tasks and shape it through a small amount of
specific domain or task data, expanding the original model’s capability to new tasks or datasets.</p>
<p>Fine-tuning can effectively improve the performance of existing pre-trained models in specific application scenarios.
Continuous training and adjustment of the parameters of the base model in the target domain or task can better capture
the semantic characteristics and patterns in specific scenarios, thereby significantly improving the key indicators of
the model in that domain or task. For example, by fine-tuning the Llama 2 model, its performance in certain applications
can be improve over the base model.</p>
<section id="the-challenge-of-fine-tuning-models">
<span id="fine-tuning-llms-concept-challenge"></span><h5>The challenge of fine-tuning models<a class="headerlink" href="#the-challenge-of-fine-tuning-models" title="Link to this heading">#</a></h5>
<p>However, the computational cost of fine-tuning is still high, especially for complex models and large datasets, which
poses distinct challenges related to substantial computational and memory requirements. This might be a barrier for
accelerators or GPUs with low computing power or limited device memory resources.</p>
<p>For example, suppose we have a language model with 7 billion (7B) parameters, represented by a weight matrix <span class="math notranslate nohighlight">\(W\)</span>.
During backpropagation, the model needs to learn a <span class="math notranslate nohighlight">\(ΔW\)</span> matrix, which updates the original weights to minimize the
value of the loss function.</p>
<p>The weight update is as follows: <span class="math notranslate nohighlight">\(W_{updated} = W + ΔW\)</span>.</p>
<p>If the weight matrix <span class="math notranslate nohighlight">\(W\)</span> contains 7B parameters, then the weight update matrix <span class="math notranslate nohighlight">\(ΔW\)</span> should also
contain 7B parameters. Therefore, the <span class="math notranslate nohighlight">\(ΔW\)</span> calculation is computationally and memory intensive.</p>
<figure class="align-default" id="id1">
<img alt="Weight update diagram" src="_images/weight-update.png"/>
<figcaption>
<p><span class="caption-text">(a) Weight update in regular fine-tuning. (b) Weight update in LoRA where the product of matrix A (<span class="math notranslate nohighlight">\(M\times K\)</span>)
and matrix B (<span class="math notranslate nohighlight">\(K\times N\)</span>) is <span class="math notranslate nohighlight">\(ΔW(M\times N)\)</span>; dimension K is a hyperparameter. By representing
<span class="math notranslate nohighlight">\(ΔW\)</span> as the product of two smaller matrices (A and B) with a lower rank K, the number of trainable parameters
is significantly reduced.</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="optimizations-for-model-fine-tuning">
<span id="fine-tuning-llms-concept-optimizations"></span><h5>Optimizations for model fine-tuning<a class="headerlink" href="#optimizations-for-model-fine-tuning" title="Link to this heading">#</a></h5>
<p>Low-Rank Adaptation (LoRA) is a technique allowing fast and cost-effective fine-tuning of state-of-the-art LLMs that can
overcome this issue of high memory consumption.</p>
<p>LoRA accelerates the adjustment process and reduces related memory costs. To be precise, LoRA decomposes the portion of
weight changes <span class="math notranslate nohighlight">\(ΔW\)</span> into high-precision low-rank representations, which do not require the calculations of all
<span class="math notranslate nohighlight">\(ΔW\)</span>. It learns the decomposition representation of <span class="math notranslate nohighlight">\(ΔW\)</span> during training, as shown in
the <a class="reference internal" href="#fine-tuning-llms-concept-challenge"><span class="std std-ref">weight update diagram</span></a>. This is how LoRA saves on
computing resources.</p>
<p>LoRA is integrated into the <a class="reference external" href="https://huggingface.co/docs/peft/en/index">Hugging Face Parameter-Efficient Fine-Tuning (PEFT)</a> library, as well as other computation and memory efficiency optimization
variants for model fine-tuning such as <a class="reference external" href="https://huggingface.co/docs/peft/en/package_reference/adalora">AdaLoRA</a>. This
library efficiently adapts large pre-trained models to various downstream applications without fine-tuning all model
parameters. PEFT methods only fine-tune a few model parameters, significantly decreasing computational and storage
costs while yielding performance comparable to a fully fine-tuned model. PEFT is integrated with the <a class="reference external" href="https://huggingface.co/docs/transformers/en/index">Hugging Face
Transformers</a> library, providing a faster and easier way to load,
train, and use large models for inference.</p>
<p>To simplify running a fine-tuning implementation, the <a class="reference external" href="https://huggingface.co/docs/trl/en/index">Transformer Reinforcement Learning (TRL)</a> library provides a set of tools to train transformer language models with
reinforcement learning, from the Supervised Fine-Tuning step (SFT), Reward Modeling step (RM), to the Proximal Policy
Optimization (PPO) step. The <code class="docutils literal notranslate"><span class="pre">SFTTrainer</span></code> API in TRL encapsulates these PEFT optimizations so you can easily import
their custom training configuration and run the training process.</p>
</section>
<section id="walkthrough">
<span id="fine-tuning-llms-walkthrough-desc"></span><h5>Walkthrough<a class="headerlink" href="#walkthrough" title="Link to this heading">#</a></h5>
<p>To demonstrate the benefits of LoRA and the ideal compute compatibility of using PEFT and TRL libraries on AMD
ROCm-compatible accelerators and GPUs, let’s step through a comprehensive implementation of the fine-tuning process
using the Llama 2 7B model with LoRA tailored specifically for question-and-answer tasks on AMD MI300X accelerators.</p>
<p>Before starting, review and understand the key components of this walkthrough:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/meta-llama">Llama 2</a>: a family of large language models developed and publicly released by
Meta. Its variants range in scale from 7 billion to 70 billion parameters.</p></li>
<li><p>Fine-tuning: a critical process that refines LLMs for specialized tasks and optimizes performance.</p></li>
<li><p>LoRA: a memory-efficient implementation of LLM fine-tuning that significantly reduces the number of trainable
parameters.</p></li>
<li><p><a class="reference external" href="https://huggingface.co/docs/trl/v0.8.6/en/sft_trainer#supervised-fine-tuning-trainer">SFTTrainer</a>: an optimized
trainer with a simple interface to easily fine-tune pre-trained models with PEFT adapters, for example, LoRA, for
memory efficiency purposes on a custom dataset.</p></li>
</ul>
<p>Continue the walkthrough in <a class="reference internal" href="#document-how-to/rocm-for-ai/fine-tuning/fine-tuning-and-inference"><span class="doc">Fine-tuning and inference</span></a>.</p>
</section>
</section>
<span id="document-how-to/rocm-for-ai/fine-tuning/fine-tuning-and-inference"></span><section id="fine-tuning-and-inference">
<h4>Fine-tuning and inference<a class="headerlink" href="#fine-tuning-and-inference" title="Link to this heading">#</a></h4>
<p>Fine-tuning using ROCm involves leveraging AMD’s GPU-accelerated <a class="reference external" href="https://rocm.docs.amd.com/en/latest/reference/api-libraries.html" title="(in ROCm Documentation v6.4.2)"><span class="xref std std-doc">libraries</span></a> and
<a class="reference external" href="https://rocm.docs.amd.com/en/latest/reference/rocm-tools.html" title="(in ROCm Documentation v6.4.2)"><span class="xref std std-doc">tools</span></a> to optimize and train deep learning models. ROCm provides a comprehensive
ecosystem for deep learning development, including open-source libraries for optimized deep learning operations and
ROCm-aware versions of <a class="reference internal" href="#document-how-to/deep-learning-rocm"><span class="doc">deep learning frameworks</span></a> such as PyTorch, TensorFlow, and JAX.</p>
<p>Single-accelerator systems, such as a machine equipped with a single accelerator or GPU, are commonly used for
smaller-scale deep learning tasks, including fine-tuning pre-trained models and running inference on moderately
sized datasets. See <a class="reference internal" href="#document-how-to/rocm-for-ai/fine-tuning/single-gpu-fine-tuning-and-inference"><span class="doc">Fine-tuning and inference using a single accelerator</span></a>.</p>
<p>Multi-accelerator systems, on the other hand, consist of multiple accelerators working in parallel. These systems are
typically used in LLMs and other large-scale deep learning tasks where performance, scalability, and the handling of
massive datasets are crucial. See <a class="reference internal" href="#document-how-to/rocm-for-ai/fine-tuning/multi-gpu-fine-tuning-and-inference"><span class="doc">Fine-tuning and inference using multiple accelerators</span></a>.</p>
<div class="toctree-wrapper compound">
<span id="document-how-to/rocm-for-ai/fine-tuning/single-gpu-fine-tuning-and-inference"></span><section id="fine-tuning-and-inference-using-a-single-accelerator">
<h5>Fine-tuning and inference using a single accelerator<a class="headerlink" href="#fine-tuning-and-inference-using-a-single-accelerator" title="Link to this heading">#</a></h5>
<p>This section explains model fine-tuning and inference techniques on a single-accelerator system. See
<a class="reference internal" href="#document-how-to/rocm-for-ai/fine-tuning/multi-gpu-fine-tuning-and-inference"><span class="doc">Multi-accelerator fine-tuning</span></a> for a setup with multiple accelerators or
GPUs.</p>
<section id="environment-setup">
<span id="fine-tuning-llms-single-gpu-env"></span><h6>Environment setup<a class="headerlink" href="#environment-setup" title="Link to this heading">#</a></h6>
<p>This section was tested using the following hardware and software environment.</p>
<div class="pst-scrollable-table-container"><table class="table">
<tbody>
<tr class="row-odd"><th class="stub"><p>Hardware</p></th>
<td><p>AMD Instinct MI300X accelerator</p></td>
</tr>
<tr class="row-even"><th class="stub"><p>Software</p></th>
<td><p>ROCm 6.1, Ubuntu 22.04, PyTorch 2.1.2, Python 3.10</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>Libraries</p></th>
<td><p><code class="docutils literal notranslate"><span class="pre">transformers</span></code> <code class="docutils literal notranslate"><span class="pre">datasets</span></code> <code class="docutils literal notranslate"><span class="pre">huggingface-hub</span></code> <code class="docutils literal notranslate"><span class="pre">peft</span></code> <code class="docutils literal notranslate"><span class="pre">trl</span></code> <code class="docutils literal notranslate"><span class="pre">scipy</span></code></p></td>
</tr>
<tr class="row-even"><th class="stub"><p>Base model</p></th>
<td><p><code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-2-7b-chat-hf</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<section id="setting-up-the-base-implementation-environment">
<span id="fine-tuning-llms-single-gpu-env-setup"></span><h6 aria-level="7">Setting up the base implementation environment<a class="headerlink" href="#setting-up-the-base-implementation-environment" title="Link to this heading">#</a></h6>
<ol class="arabic">
<li><p>Install PyTorch for ROCm. Refer to the
<a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/pytorch-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">PyTorch installation guide</span></a>. For a consistent
installation, it’s recommended to use official ROCm prebuilt Docker images with the framework pre-installed.</p></li>
<li><p>In the Docker container, check the availability of ROCm-capable accelerators using the following command.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>rocm-smi<span class="w"> </span>--showproductname
</pre></div>
</div>
<p>Your output should look like this:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="o">============================</span><span class="w"> </span>ROCm<span class="w"> </span>System<span class="w"> </span>Management<span class="w"> </span><span class="nv">Interface</span><span class="w"> </span><span class="o">============================</span>
<span class="o">======================================</span><span class="w"> </span>Product<span class="w"> </span><span class="nv">Info</span><span class="w"> </span><span class="o">======================================</span>
GPU<span class="o">[</span><span class="m">0</span><span class="o">]</span><span class="w">          </span>:<span class="w"> </span>Card<span class="w"> </span>series:<span class="w">          </span>AMD<span class="w"> </span>Instinct<span class="w"> </span>MI300X<span class="w"> </span>OAM
GPU<span class="o">[</span><span class="m">0</span><span class="o">]</span><span class="w">          </span>:<span class="w"> </span>Card<span class="w"> </span>model:<span class="w">           </span>0x74a1
GPU<span class="o">[</span><span class="m">0</span><span class="o">]</span><span class="w">          </span>:<span class="w"> </span>Card<span class="w"> </span>vendor:<span class="w">          </span>Advanced<span class="w"> </span>Micro<span class="w"> </span>Devices,<span class="w"> </span>Inc.<span class="w"> </span><span class="o">[</span>AMD/ATI<span class="o">]</span>
GPU<span class="o">[</span><span class="m">0</span><span class="o">]</span><span class="w">          </span>:<span class="w"> </span>Card<span class="w"> </span>SKU:<span class="w">             </span><span class="nv">MI3SRIOV</span>
<span class="o">==========================================================================================</span>
<span class="o">==================================</span><span class="w"> </span>End<span class="w"> </span>of<span class="w"> </span>ROCm<span class="w"> </span>SMI<span class="w"> </span><span class="nv">Log</span><span class="w"> </span><span class="o">===================================</span>
</pre></div>
</div>
</li>
<li><p>Check that your accelerators are available to PyTorch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Is a ROCm-GPU detected? "</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"How many ROCm-GPUs are detected? "</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">())</span>
</pre></div>
</div>
<p>If successful, your output should look like this:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span>print<span class="o">(</span><span class="s2">"Is a ROCm-GPU detected? "</span>,<span class="w"> </span>torch.cuda.is_available<span class="o">())</span>
Is<span class="w"> </span>a<span class="w"> </span>ROCm-GPU<span class="w"> </span>detected?<span class="w">  </span>True
&gt;&gt;&gt;<span class="w"> </span>print<span class="o">(</span><span class="s2">"How many ROCm-GPUs are detected? "</span>,<span class="w"> </span>torch.cuda.device_count<span class="o">())</span>
How<span class="w"> </span>many<span class="w"> </span>ROCm-GPUs<span class="w"> </span>are<span class="w"> </span>detected?<span class="w">  </span><span class="m">4</span>
</pre></div>
</div>
</li>
<li><p>Install the required dependencies.</p>
<p>bitsandbytes is a library that facilitates quantization to improve the efficiency of deep learning models. Learn more
about its use in <a class="reference internal" href="#document-how-to/rocm-for-ai/inference-optimization/model-quantization"><span class="doc">Model quantization techniques</span></a>.</p>
<p>See the <a class="reference internal" href="#fine-tuning-llms-concept-optimizations"><span class="std std-ref">Optimizations for model fine-tuning</span></a> for a brief discussion on
PEFT and TRL.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install `bitsandbytes` for ROCm 6.0+.</span>
<span class="c1"># Use -DBNB_ROCM_ARCH to target a specific GPU architecture.</span>
git<span class="w"> </span>clone<span class="w"> </span>--recurse<span class="w"> </span>https://github.com/ROCm/bitsandbytes.git
<span class="nb">cd</span><span class="w"> </span>bitsandbytes
git<span class="w"> </span>checkout<span class="w"> </span>rocm_enabled_multi_backend
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements-dev.txt
cmake<span class="w"> </span>-DBNB_ROCM_ARCH<span class="o">=</span><span class="s2">"gfx942"</span><span class="w"> </span>-DCOMPUTE_BACKEND<span class="o">=</span>hip<span class="w"> </span>-S<span class="w"> </span>.
python<span class="w"> </span>setup.py<span class="w"> </span>install

<span class="c1"># To leverage the SFTTrainer in TRL for model fine-tuning.</span>
pip<span class="w"> </span>install<span class="w"> </span>trl

<span class="c1"># To leverage PEFT for efficiently adapting pre-trained language models .</span>
pip<span class="w"> </span>install<span class="w"> </span>peft

<span class="c1"># Install the other dependencies.</span>
pip<span class="w"> </span>install<span class="w"> </span>transformers<span class="w"> </span>datasets<span class="w"> </span>huggingface-hub<span class="w"> </span>scipy
</pre></div>
</div>
</li>
<li><p>Check that the required packages can be imported.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AutoModelForCausalLM</span><span class="p">,</span>
    <span class="n">AutoTokenizer</span><span class="p">,</span>
    <span class="n">TrainingArguments</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">peft</span><span class="w"> </span><span class="kn">import</span> <span class="n">LoraConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">trl</span><span class="w"> </span><span class="kn">import</span> <span class="n">SFTTrainer</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="download-the-base-model-and-fine-tuning-dataset">
<span id="fine-tuning-llms-single-gpu-download-model-dataset"></span><h6 aria-level="7">Download the base model and fine-tuning dataset<a class="headerlink" href="#download-the-base-model-and-fine-tuning-dataset" title="Link to this heading">#</a></h6>
<ol class="arabic">
<li><p>Request to access to download the <a class="reference external" href="https://huggingface.co/meta-llama">Meta’s official Llama model</a> from Hugging
Face. After permission is granted, log in with the following command using your personal access tokens:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>huggingface-cli<span class="w"> </span>login
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can also use the <a class="reference external" href="https://huggingface.co/NousResearch/Llama-2-7b-chat-hf">NousResearch Llama-2-7b-chat-hf</a>
as a substitute. It has the same model weights as the original.</p>
</div>
</li>
<li><p>Run the following code to load the base model and tokenizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Base model and tokenizer names.</span>
<span class="n">base_model_name</span> <span class="o">=</span> <span class="s2">"meta-llama/Llama-2-7b-chat-hf"</span>

<span class="c1"># Load base model to GPU memory.</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda:0"</span>
<span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model_name</span><span class="p">,</span> <span class="n">trust_remote_code</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Load tokenizer.</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="n">base_model_name</span><span class="p">,</span>
        <span class="n">trust_remote_code</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="s2">"right"</span>
</pre></div>
</div>
</li>
<li><p>Now, let’s fine-tune the base model for a question-and-answer task using a small dataset called
<a class="reference external" href="https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k">mlabonne/guanaco-llama2-1k</a>, which is a 1000 sample
subset of the <a class="reference external" href="https://huggingface.co/datasets/OpenAssistant/oasst1">timdettmers/openassistant-guanaco</a> dataset.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dataset for fine-tuning.</span>
<span class="n">training_dataset_name</span> <span class="o">=</span> <span class="s2">"mlabonne/guanaco-llama2-1k"</span>
<span class="n">training_dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">training_dataset_name</span><span class="p">,</span> <span class="n">split</span> <span class="o">=</span> <span class="s2">"train"</span><span class="p">)</span>

<span class="c1"># Check the data.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">training_dataset</span><span class="p">)</span>

<span class="c1"># Dataset 11 is a QA sample in English.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">training_dataset</span><span class="p">[</span><span class="mi">11</span><span class="p">])</span>
</pre></div>
</div>
</li>
<li><p>With the base model and the dataset, let’s start fine-tuning!</p></li>
</ol>
</section>
<section id="configure-fine-tuning-parameters">
<span id="fine-tuning-llms-single-gpu-configure-params"></span><h6 aria-level="7">Configure fine-tuning parameters<a class="headerlink" href="#configure-fine-tuning-parameters" title="Link to this heading">#</a></h6>
<p>To set up <code class="docutils literal notranslate"><span class="pre">SFTTrainer</span></code> parameters, you can use the following code as reference.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training parameters for SFTTrainer.</span>
<span class="n">training_arguments</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span> <span class="o">=</span> <span class="s2">"./results"</span><span class="p">,</span>
         <span class="n">num_train_epochs</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
         <span class="n">per_device_train_batch_size</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
         <span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
         <span class="n">optim</span> <span class="o">=</span> <span class="s2">"paged_adamw_32bit"</span><span class="p">,</span>
         <span class="n">save_steps</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
         <span class="n">logging_steps</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
         <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">4e-5</span><span class="p">,</span>
         <span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
         <span class="n">fp16</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
         <span class="n">bf16</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
         <span class="n">max_grad_norm</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span>
         <span class="n">max_steps</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
         <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.03</span><span class="p">,</span>
         <span class="n">group_by_length</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
         <span class="n">lr_scheduler_type</span> <span class="o">=</span> <span class="s2">"constant"</span><span class="p">,</span>
         <span class="n">report_to</span> <span class="o">=</span> <span class="s2">"tensorboard"</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="fine-tuning">
<span id="fine-tuning-llms-single-gpu-start"></span><h6>Fine-tuning<a class="headerlink" href="#fine-tuning" title="Link to this heading">#</a></h6>
<p>In this section, you’ll see two ways of training: with the LoRA technique and without. See <a class="reference internal" href="#fine-tuning-llms-concept-optimizations"><span class="std std-ref">Optimizations for model
fine-tuning</span></a> for an introduction to LoRA. Training with LoRA uses the
<code class="docutils literal notranslate"><span class="pre">SFTTrainer</span></code> API with its PEFT integration. Training without LoRA forgoes these benefits.</p>
<p>Compare the number of trainable parameters and training time under the two different methodologies.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-10" name="sd-tab-set-5" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="with" for="sd-tab-item-10">
Fine-tuning with LoRA and PEFT</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Configure LoRA using the following code snippet.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">peft_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
        <span class="n">lora_alpha</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
        <span class="n">lora_dropout</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">r</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="s2">"none"</span><span class="p">,</span>
        <span class="n">task_type</span> <span class="o">=</span> <span class="s2">"CAUSAL_LM"</span>
<span class="p">)</span>
<span class="c1"># View the number of trainable parameters.</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">peft</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_peft_model</span>
<span class="n">peft_model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="n">peft_config</span><span class="p">)</span>
<span class="n">peft_model</span><span class="o">.</span><span class="n">print_trainable_parameters</span><span class="p">()</span>
</pre></div>
</div>
<p>The output should look like this. Compare the number of trainable parameters to that when fine-tuning without
LoRA and PEFT.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>trainable<span class="w"> </span>params:<span class="w"> </span><span class="m">33</span>,554,432<span class="w"> </span><span class="o">||</span><span class="w"> </span>all<span class="w"> </span>params:<span class="w"> </span><span class="m">6</span>,771,970,048<span class="w"> </span><span class="o">||</span><span class="w"> </span>trainable%:<span class="w"> </span><span class="m">0</span>.49548996469513035
</pre></div>
</div>
</li>
<li><p>Initialize <code class="docutils literal notranslate"><span class="pre">SFTTrainer</span></code> with a PEFT LoRA configuration and run the trainer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize an SFT trainer.</span>
<span class="n">sft_trainer</span> <span class="o">=</span> <span class="n">SFTTrainer</span><span class="p">(</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">base_model</span><span class="p">,</span>
        <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">training_dataset</span><span class="p">,</span>
        <span class="n">peft_config</span> <span class="o">=</span> <span class="n">peft_config</span><span class="p">,</span>
        <span class="n">dataset_text_field</span> <span class="o">=</span> <span class="s2">"text"</span><span class="p">,</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">args</span> <span class="o">=</span> <span class="n">training_arguments</span>
<span class="p">)</span>

<span class="c1"># Run the trainer.</span>
<span class="n">sft_trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<p>The output should look like this:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="o">{</span><span class="s1">'loss'</span>:<span class="w"> </span><span class="m">1</span>.5973,<span class="w"> </span><span class="s1">'grad_norm'</span>:<span class="w"> </span><span class="m">0</span>.25271978974342346,<span class="w"> </span><span class="s1">'learning_rate'</span>:<span class="w"> </span>4e-05,<span class="w"> </span><span class="s1">'epoch'</span>:<span class="w"> </span><span class="m">0</span>.16<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>:<span class="w"> </span><span class="m">2</span>.0519,<span class="w"> </span><span class="s1">'grad_norm'</span>:<span class="w"> </span><span class="m">0</span>.21817368268966675,<span class="w"> </span><span class="s1">'learning_rate'</span>:<span class="w"> </span>4e-05,<span class="w"> </span><span class="s1">'epoch'</span>:<span class="w"> </span><span class="m">0</span>.32<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>:<span class="w"> </span><span class="m">1</span>.6147,<span class="w"> </span><span class="s1">'grad_norm'</span>:<span class="w"> </span><span class="m">0</span>.3046981394290924,<span class="w"> </span><span class="s1">'learning_rate'</span>:<span class="w"> </span>4e-05,<span class="w"> </span><span class="s1">'epoch'</span>:<span class="w"> </span><span class="m">0</span>.48<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>:<span class="w"> </span><span class="m">1</span>.4124,<span class="w"> </span><span class="s1">'grad_norm'</span>:<span class="w"> </span><span class="m">0</span>.11534837633371353,<span class="w"> </span><span class="s1">'learning_rate'</span>:<span class="w"> </span>4e-05,<span class="w"> </span><span class="s1">'epoch'</span>:<span class="w"> </span><span class="m">0</span>.64<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>:<span class="w"> </span><span class="m">1</span>.5627,<span class="w"> </span><span class="s1">'grad_norm'</span>:<span class="w"> </span><span class="m">0</span>.09108350425958633,<span class="w"> </span><span class="s1">'learning_rate'</span>:<span class="w"> </span>4e-05,<span class="w"> </span><span class="s1">'epoch'</span>:<span class="w"> </span><span class="m">0</span>.8<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>:<span class="w"> </span><span class="m">1</span>.417,<span class="w"> </span><span class="s1">'grad_norm'</span>:<span class="w"> </span><span class="m">0</span>.2536439299583435,<span class="w"> </span><span class="s1">'learning_rate'</span>:<span class="w"> </span>4e-05,<span class="w"> </span><span class="s1">'epoch'</span>:<span class="w"> </span><span class="m">0</span>.96<span class="o">}</span>
<span class="o">{</span><span class="s1">'train_runtime'</span>:<span class="w"> </span><span class="m">197</span>.4947,<span class="w"> </span><span class="s1">'train_samples_per_second'</span>:<span class="w"> </span><span class="m">5</span>.063,<span class="w"> </span><span class="s1">'train_steps_per_second'</span>:<span class="w"> </span><span class="m">0</span>.633,<span class="w"> </span><span class="s1">'train_loss'</span>:<span class="w"> </span><span class="m">1</span>.6194254455566406,<span class="w"> </span><span class="s1">'epoch'</span>:<span class="w"> </span><span class="m">1</span>.0<span class="o">}</span>
<span class="m">100</span>%<span class="p">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████<span class="p">|</span><span class="w"> </span><span class="m">125</span>/125<span class="w"> </span><span class="o">[</span><span class="m">03</span>:17&lt;<span class="m">00</span>:00,<span class="w">  </span><span class="m">1</span>.58s/it<span class="o">]</span>
</pre></div>
</div>
</li>
</ol>
</div>
<input id="sd-tab-item-11" name="sd-tab-set-5" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="without" for="sd-tab-item-11">
Fine-tuning without LoRA and PEFT</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Use the following code to get started.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">print_trainable_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="c1"># Prints the number of trainable parameters in the model.</span>
    <span class="n">trainable_params</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">all_param</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="n">all_param</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">trainable_params</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"trainable params: </span><span class="si">{</span><span class="n">trainable_params</span><span class="si">}</span><span class="s2"> || all params: </span><span class="si">{</span><span class="n">all_param</span><span class="si">}</span><span class="s2"> || trainable%: </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">trainable_params</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">all_param</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">sft_trainer</span><span class="o">.</span><span class="n">peft_config</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">print_trainable_parameters</span><span class="p">(</span><span class="n">sft_trainer</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>The output should look like this. Compare the number of trainable parameters to that when fine-tuning with LoRA
and PEFT.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>trainable<span class="w"> </span>params:<span class="w"> </span><span class="m">6</span>,738,415,616<span class="w"> </span><span class="o">||</span><span class="w"> </span>all<span class="w"> </span>params:<span class="w"> </span><span class="m">6</span>,738,415,616<span class="w"> </span><span class="o">||</span><span class="w"> </span>trainable%:<span class="w"> </span><span class="m">100</span>.00
</pre></div>
</div>
</li>
<li><p>Run the trainer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Trainer without LoRA config.</span>
<span class="n">trainer_full</span> <span class="o">=</span> <span class="n">SFTTrainer</span><span class="p">(</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">base_model</span><span class="p">,</span>
        <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">training_dataset</span><span class="p">,</span>
        <span class="n">dataset_text_field</span> <span class="o">=</span> <span class="s2">"text"</span><span class="p">,</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">args</span> <span class="o">=</span> <span class="n">training_arguments</span>
<span class="p">)</span>

<span class="c1"># Training.</span>
<span class="n">trainer_full</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<p>The output should look like this:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="o">{</span><span class="s1">'loss'</span>:<span class="w"> </span><span class="m">1</span>.5975,<span class="w"> </span><span class="s1">'grad_norm'</span>:<span class="w"> </span><span class="m">0</span>.25113457441329956,<span class="w"> </span><span class="s1">'learning_rate'</span>:<span class="w"> </span>4e-05,<span class="w"> </span><span class="s1">'epoch'</span>:<span class="w"> </span><span class="m">0</span>.16<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>:<span class="w"> </span><span class="m">2</span>.0524,<span class="w"> </span><span class="s1">'grad_norm'</span>:<span class="w"> </span><span class="m">0</span>.2180655151605606,<span class="w"> </span><span class="s1">'learning_rate'</span>:<span class="w"> </span>4e-05,<span class="w"> </span><span class="s1">'epoch'</span>:<span class="w"> </span><span class="m">0</span>.32<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>:<span class="w"> </span><span class="m">1</span>.6145,<span class="w"> </span><span class="s1">'grad_norm'</span>:<span class="w"> </span><span class="m">0</span>.2949850261211395,<span class="w"> </span><span class="s1">'learning_rate'</span>:<span class="w"> </span>4e-05,<span class="w"> </span><span class="s1">'epoch'</span>:<span class="w"> </span><span class="m">0</span>.48<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>:<span class="w"> </span><span class="m">1</span>.4118,<span class="w"> </span><span class="s1">'grad_norm'</span>:<span class="w"> </span><span class="m">0</span>.11036080121994019,<span class="w"> </span><span class="s1">'learning_rate'</span>:<span class="w"> </span>4e-05,<span class="w"> </span><span class="s1">'epoch'</span>:<span class="w"> </span><span class="m">0</span>.64<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>:<span class="w"> </span><span class="m">1</span>.5595,<span class="w"> </span><span class="s1">'grad_norm'</span>:<span class="w"> </span><span class="m">0</span>.08962831646203995,<span class="w"> </span><span class="s1">'learning_rate'</span>:<span class="w"> </span>4e-05,<span class="w"> </span><span class="s1">'epoch'</span>:<span class="w"> </span><span class="m">0</span>.8<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>:<span class="w"> </span><span class="m">1</span>.4119,<span class="w"> </span><span class="s1">'grad_norm'</span>:<span class="w"> </span><span class="m">0</span>.25422757863998413,<span class="w"> </span><span class="s1">'learning_rate'</span>:<span class="w"> </span>4e-05,<span class="w"> </span><span class="s1">'epoch'</span>:<span class="w"> </span><span class="m">0</span>.96<span class="o">}</span>
<span class="o">{</span><span class="s1">'train_runtime'</span>:<span class="w"> </span><span class="m">419</span>.5154,<span class="w"> </span><span class="s1">'train_samples_per_second'</span>:<span class="w"> </span><span class="m">2</span>.384,<span class="w"> </span><span class="s1">'train_steps_per_second'</span>:<span class="w"> </span><span class="m">0</span>.298,<span class="w"> </span><span class="s1">'train_loss'</span>:<span class="w"> </span><span class="m">1</span>.6171623611450194,<span class="w"> </span><span class="s1">'epoch'</span>:<span class="w"> </span><span class="m">1</span>.0<span class="o">}</span>
<span class="m">100</span>%<span class="p">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████<span class="p">|</span><span class="w"> </span><span class="m">125</span>/125<span class="w"> </span><span class="o">[</span><span class="m">06</span>:59&lt;<span class="m">00</span>:00,<span class="w">  </span><span class="m">3</span>.36s/it<span class="o">]</span>
</pre></div>
</div>
</li>
</ol>
</div>
</div>
<section id="saving-adapters-or-fully-fine-tuned-models">
<span id="fine-tuning-llms-single-gpu-saving"></span><h6 aria-level="7">Saving adapters or fully fine-tuned models<a class="headerlink" href="#saving-adapters-or-fully-fine-tuned-models" title="Link to this heading">#</a></h6>
<p>PEFT methods freeze the pre-trained model parameters during fine-tuning and add a smaller number of trainable
parameters, namely the adapters, on top of it. The adapters are trained to learn specific task information. The adapters
trained with PEFT are usually an order of magnitude smaller than the full base model, making them convenient to share,
store, and load.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-12" name="sd-tab-set-6" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="with" for="sd-tab-item-12">
Saving a PEFT adapter</label><div class="sd-tab-content docutils">
<p>If you’re using LoRA and PEFT, use the following code to save a PEFT adapter to your system once the fine-tuning
is completed.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># PEFT adapter name.</span>
<span class="n">adapter_name</span> <span class="o">=</span> <span class="s2">"llama-2-7b-enhanced-adapter"</span>

<span class="c1"># Save PEFT adapter.</span>
<span class="n">sft_trainer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">)</span>
</pre></div>
</div>
<p>The saved PEFT adapter should look like this on your system:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Access adapter directory.</span>
<span class="nb">cd</span><span class="w"> </span>llama-2-7b-enhanced-adapter

<span class="c1"># List all adapter files.</span>
README.md<span class="w">  </span>adapter_config.json<span class="w">  </span>adapter_model.safetensors
</pre></div>
</div>
</div>
<input id="sd-tab-item-13" name="sd-tab-set-6" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="without" for="sd-tab-item-13">
Saving a fully fine-tuned model</label><div class="sd-tab-content docutils">
<p>If you’re not using LoRA and PEFT so there is no PEFT LoRA configuration used for training, use the following code
to save your fine-tuned model to your system.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fully fine-tuned model name.</span>
<span class="n">new_model_name</span> <span class="o">=</span> <span class="s2">"llama-2-7b-enhanced"</span>

<span class="c1"># Save the fully fine-tuned model.</span>
<span class="n">full_trainer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">new_model_name</span><span class="p">)</span>
</pre></div>
</div>
<p>The saved new full model should look like this on your system:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Access new model directory.</span>
<span class="nb">cd</span><span class="w"> </span>llama-2-7b-enhanced

<span class="c1"># List all model files.</span>
config.json<span class="w">                       </span>model-00002-of-00006.safetensors<span class="w">  </span>model-00005-of-00006.safetensors
generation_config.json<span class="w">            </span>model-00003-of-00006.safetensors<span class="w">  </span>model-00006-of-00006.safetensors
model-00001-of-00006.safetensors<span class="w">  </span>model-00004-of-00006.safetensors<span class="w">  </span>model.safetensors.index.json
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>PEFT adapters can’t be loaded by <code class="docutils literal notranslate"><span class="pre">AutoModelForCausalLM</span></code> from the Transformers library as they do not contain
full model parameters and model configurations, for example, <code class="docutils literal notranslate"><span class="pre">config.json</span></code>. To use it as a normal transformer
model, you need to merge them into the base model.</p>
</div>
</section>
</section>
<section id="basic-model-inference">
<h6>Basic model inference<a class="headerlink" href="#basic-model-inference" title="Link to this heading">#</a></h6>
<p>A trained model can be classified into one of three types:</p>
<ul class="simple">
<li><p>A PEFT adapter</p></li>
<li><p>A pre-trained language model in Hugging Face</p></li>
<li><p>A fully fine-tuned model not using PEFT</p></li>
</ul>
<p>Let’s look at achieving model inference using these types of models.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-14" name="sd-tab-set-7" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-14">
Inference using PEFT adapters</label><div class="sd-tab-content docutils">
<p>To use PEFT adapters like a normal transformer model, you can run the generation by loading a base model along with PEFT
adapters as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">peft</span><span class="w"> </span><span class="kn">import</span> <span class="n">PeftModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>

<span class="c1"># Set the path of the model or the name on Hugging face hub</span>
<span class="n">base_model_name</span> <span class="o">=</span> <span class="s2">"meta-llama/Llama-2-7b-chat-hf"</span>

<span class="c1"># Set the path of the adapter</span>
<span class="n">adapter_name</span> <span class="o">=</span> <span class="s2">"Llama-2-7b-enhanced-adpater"</span>

<span class="c1"># Load base model</span>
<span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model_name</span><span class="p">)</span>

<span class="c1"># Adapt the base model with the adapter</span>
<span class="n">new_model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">)</span>

<span class="c1"># Then, run generation as the same with a normal model outlined in 2.1</span>
</pre></div>
</div>
<p>The PEFT library provides a <code class="docutils literal notranslate"><span class="pre">merge_and_unload</span></code> method, which merges the adapter layers into the base model. This is
needed if someone wants to save the adapted model into local storage and use it as a normal standalone model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load base model</span>
<span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model_name</span><span class="p">)</span>

<span class="c1"># Adapt the base model with the adapter</span>
<span class="n">new_model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">)</span>

<span class="c1"># Merge adapter</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">merge_and_unload</span><span class="p">()</span>

<span class="c1"># Save the merged model into local</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">"merged_adpaters"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-15" name="sd-tab-set-7" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-15">
Inference using pre-trained or fully fine-tuned models</label><div class="sd-tab-content docutils">
<p>If you have a fully fine-tuned model not using PEFT, you can load it like any other pre-trained language model in
<a class="reference external" href="https://huggingface.co/docs/hub/en/index">Hugging Face Hub</a> using the <a class="reference external" href="https://huggingface.co/docs/transformers/en/index">Transformers</a> library.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import relevant class for loading model and tokenizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="c1"># Set the pre-trained model name on Hugging face hub</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"meta-llama/Llama-2-7b-chat-hf"</span>

<span class="c1"># Set device type</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda:0"</span>

<span class="c1"># Load model and tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># Input prompt encoding</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">"What is a large language model?"</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Token generation</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

<span class="c1"># Outputs decoding</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>
</div>
<p>In addition, pipelines from Transformers offer simple APIs to use pre-trained models for different tasks, including
sentiment analysis, feature extraction, question answering and so on. You can use the pipeline abstraction to achieve
model inference easily.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import relevant class for loading model and tokenizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>

<span class="c1"># Set the path of your model or the name on Hugging face hub</span>
<span class="n">model_name_or_path</span> <span class="o">=</span> <span class="s2">"meta-llama/Llama-2-7b-chat-hf"</span>

<span class="c1"># Set pipeline</span>
<span class="c1"># A positive device value will run the model on associated CUDA device id</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Token generation</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pipe</span><span class="p">(</span><span class="s2">"What is a large language model?"</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s2">"generated_text"</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>If using multiple accelerators, see
<a class="reference internal" href="#fine-tuning-llms-multi-gpu-hugging-face-accelerate"><span class="std std-ref">Multi-accelerator fine-tuning and inference</span></a> to explore
popular libraries that simplify fine-tuning and inference in a multi-accelerator system.</p>
<p>Read more about inference frameworks like vLLM and Hugging Face TGI in
<a class="reference internal" href="#document-how-to/rocm-for-ai/inference/llm-inference-frameworks"><span class="doc">LLM inference frameworks</span></a>.</p>
</section>
</section>
<span id="document-how-to/rocm-for-ai/fine-tuning/multi-gpu-fine-tuning-and-inference"></span><section id="fine-tuning-and-inference-using-multiple-accelerators">
<h5>Fine-tuning and inference using multiple accelerators<a class="headerlink" href="#fine-tuning-and-inference-using-multiple-accelerators" title="Link to this heading">#</a></h5>
<p>This section explains how to fine-tune a model on a multi-accelerator system. See
<a class="reference internal" href="#document-how-to/rocm-for-ai/fine-tuning/single-gpu-fine-tuning-and-inference"><span class="doc">Single-accelerator fine-tuning</span></a> for a single accelerator or GPU setup.</p>
<section id="environment-setup">
<span id="fine-tuning-llms-multi-gpu-env"></span><h6>Environment setup<a class="headerlink" href="#environment-setup" title="Link to this heading">#</a></h6>
<p>This section was tested using the following hardware and software environment.</p>
<div class="pst-scrollable-table-container"><table class="table">
<tbody>
<tr class="row-odd"><th class="stub"><p>Hardware</p></th>
<td><p>4 AMD Instinct MI300X accelerators</p></td>
</tr>
<tr class="row-even"><th class="stub"><p>Software</p></th>
<td><p>ROCm 6.1, Ubuntu 22.04, PyTorch 2.1.2, Python 3.10</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>Libraries</p></th>
<td><p><code class="docutils literal notranslate"><span class="pre">transformers</span></code> <code class="docutils literal notranslate"><span class="pre">datasets</span></code> <code class="docutils literal notranslate"><span class="pre">accelerate</span></code> <code class="docutils literal notranslate"><span class="pre">huggingface-hub</span></code> <code class="docutils literal notranslate"><span class="pre">peft</span></code> <code class="docutils literal notranslate"><span class="pre">trl</span></code> <code class="docutils literal notranslate"><span class="pre">scipy</span></code></p></td>
</tr>
<tr class="row-even"><th class="stub"><p>Base model</p></th>
<td><p><code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-2-7b-chat-hf</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<section id="setting-up-the-base-implementation-environment">
<span id="fine-tuning-llms-multi-gpu-env-setup"></span><h6 aria-level="7">Setting up the base implementation environment<a class="headerlink" href="#setting-up-the-base-implementation-environment" title="Link to this heading">#</a></h6>
<ol class="arabic">
<li><p>Install PyTorch for ROCm. Refer to the
<a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/pytorch-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">PyTorch installation guide</span></a>. For consistent
installation, it’s recommended to use official ROCm prebuilt Docker images with the framework pre-installed.</p></li>
<li><p>In the Docker container, check the availability of ROCM-capable accelerators using the following command.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>rocm-smi<span class="w"> </span>--showproductname
</pre></div>
</div>
</li>
<li><p>Check that your accelerators are available to PyTorch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Is a ROCm-GPU detected? "</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"How many ROCm-GPUs are detected? "</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">())</span>
</pre></div>
</div>
<p>If successful, your output should look like this:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span>print<span class="o">(</span><span class="s2">"Is a ROCm-GPU detected? "</span>,<span class="w"> </span>torch.cuda.is_available<span class="o">())</span>
Is<span class="w"> </span>a<span class="w"> </span>ROCm-GPU<span class="w"> </span>detected?<span class="w">  </span>True
&gt;&gt;&gt;<span class="w"> </span>print<span class="o">(</span><span class="s2">"How many ROCm-GPUs are detected? "</span>,<span class="w"> </span>torch.cuda.device_count<span class="o">())</span>
How<span class="w"> </span>many<span class="w"> </span>ROCm-GPUs<span class="w"> </span>are<span class="w"> </span>detected?<span class="w">  </span><span class="m">4</span>
</pre></div>
</div>
</li>
</ol>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>During training and inference, you can check the memory usage by running the <code class="docutils literal notranslate"><span class="pre">rocm-smi</span></code> command in your terminal.
This tool helps you see shows which accelerators or GPUs are involved.</p>
</div>
</section>
</section>
<section id="hugging-face-accelerate-for-fine-tuning-and-inference">
<span id="fine-tuning-llms-multi-gpu-hugging-face-accelerate"></span><h6>Hugging Face Accelerate for fine-tuning and inference<a class="headerlink" href="#hugging-face-accelerate-for-fine-tuning-and-inference" title="Link to this heading">#</a></h6>
<p><a class="reference external" href="https://huggingface.co/docs/accelerate/en/index">Hugging Face Accelerate</a> is a library that simplifies turning raw
PyTorch code for a single accelerator into code for multiple accelerators for LLM fine-tuning and inference. It is
integrated with <a class="reference external" href="https://huggingface.co/docs/transformers/en/index">Transformers</a> allowing you to scale your PyTorch
code while maintaining performance and flexibility.</p>
<p>As a brief example of model fine-tuning and inference using multiple GPUs, let’s use Transformers and load in the Llama
2 7B model.</p>
<p>Here, let’s reuse the code in <a class="reference internal" href="#fine-tuning-llms-single-gpu-download-model-dataset"><span class="std std-ref">Single-accelerator fine-tuning</span></a>
to load the base model and tokenizer.</p>
<p>Now, it’s important to adjust how you load the model. Add the <code class="docutils literal notranslate"><span class="pre">device_map</span></code> parameter to your base model configuration.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="n">base_model_name</span> <span class="o">=</span> <span class="s2">"meta-llama/Llama-2-7b-chat-hf"</span>

<span class="c1"># Load base model to GPU memory</span>
<span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="n">base_model_name</span><span class="p">,</span>
        <span class="n">device_map</span> <span class="o">=</span> <span class="s2">"auto"</span><span class="p">,</span>
        <span class="n">trust_remote_code</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="o">...</span>
<span class="c1"># Run training</span>
<span class="n">sft_trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can let Accelerate handle the device map computation by setting <code class="docutils literal notranslate"><span class="pre">device_map</span></code> to one of the supported options
(<code class="docutils literal notranslate"><span class="pre">"auto"</span></code>, <code class="docutils literal notranslate"><span class="pre">"balanced"</span></code>, <code class="docutils literal notranslate"><span class="pre">"balanced_low_0"</span></code>, <code class="docutils literal notranslate"><span class="pre">"sequential"</span></code>).</p>
<p>It’s recommended to set the <code class="docutils literal notranslate"><span class="pre">device_map</span></code> parameter to <code class="docutils literal notranslate"><span class="pre">“auto”</span></code> to allow Accelerate to automatically and
efficiently allocate the model given the available resources (4 accelerators in this case).</p>
<p>When you have more GPU memory available than the model size, here is the difference between each <code class="docutils literal notranslate"><span class="pre">device_map</span></code>
option:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">"auto"</span></code> and <code class="docutils literal notranslate"><span class="pre">"balanced"</span></code> evenly split the model on all available GPUs, making it possible for you to use a
batch size greater than 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">"balanced_low_0"</span></code> evenly splits the model on all GPUs except the first
one, and only puts on GPU 0 what does not fit on the others. This
option is great when you need to use GPU 0 for some processing of the
outputs, like when using the generate function for Transformers
models.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">"sequential"</span></code> will fit what it can on GPU 0, then move on GPU 1 and so forth. Not all GPUs might be used.</p></li>
</ul>
</div>
<p>After loading the model in this way, the model is fully ready to use the resources available to it.</p>
</section>
<section id="torchtune-for-fine-tuning-and-inference">
<span id="fine-tuning-llms-multi-gpu-torchtune"></span><h6>torchtune for fine-tuning and inference<a class="headerlink" href="#torchtune-for-fine-tuning-and-inference" title="Link to this heading">#</a></h6>
<p><a class="reference external" href="https://pytorch.org/torchtune/main/">torchtune</a> is a PyTorch-native library for easy single and multi-accelerator or
GPU model fine-tuning and inference with LLMs.</p>
<ol class="arabic">
<li><p>Install torchtune using pip.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install torchtune with PyTorch release 2.2.2+</span>
pip<span class="w"> </span>install<span class="w"> </span>torchtune

<span class="c1"># To confirm that the package is installed correctly</span>
tune<span class="w"> </span>--help
</pre></div>
</div>
<p>The output should look like this:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>usage:<span class="w"> </span>tune<span class="w"> </span><span class="o">[</span>-h<span class="o">]</span><span class="w"> </span><span class="o">{</span>download,ls,cp,run,validate<span class="o">}</span><span class="w"> </span>...

Welcome<span class="w"> </span>to<span class="w"> </span>the<span class="w"> </span>TorchTune<span class="w"> </span>CLI!

options:
<span class="w">  </span>-h,<span class="w"> </span>--help<span class="w">            </span>show<span class="w"> </span>this<span class="w"> </span><span class="nb">help</span><span class="w"> </span>message<span class="w"> </span>and<span class="w"> </span><span class="nb">exit</span>

subcommands:
<span class="w">  </span><span class="o">{</span>download,ls,cp,run,validate<span class="o">}</span>
</pre></div>
</div>
</li>
<li><p>torchtune recipes are designed around easily composable components and workable training loops, with minimal abstraction
getting in the way of fine-tuning. Run <code class="docutils literal notranslate"><span class="pre">tune</span> <span class="pre">ls</span></code> to show built-in torchtune configuration recipes.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>RECIPE<span class="w">                                   </span>CONFIG
full_finetune_single_device<span class="w">              </span>llama2/7B_full_low_memory
<span class="w">                                         </span>llama3/8B_full_single_device
<span class="w">                                         </span>mistral/7B_full_low_memory
full_finetune_distributed<span class="w">                </span>llama2/7B_full
<span class="w">                                         </span>llama2/13B_full
<span class="w">                                         </span>llama3/8B_full
<span class="w">                                         </span>mistral/7B_full
<span class="w">                                         </span>gemma/2B_full
lora_finetune_single_device<span class="w">              </span>llama2/7B_lora_single_device
<span class="w">                                         </span>llama2/7B_qlora_single_device
<span class="w">                                         </span>llama3/8B_lora_single_device
<span class="w">                                         </span>llama3/8B_qlora_single_device
<span class="w">                                         </span>llama2/13B_qlora_single_device
<span class="w">                                         </span>mistral/7B_lora_single_device
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">RECIPE</span></code> column shows the easy-to-use and workable fine-tuning and inference recipes for popular fine-tuning
techniques (such as LoRA). The <code class="docutils literal notranslate"><span class="pre">CONFIG</span></code> column lists the YAML configurations for easily configuring training,
evaluation, quantization, or inference recipes.</p>
<p>The snippet shows the architecture of a model’s YAML configuration file:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># Model arguments</span>
<span class="nt">model</span><span class="p">:</span>
<span class="w">  </span><span class="nt">_component_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torchtune.models.llama2.lora_llama2_7b</span>
<span class="w">  </span><span class="nt">lora_attn_modules</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'q_proj'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'v_proj'</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">apply_lora_to_mlp</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">  </span><span class="nt">apply_lora_to_output</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">  </span><span class="nt">lora_rank</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
<span class="w">  </span><span class="nt">lora_alpha</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16</span>

<span class="nt">tokenizer</span><span class="p">:</span>
<span class="w">  </span><span class="nt">_component_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torchtune.models.llama2.llama2_tokenizer</span>
<span class="w">  </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/tmp/Llama-2-7b-hf/tokenizer.model</span>

<span class="c1"># Dataset and sampler</span>
<span class="nt">dataset</span><span class="p">:</span>
<span class="w">  </span><span class="nt">_component_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torchtune.datasets.alpaca_cleaned_dataset</span>
<span class="w">  </span><span class="nt">train_on_input</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
</pre></div>
</div>
</li>
<li><p>This configuration file defines the fine-tuning base model path, data set, hyper-parameters for optimizer and scheduler,
and training data type. To download the base model for fine-tuning, run the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>tune<span class="w"> </span>download<span class="w"> </span>meta-llama/Llama-2-7b-hf<span class="w"> </span>--output-dir<span class="w"> </span>/tmp/Llama-2-7b-hf<span class="w"> </span>--hf-token
</pre></div>
</div>
<p>The output directory argument for <code class="docutils literal notranslate"><span class="pre">--output-dir</span></code> should map the model path specified in YAML config file.</p>
</li>
<li><p>To launch <code class="docutils literal notranslate"><span class="pre">lora_finetune_distributed</span></code> on four devices, run the following
command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>tune<span class="w"> </span>run<span class="w"> </span>--nnodes<span class="w"> </span><span class="m">1</span><span class="w"> </span>--nproc_per_node<span class="w"> </span><span class="m">4</span><span class="w"> </span>lora_finetune_distributed<span class="w"> </span>--config<span class="w"> </span>llama2/7B_lora
</pre></div>
</div>
<p>If successful, you should something like the following output:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>INFO:torchtune.utils.logging:FSDP<span class="w"> </span>is<span class="w"> </span>enabled.<span class="w"> </span>Instantiating<span class="w"> </span>Model<span class="w"> </span>on<span class="w"> </span>CPU<span class="w"> </span><span class="k">for</span><span class="w"> </span>Rank<span class="w"> </span><span class="m">0</span><span class="w"> </span>...
INFO:torchtune.utils.logging:Model<span class="w"> </span>instantiation<span class="w"> </span>took<span class="w"> </span><span class="m">7</span>.32<span class="w"> </span>secs
INFO:torchtune.utils.logging:Memory<span class="w"> </span>Stats<span class="w"> </span>after<span class="w"> </span>model<span class="w"> </span>init:
<span class="o">{</span><span class="s1">'peak_memory_active'</span>:<span class="w"> </span><span class="m">9</span>.478172672,<span class="w"> </span><span class="s1">'peak_memory_alloc'</span>:<span class="w"> </span><span class="m">8</span>.953868288,<span class="w"> </span><span class="s1">'peak_memory_reserved'</span>:<span class="w"> </span><span class="m">11</span>.112808448<span class="o">}</span>
INFO:torchtune.utils.logging:Optimizer<span class="w"> </span>and<span class="w"> </span>loss<span class="w"> </span>are<span class="w"> </span>initialized.
INFO:torchtune.utils.logging:Dataset<span class="w"> </span>and<span class="w"> </span>Sampler<span class="w"> </span>are<span class="w"> </span>initialized.
INFO:torchtune.utils.logging:Learning<span class="w"> </span>rate<span class="w"> </span>scheduler<span class="w"> </span>is<span class="w"> </span>initialized.
<span class="m">1</span><span class="p">|</span><span class="m">111</span><span class="p">|</span>Loss:<span class="w"> </span><span class="m">1</span>.5790324211120605:<span class="w">   </span><span class="m">7</span>%<span class="p">|</span>█<span class="w">                                          </span><span class="p">|</span><span class="w"> </span><span class="m">114</span>/1618
</pre></div>
</div>
</li>
</ol>
<p>Read more about inference frameworks in <a class="reference internal" href="#document-how-to/rocm-for-ai/inference/llm-inference-frameworks"><span class="doc">LLM inference frameworks</span></a>.</p>
</section>
</section>
</div>
</section>
</div>
</section>
<span id="document-how-to/rocm-for-ai/inference/index"></span><section id="use-rocm-for-ai-inference">
<h3>Use ROCm for AI inference<a class="headerlink" href="#use-rocm-for-ai-inference" title="Link to this heading">#</a></h3>
<p>AI inference is a process of deploying a trained machine learning model to make predictions or classifications on new data. This commonly involves using the model with real-time data and making quick decisions based on the predictions made by the model.</p>
<p>Understanding the ROCm™ software platform’s architecture and capabilities is vital for running AI inference. By leveraging the ROCm platform’s capabilities, you can harness the power of high-performance computing and efficient resource management to run inference workloads, leading to faster predictions and classifications on real-time data.</p>
<p>Throughout the following topics, this section provides a comprehensive guide to setting up and deploying AI inference on AMD GPUs. This includes instructions on how to install ROCm, how to use Hugging Face Transformers to manage pre-trained models for natural language processing (NLP) tasks, how to validate vLLM on AMD Instinct™ MI300X accelerators and illustrate how to deploy trained models in production environments.</p>
<p>The AI Developer Hub contains <a class="reference external" href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/">AMD ROCm tutorials</a> for
training, fine-tuning, and inference. It leverages popular machine learning frameworks on AMD GPUs.</p>
<ul class="simple">
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/install"><span class="doc">Installing ROCm and machine learning frameworks</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/inference/hugging-face-models"><span class="doc">Running models from Hugging Face</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/inference/llm-inference-frameworks"><span class="doc">LLM inference frameworks</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/inference/benchmark-docker/vllm"><span class="doc">vLLM inference performance testing</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/inference/benchmark-docker/pytorch-inference"><span class="doc">PyTorch inference performance testing</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/inference/benchmark-docker/sglang"><span class="doc">SGLang inference performance testing</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/inference/deploy-your-model"><span class="doc">Deploying your model</span></a></p></li>
</ul>
<div class="toctree-wrapper compound">
<span id="document-how-to/rocm-for-ai/inference/hugging-face-models"></span><section id="running-models-from-hugging-face">
<h4>Running models from Hugging Face<a class="headerlink" href="#running-models-from-hugging-face" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://huggingface.co">Hugging Face</a> hosts the world’s largest AI model repository for developers to obtain
transformer models. Hugging Face models and tools significantly enhance productivity, performance, and accessibility in
developing and deploying AI solutions.</p>
<p>This section describes how to run popular community transformer models from Hugging Face on AMD accelerators and GPUs.</p>
<section id="using-hugging-face-transformers">
<span id="rocm-for-ai-hugging-face-transformers"></span><h5>Using Hugging Face Transformers<a class="headerlink" href="#using-hugging-face-transformers" title="Link to this heading">#</a></h5>
<p>First, <a class="reference external" href="https://huggingface.co/docs/transformers/en/installation">install the Hugging Face Transformers library</a>,
which lets you easily import any of the transformer models into your Python application.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>transformers
</pre></div>
</div>
<p>Here is an example of running <a class="reference external" href="https://huggingface.co/openai-community/gpt2">GPT2</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT2Tokenizer</span><span class="p">,</span> <span class="n">GPT2Model</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'gpt2'</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2Model</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'gpt2'</span><span class="p">)</span>

<span class="n">text</span> <span class="o">=</span> <span class="s2">"Replace me with any text you'd like."</span>

<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'pt'</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">encoded_input</span><span class="p">)</span>
</pre></div>
</div>
<p>Mainstream transformer models are regularly tested on supported hardware platforms. Models derived from those core
models should also function correctly.</p>
<p>Here are some mainstream models to get you started:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/bert-base-uncased">BERT</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/bigscience/bloom">BLOOM</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/huggyllama/llama-7b">Llama</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/facebook/opt-66b">OPT</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/t5-base">T5</a></p></li>
</ul>
</section>
<section id="using-hugging-face-with-optimum-amd">
<span id="rocm-for-ai-hugging-face-optimum"></span><h5>Using Hugging Face with Optimum-AMD<a class="headerlink" href="#using-hugging-face-with-optimum-amd" title="Link to this heading">#</a></h5>
<p>Optimum-AMD is the interface between Hugging Face libraries and the ROCm software stack.</p>
<p>For a deeper dive into using Hugging Face libraries on AMD accelerators and GPUs, refer to the
<a class="reference external" href="https://huggingface.co/docs/optimum/main/en/amd/amdgpu/overview">Optimum-AMD</a> page on Hugging Face for guidance on
using Flash Attention 2, GPTQ quantization and the ONNX Runtime integration.</p>
<p>Hugging Face libraries natively support AMD Instinct accelerators. For other
<a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">ROCm-capable hardware</span></a>, support is currently not
validated, but most features are expected to work without issues.</p>
<section id="installation">
<span id="rocm-for-ai-install-optimum-amd"></span><h6>Installation<a class="headerlink" href="#installation" title="Link to this heading">#</a></h6>
<p>Install Optimum-AMD using pip.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>--upgrade-strategy<span class="w"> </span>eager<span class="w"> </span>optimum<span class="o">[</span>amd<span class="o">]</span>
</pre></div>
</div>
<p>Or, install from source.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/huggingface/optimum-amd.git
<span class="nb">cd</span><span class="w"> </span>optimum-amd
pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span>.
</pre></div>
</div>
</section>
</section>
<section id="flash-attention">
<span id="rocm-for-ai-flash-attention"></span><h5>Flash Attention<a class="headerlink" href="#flash-attention" title="Link to this heading">#</a></h5>
<ol class="arabic">
<li><p>Use <a class="reference external" href="https://github.com/huggingface/optimum-amd/blob/main/docker/transformers-pytorch-amd-gpu-flash/Dockerfile">the Hugging Face team’s example Dockerfile</a> to use
Flash Attention with ROCm.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>build<span class="w"> </span>-f<span class="w"> </span>Dockerfile<span class="w"> </span>-t<span class="w"> </span>transformers_pytorch_amd_gpu_flash<span class="w"> </span>.
<span class="nv">volume</span><span class="o">=</span><span class="nv">$PWD</span>
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>--network<span class="o">=</span>host<span class="w"> </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span>--group-add<span class="o">=</span>video<span class="w"> </span>--ipc<span class="o">=</span>host<span class="w"> </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span>-v<span class="w"> </span><span class="nv">$volume</span>:/workspace<span class="w"> </span>--name<span class="w"> </span>transformer_amd
transformers_pytorch_amd_gpu_flash:latest
</pre></div>
</div>
</li>
<li><p>Use Flash Attention 2 with <a class="reference external" href="https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2">Transformers</a> by adding the
<code class="docutils literal notranslate"><span class="pre">use_flash_attention_2</span></code> parameter to <code class="docutils literal notranslate"><span class="pre">from_pretrained()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">LlamaForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"tiiuae/falcon-7b"</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">):</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
  <span class="s2">"tiiuae/falcon-7b"</span><span class="p">,</span>
  <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
  <span class="n">use_flash_attention_2</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
  <span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="gptq">
<span id="rocm-for-ai-gptq"></span><h5>GPTQ<a class="headerlink" href="#gptq" title="Link to this heading">#</a></h5>
<p>To enable <a class="reference external" href="https://arxiv.org/abs/2210.17323">GPTQ</a>, hosted wheels are available for ROCm.</p>
<ol class="arabic">
<li><p>First, <a class="reference internal" href="#rocm-for-ai-install-optimum-amd"><span class="std std-ref">install Optimum-AMD</span></a>.</p></li>
<li><p>Install AutoGPTQ using pip. Refer to <a class="reference external" href="https://github.com/AutoGPTQ/AutoGPTQ#Installation">AutoGPTQ Installation</a> for
in-depth guidance.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>auto-gptq<span class="w"> </span>--no-build-isolation<span class="w"> </span>--extra-index-url<span class="w"> </span>https://huggingface.github.io/autogptq-index/whl/rocm573/
</pre></div>
</div>
<p>Or, to install from source for AMD accelerators supporting ROCm, specify the <code class="docutils literal notranslate"><span class="pre">ROCM_VERSION</span></code> environment variable.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">ROCM_VERSION</span><span class="o">=</span><span class="m">6</span>.1<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>-vvv<span class="w"> </span>--no-build-isolation<span class="w"> </span>-e<span class="w"> </span>.
</pre></div>
</div>
</li>
<li><p>Load GPTQ-quantized models in Transformers using the backend <a class="reference external" href="https://github.com/PanQiWei/AutoGPTQ">AutoGPTQ library</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">LlamaForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"TheBloke/Llama-2-7B-Chat-GPTQ"</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">):</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
  <span class="s2">"TheBloke/Llama-2-7B-Chat-GPTQ"</span><span class="p">,</span>
  <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
  <span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="onnx">
<span id="rocm-for-ai-onnx"></span><h5>ONNX<a class="headerlink" href="#onnx" title="Link to this heading">#</a></h5>
<p>Hugging Face Optimum also supports the <a class="reference external" href="https://onnxruntime.ai">ONNX Runtime</a> integration. For ONNX models, usage is
straightforward.</p>
<ol class="arabic">
<li><p>Specify the provider argument in the <code class="docutils literal notranslate"><span class="pre">ORTModel.from_pretrained()</span></code> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">optimum.onnxruntime</span><span class="w"> </span><span class="kn">import</span> <span class="n">ORTModelForSequenceClassification</span>
<span class="o">..</span>
<span class="n">ort_model</span> <span class="o">=</span> <span class="n">ORTModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
<span class="o">..</span>
<span class="n">provider</span><span class="o">=</span><span class="s2">"ROCMExecutionProvider"</span>
<span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Try running a <a class="reference external" href="https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english">BERT text classification</a> ONNX model with ROCm:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">optimum.onnxruntime</span><span class="w"> </span><span class="kn">import</span> <span class="n">ORTModelForSequenceClassification</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">optimum.pipelines</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">onnxruntime</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ort</span>

<span class="n">session_options</span> <span class="o">=</span> <span class="n">ort</span><span class="o">.</span><span class="n">SessionOptions</span><span class="p">()</span>

<span class="n">session_options</span><span class="o">.</span><span class="n">log_severity_level</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">ort_model</span> <span class="o">=</span> <span class="n">ORTModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
   <span class="s2">"distilbert-base-uncased-finetuned-sst-2-english"</span><span class="p">,</span>
   <span class="n">export</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
   <span class="n">provider</span><span class="o">=</span><span class="s2">"ROCMExecutionProvider"</span><span class="p">,</span>
   <span class="n">session_options</span><span class="o">=</span><span class="n">session_options</span>
   <span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"distilbert-base-uncased-finetuned-sst-2-english"</span><span class="p">)</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"text-classification"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">ort_model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cuda:0"</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="s2">"Both the music and visual were astounding, not to mention the actors performance."</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>
<span id="document-how-to/rocm-for-ai/inference/llm-inference-frameworks"></span><section id="llm-inference-frameworks">
<h4>LLM inference frameworks<a class="headerlink" href="#llm-inference-frameworks" title="Link to this heading">#</a></h4>
<p>This section discusses how to implement <a class="reference external" href="https://docs.vllm.ai/en/latest">vLLM</a> and <a class="reference external" href="https://huggingface.co/docs/text-generation-inference/en/index">Hugging Face TGI</a> using
<a class="reference internal" href="#document-how-to/rocm-for-ai/fine-tuning/single-gpu-fine-tuning-and-inference"><span class="doc">single-accelerator</span></a> and
<a class="reference internal" href="#document-how-to/rocm-for-ai/fine-tuning/multi-gpu-fine-tuning-and-inference"><span class="doc">multi-accelerator</span></a> systems.</p>
<section id="vllm-inference">
<span id="fine-tuning-llms-vllm"></span><h5>vLLM inference<a class="headerlink" href="#vllm-inference" title="Link to this heading">#</a></h5>
<p>vLLM is renowned for its PagedAttention algorithm that can reduce memory consumption and increase throughput thanks to
its paging scheme. Instead of allocating GPU high-bandwidth memory (HBM) for the maximum output token lengths of the
models, the paged attention of vLLM allocates GPU HBM dynamically for its actual decoding lengths. This paged attention
is also effective when multiple requests share the same key and value contents for a large value of beam search or
multiple parallel requests.</p>
<p>vLLM also incorporates many modern LLM acceleration and quantization algorithms, such as Flash Attention, HIP and CUDA
graphs, tensor parallel multi-GPU, GPTQ, AWQ, and token speculation.</p>
<section id="installing-vllm">
<h6>Installing vLLM<a class="headerlink" href="#installing-vllm" title="Link to this heading">#</a></h6>
<ol class="arabic" id="fine-tuning-llms-vllm-rocm-docker-image">
<li><p>Run the following commands to build a Docker image <code class="docutils literal notranslate"><span class="pre">vllm-rocm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/vllm-project/vllm.git
<span class="nb">cd</span><span class="w"> </span>vllm
docker<span class="w"> </span>build<span class="w"> </span>-f<span class="w"> </span>docker/Dockerfile.rocm<span class="w"> </span>-t<span class="w"> </span>vllm-rocm<span class="w"> </span>.
</pre></div>
</div>
</li>
</ol>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-16" name="sd-tab-set-8" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="single" for="sd-tab-item-16">
vLLM on a single-accelerator system</label><div class="sd-tab-content docutils">
<ol class="arabic" start="2">
<li><p>To use vLLM as an API server to serve reference requests, first start a container using the <a class="reference internal" href="#fine-tuning-llms-vllm-rocm-docker-image"><span class="std std-ref">vllm-rocm
Docker image</span></a>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--network<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--group-add<span class="o">=</span>video<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="w"> </span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="w"> </span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>-v<span class="w"> </span>&lt;path/to/model&gt;:/app/model<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>vllm-rocm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>bash
</pre></div>
</div>
</li>
<li><p>Inside the container, start the API server to run on a single accelerator on port 8000 using the following command.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>vllm.entrypoints.api_server<span class="w"> </span>--model<span class="w"> </span>/app/model<span class="w"> </span>--dtype<span class="w"> </span>float16<span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
<p>The following log message is displayed in your command line indicates that the server is listening for requests.</p>
<img alt="vLLM API server log message" class="align-center" src="_images/vllm-single-gpu-log.png"/>
</li>
<li><p>To test, send it a curl request containing a prompt.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>http://localhost:8000/generate<span class="w"> </span>-H<span class="w"> </span><span class="s2">"Content-Type: application/json"</span><span class="w"> </span>-d<span class="w"> </span><span class="s1">'{"prompt": "What is AMD Instinct?", "max_tokens": 80, "temperature": 0.0 }'</span>
</pre></div>
</div>
<p>You should receive a response like the following.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{"text":["What is AMD Instinct?\nAmd Instinct is a brand new line of high-performance computing (HPC) processors from Advanced Micro Devices (AMD). These processors are designed to deliver unparalleled performance for HPC workloads, including scientific simulations, data analytics, and machine learning.\nThe Instinct lineup includes a range of processors, from the entry-level Inst"]}
</pre></div>
</div>
</li>
</ol>
</div>
<input id="sd-tab-item-17" name="sd-tab-set-8" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="multi" for="sd-tab-item-17">
vLLM on a multi-accelerator system</label><div class="sd-tab-content docutils">
<ol class="arabic" start="2">
<li><p>To use vLLM as an API server to serve reference requests, first start a container using the <a class="reference internal" href="#fine-tuning-llms-vllm-rocm-docker-image"><span class="std std-ref">vllm-rocm
Docker image</span></a>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--network<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--group-add<span class="o">=</span>video<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="w"> </span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="w"> </span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>-v<span class="w"> </span>&lt;path/to/model&gt;:/app/model<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>vllm-rocm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>bash
</pre></div>
</div>
</li>
<li><p>To run API server on multiple GPUs, use the <code class="docutils literal notranslate"><span class="pre">-tp</span></code>  or <code class="docutils literal notranslate"><span class="pre">--tensor-parallel-size</span></code>  parameter. For example, to use two
GPUs, start the API server using the following command.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>vllm.entrypoints.api_server<span class="w"> </span>--model<span class="w"> </span>/app/model<span class="w"> </span>--dtype<span class="w"> </span>float16<span class="w"> </span>-tp<span class="w"> </span><span class="m">2</span><span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
</li>
<li><p>To run multiple instances of API Servers, specify different ports for each server, and use <code class="docutils literal notranslate"><span class="pre">ROCR_VISIBLE_DEVICES</span></code> to
isolate each instance to a different accelerator.</p>
<p>For example, to run two API servers, one on port 8000 using GPU 0 and 1, one on port 8001 using GPU 2 and 3, use a
a command like the following.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">ROCR_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>vllm.entrypoints.api_server<span class="w"> </span>--model<span class="w"> </span>/data/llama-2-7b-chat-hf<span class="w"> </span>--dtype<span class="w"> </span>float16<span class="w"> </span>–tp<span class="w"> </span><span class="m">2</span><span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="p">&amp;</span>
<span class="nv">ROCR_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">2</span>,3<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>vllm.entrypoints.api_server<span class="w"> </span>--model<span class="w"> </span>/data/llama-2-7b-chat-hf<span class="w"> </span>--dtype<span class="w"> </span>float16<span class="w"> </span>–tp<span class="w"> </span><span class="m">2</span>--port<span class="w"> </span><span class="m">8001</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
</li>
<li><p>To test, send it a curl request containing a prompt.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>http://localhost:8000/generate<span class="w"> </span>-H<span class="w"> </span><span class="s2">"Content-Type: application/json"</span><span class="w"> </span>-d<span class="w"> </span><span class="s1">'{"prompt": "What is AMD Instinct?", "max_tokens": 80, "temperature": 0.0 }'</span>
</pre></div>
</div>
<p>You should receive a response like the following.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{"text":["What is AMD Instinct?\nAmd Instinct is a brand new line of high-performance computing (HPC) processors from Advanced Micro Devices (AMD). These processors are designed to deliver unparalleled performance for HPC workloads, including scientific simulations, data analytics, and machine learning.\nThe Instinct lineup includes a range of processors, from the entry-level Inst"]}
</pre></div>
</div>
</li>
</ol>
</div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See <a class="reference internal" href="#mi300x-vllm-optimization"><span class="std std-ref">vLLM performance optimization</span></a> for performance optimization tips.</p>
<p>ROCm provides a prebuilt optimized Docker image for validating the performance of LLM inference with vLLM
on the MI300X accelerator. The Docker image includes ROCm, vLLM, and PyTorch.
For more information, see <a class="reference internal" href="#document-how-to/rocm-for-ai/inference/benchmark-docker/vllm"><span class="doc">vLLM inference performance testing</span></a>.</p>
</div>
</section>
</section>
<section id="fine-tuning-llms-tgi">
<span id="id1"></span><h5>Hugging Face TGI<a class="headerlink" href="#fine-tuning-llms-tgi" title="Link to this heading">#</a></h5>
<p>Text Generation Inference (TGI) is LLM serving framework from Hugging
Face, and it also supports the majority of high-performance LLM
acceleration algorithms such as Flash Attention, Paged Attention,
CUDA/HIP graph, tensor parallel multi-GPU, GPTQ, AWQ, and token
speculation.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>In addition to LLM serving capability, TGI also provides the <a class="reference external" href="https://github.com/huggingface/text-generation-inference/blob/main/benchmark/README.md">Text Generation Inference benchmarking tool</a>.</p>
</div>
<section id="install-tgi">
<h6>Install TGI<a class="headerlink" href="#install-tgi" title="Link to this heading">#</a></h6>
<ol class="arabic">
<li><p>Launch the TGI Docker container in the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>--name<span class="w"> </span>tgi<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined
--device<span class="o">=</span>/dev/kfd<span class="w"> </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span>--group-add<span class="w"> </span>video<span class="w"> </span>--ipc<span class="o">=</span>host<span class="w"> </span>--shm-size<span class="w"> </span>256g
--net<span class="w"> </span>host<span class="w"> </span>-v<span class="w"> </span><span class="nv">$PWD</span>:/data
--entrypoint<span class="w"> </span><span class="s2">"/bin/bash"</span>
--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/data
ghcr.io/huggingface/text-generation-inference:latest-rocm
</pre></div>
</div>
</li>
</ol>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-18" name="sd-tab-set-9" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="single" for="sd-tab-item-18">
TGI on a single-accelerator system</label><div class="sd-tab-content docutils">
<ol class="arabic" start="2">
<li><p>Inside the container, launch a model using TGI server on a single accelerator.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">ROCM_USE_FLASH_ATTN_V2_TRITON</span><span class="o">=</span>True
text-generation-launcher<span class="w"> </span>--model-id<span class="w"> </span>NousResearch/Meta-Llama-3-70B<span class="w"> </span>--dtype<span class="w"> </span>float16<span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
</li>
<li><p>To test, send it a curl request containing a prompt.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>http://localhost:8000/generate_stream<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>-d<span class="w"> </span><span class="s1">'{"inputs":"What is AMD Instinct?","parameters":{"max_new_tokens":20}}'</span><span class="w"> </span>-H<span class="w"> </span><span class="s1">'Content-Type: application/json'</span>
</pre></div>
</div>
<p>You should receive a response like the following.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>data:<span class="o">{</span><span class="s2">"index"</span>:20,<span class="s2">"token"</span>:<span class="o">{</span><span class="s2">"id"</span>:304,<span class="s2">"text"</span>:<span class="s2">" in"</span>,<span class="s2">"logprob"</span>:-1.2822266,<span class="s2">"special"</span>:false<span class="o">}</span>,<span class="s2">"generated_text"</span>:<span class="s2">" AMD Instinct is a new family of data center GPUs designed to accelerate the most demanding workloads in"</span>,<span class="s2">"details"</span>:null<span class="o">}</span>
</pre></div>
</div>
</li>
</ol>
</div>
<input id="sd-tab-item-19" name="sd-tab-set-9" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-19">
TGI on a multi-accelerator system</label><div class="sd-tab-content docutils">
<ol class="arabic" start="2">
<li><p>Inside the container, launch a model using TGI server on multiple accelerators (4 in this case).</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">ROCM_USE_FLASH_ATTN_V2_TRITON</span><span class="o">=</span>True
text-generation-launcher<span class="w"> </span>--model-id<span class="w"> </span>NousResearch/Meta-Llama-3-8B<span class="w"> </span>--dtype<span class="w"> </span>float16<span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span><span class="w"> </span>--num-shard<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
</li>
<li><p>To test, send it a curl request containing a prompt.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>http://localhost:8000/generate_stream<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>-d<span class="w"> </span><span class="s1">'{"inputs":"What is AMD Instinct?","parameters":{"max_new_tokens":20}}'</span><span class="w"> </span>-H<span class="w"> </span><span class="s1">'Content-Type: application/json'</span>
</pre></div>
</div>
<p>You should receive a response like the following.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>data:<span class="o">{</span><span class="s2">"index"</span>:20,<span class="s2">"token"</span>:<span class="o">{</span><span class="s2">"id"</span>:304,<span class="s2">"text"</span>:<span class="s2">" in"</span>,<span class="s2">"logprob"</span>:-1.2773438,<span class="s2">"special"</span>:false<span class="o">}</span>,<span class="s2">"generated_text"</span>:<span class="s2">" AMD Instinct is a new family of data center GPUs designed to accelerate the most demanding workloads in"</span>,<span class="s2">"details"</span>:null<span class="o">}</span>
</pre></div>
</div>
</li>
</ol>
</div>
</div>
</section>
</section>
</section>
<span id="document-how-to/rocm-for-ai/inference/benchmark-docker/vllm"></span><section id="vllm-inference-performance-testing">
<h4>vLLM inference performance testing<a class="headerlink" href="#vllm-inference-performance-testing" title="Link to this heading">#</a></h4>
<p id="vllm-benchmark-unified-docker">The <a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">ROCm vLLM Docker</a> image offers
a prebuilt, optimized environment for validating large language model (LLM)
inference performance on AMD Instinct™ MI300X series accelerators. This ROCm vLLM
Docker image integrates vLLM and PyTorch tailored specifically for MI300X series
accelerators and includes the following components:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Software component</p></th>
<th class="head"><p>Version</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ROCm/ROCm">ROCm</a></p></td>
<td><p>6.4.1</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://docs.vllm.ai/en/latest">vLLM</a></p></td>
<td><p>0.9.1 (0.9.2.dev364+gb432b7a28.rocm641)</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ROCm/pytorch">PyTorch</a></p></td>
<td><p>2.7.0+gitf717b2a</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/ROCm/hipBLASLt">hipBLASLt</a></p></td>
<td><p>0.15</p></td>
</tr>
</tbody>
</table>
</div>
<p>With this Docker image, you can quickly test the <a class="reference internal" href="#vllm-benchmark-performance-measurements"><span class="std std-ref">expected
inference performance numbers</span></a> for
MI300X series accelerators.</p>
<section id="what-s-new">
<h5>What’s new<a class="headerlink" href="#what-s-new" title="Link to this heading">#</a></h5>
<p>The following is summary of notable changes since the <a class="reference internal" href="#document-how-to/rocm-for-ai/inference/benchmark-docker/previous-versions/vllm-history"><span class="doc">previous ROCm/vLLM Docker release</span></a>.</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">--compilation-config-parameter</span></code> is no longer required as its options are now enabled by default.
This parameter has been removed from the benchmarking script.</p></li>
<li><p>Resolved Llama 3.1 405 B custom all-reduce issue, eliminating the need for <code class="docutils literal notranslate"><span class="pre">--disable-custom-all-reduce</span></code>.
This parameter has been removed from the benchmarking script.</p></li>
<li><p>Fixed a <code class="docutils literal notranslate"><span class="pre">+rms_norm</span></code> custom kernel issue.</p></li>
<li><p>Added quick reduce functionality. Set <code class="docutils literal notranslate"><span class="pre">VLLM_ROCM_QUICK_REDUCE_QUANTIZATION=FP</span></code> to enable; supported modes are <code class="docutils literal notranslate"><span class="pre">FP</span></code>, <code class="docutils literal notranslate"><span class="pre">INT8</span></code>, <code class="docutils literal notranslate"><span class="pre">INT6</span></code>, <code class="docutils literal notranslate"><span class="pre">INT4</span></code>.</p></li>
<li><p>Implemented a workaround to potentially mitigate GPU crashes experienced with the Command R+ model, pending a driver fix.</p></li>
</ul>
</section>
<section id="supported-models">
<h5>Supported models<a class="headerlink" href="#supported-models" title="Link to this heading">#</a></h5>
<p id="vllm-benchmark-available-models">The following models are supported for inference performance benchmarking
with vLLM and ROCm. Some instructions, commands, and recommendations in this
documentation might vary by model – select one to get started.</p>
<div class="container-fluid" id="vllm-benchmark-ud-params-picker">
<div class="row">
<div class="col-2 me-2 model-param-head">Model group</div>
<div class="row col-10">
<div class="col-3 model-param" data-param-k="model-group" data-param-v="llama" tabindex="0">Meta Llama</div>
<div class="col-3 model-param" data-param-k="model-group" data-param-v="mistral" tabindex="0">Mistral AI</div>
<div class="col-3 model-param" data-param-k="model-group" data-param-v="qwen" tabindex="0">Qwen</div>
<div class="col-3 model-param" data-param-k="model-group" data-param-v="dbrx" tabindex="0">Databricks DBRX</div>
<div class="col-3 model-param" data-param-k="model-group" data-param-v="gemma" tabindex="0">Google Gemma</div>
<div class="col-3 model-param" data-param-k="model-group" data-param-v="cohere" tabindex="0">Cohere</div>
<div class="col-3 model-param" data-param-k="model-group" data-param-v="deepseek" tabindex="0">DeepSeek</div>
<div class="col-3 model-param" data-param-k="model-group" data-param-v="phi" tabindex="0">Microsoft Phi</div>
<div class="col-3 model-param" data-param-k="model-group" data-param-v="falcon" tabindex="0">TII Falcon</div>
</div>
</div>
<div class="row mt-1">
<div class="col-2 me-2 model-param-head">Model</div>
<div class="row col-10">
<div class="col-6 model-param" data-param-group="llama" data-param-k="model" data-param-v="pyt_vllm_llama-3.1-8b" tabindex="0">Llama 3.1 8B</div>
<div class="col-6 model-param" data-param-group="llama" data-param-k="model" data-param-v="pyt_vllm_llama-3.1-70b" tabindex="0">Llama 3.1 70B</div>
<div class="col-6 model-param" data-param-group="llama" data-param-k="model" data-param-v="pyt_vllm_llama-3.1-405b" tabindex="0">Llama 3.1 405B</div>
<div class="col-6 model-param" data-param-group="llama" data-param-k="model" data-param-v="pyt_vllm_llama-2-7b" tabindex="0">Llama 2 7B</div>
<div class="col-6 model-param" data-param-group="llama" data-param-k="model" data-param-v="pyt_vllm_llama-2-70b" tabindex="0">Llama 2 70B</div>
<div class="col-6 model-param" data-param-group="llama" data-param-k="model" data-param-v="pyt_vllm_llama-3.1-8b_fp8" tabindex="0">Llama 3.1 8B FP8</div>
<div class="col-6 model-param" data-param-group="llama" data-param-k="model" data-param-v="pyt_vllm_llama-3.1-70b_fp8" tabindex="0">Llama 3.1 70B FP8</div>
<div class="col-6 model-param" data-param-group="llama" data-param-k="model" data-param-v="pyt_vllm_llama-3.1-405b_fp8" tabindex="0">Llama 3.1 405B FP8</div>
<div class="col-4 model-param" data-param-group="mistral" data-param-k="model" data-param-v="pyt_vllm_mixtral-8x7b" tabindex="0">Mixtral MoE 8x7B</div>
<div class="col-4 model-param" data-param-group="mistral" data-param-k="model" data-param-v="pyt_vllm_mixtral-8x22b" tabindex="0">Mixtral MoE 8x22B</div>
<div class="col-4 model-param" data-param-group="mistral" data-param-k="model" data-param-v="pyt_vllm_mistral-7b" tabindex="0">Mistral 7B</div>
<div class="col-4 model-param" data-param-group="mistral" data-param-k="model" data-param-v="pyt_vllm_mixtral-8x7b_fp8" tabindex="0">Mixtral MoE 8x7B FP8</div>
<div class="col-4 model-param" data-param-group="mistral" data-param-k="model" data-param-v="pyt_vllm_mixtral-8x22b_fp8" tabindex="0">Mixtral MoE 8x22B FP8</div>
<div class="col-4 model-param" data-param-group="mistral" data-param-k="model" data-param-v="pyt_vllm_mistral-7b_fp8" tabindex="0">Mistral 7B FP8</div>
<div class="col-4 model-param" data-param-group="qwen" data-param-k="model" data-param-v="pyt_vllm_qwen2-7b" tabindex="0">Qwen2 7B</div>
<div class="col-4 model-param" data-param-group="qwen" data-param-k="model" data-param-v="pyt_vllm_qwen2-72b" tabindex="0">Qwen2 72B</div>
<div class="col-4 model-param" data-param-group="qwen" data-param-k="model" data-param-v="pyt_vllm_qwq-32b" tabindex="0">QwQ-32B</div>
<div class="col-6 model-param" data-param-group="dbrx" data-param-k="model" data-param-v="pyt_vllm_dbrx-instruct" tabindex="0">DBRX Instruct</div>
<div class="col-6 model-param" data-param-group="dbrx" data-param-k="model" data-param-v="pyt_vllm_dbrx_fp8" tabindex="0">DBRX Instruct FP8</div>
<div class="col-6 model-param" data-param-group="gemma" data-param-k="model" data-param-v="pyt_vllm_gemma-2-27b" tabindex="0">Gemma 2 27B</div>
<div class="col-6 model-param" data-param-group="cohere" data-param-k="model" data-param-v="pyt_vllm_c4ai-command-r-plus-08-2024" tabindex="0">C4AI Command R+ 08-2024</div>
<div class="col-6 model-param" data-param-group="cohere" data-param-k="model" data-param-v="pyt_vllm_command-r-plus_fp8" tabindex="0">C4AI Command R+ 08-2024 FP8</div>
<div class="col-6 model-param" data-param-group="deepseek" data-param-k="model" data-param-v="pyt_vllm_deepseek-moe-16b-chat" tabindex="0">DeepSeek MoE 16B</div>
<div class="col-6 model-param" data-param-group="phi" data-param-k="model" data-param-v="pyt_vllm_phi-4" tabindex="0">Phi-4</div>
<div class="col-6 model-param" data-param-group="falcon" data-param-k="model" data-param-v="pyt_vllm_falcon-180b" tabindex="0">Falcon 180B</div>
</div>
</div>
</div><div class="model-doc pyt-vllm-llama-3-1-8b docutils container" id="vllm-benchmark-vllm">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.1-8B">Llama 3.1 8B model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-llama-3-1-70b docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct">Llama 3.1 70B model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-llama-3-1-405b docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct">Llama 3.1 405B model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-llama-2-7b docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf">Llama 2 7B model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-llama-2-70b docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-70b-chat-hf">Llama 2 70B model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-llama-3-1-8b-fp8 docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/amd/Llama-3.1-8B-Instruct-FP8-KV">Llama 3.1 8B FP8 model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-llama-3-1-70b-fp8 docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/amd/Llama-3.1-70B-Instruct-FP8-KV">Llama 3.1 70B FP8 model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-llama-3-1-405b-fp8 docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/amd/Llama-3.1-405B-Instruct-FP8-KV">Llama 3.1 405B FP8 model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-mixtral-8x7b docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1">Mixtral MoE 8x7B model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-mixtral-8x22b docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1">Mixtral MoE 8x22B model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-mistral-7b docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3">Mistral 7B model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-mixtral-8x7b-fp8 docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/amd/Mixtral-8x7B-Instruct-v0.1-FP8-KV">Mixtral MoE 8x7B FP8 model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-mixtral-8x22b-fp8 docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/amd/Mixtral-8x22B-Instruct-v0.1-FP8-KV">Mixtral MoE 8x22B FP8 model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-mistral-7b-fp8 docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/amd/Mistral-7B-v0.1-FP8-KV">Mistral 7B FP8 model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-qwen2-7b docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/Qwen/Qwen2-7B-Instruct">Qwen2 7B model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-qwen2-72b docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/Qwen/Qwen2-72B-Instruct">Qwen2 72B model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-qwq-32b docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/Qwen/QwQ-32B">QwQ-32B model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-dbrx-instruct docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/databricks/dbrx-instruct">DBRX Instruct model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-dbrx-fp8 docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/amd/dbrx-instruct-FP8-KV">DBRX Instruct FP8 model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-gemma-2-27b docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/google/gemma-2-27b">Gemma 2 27B model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-c4ai-command-r-plus-08-2024 docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/CohereForAI/c4ai-command-r-plus-08-2024">C4AI Command R+ 08-2024 model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-command-r-plus-fp8 docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/amd/c4ai-command-r-plus-FP8-KV">C4AI Command R+ 08-2024 FP8 model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-deepseek-moe-16b-chat docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/deepseek-ai/deepseek-moe-16b-chat">DeepSeek MoE 16B model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-phi-4 docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/microsoft/phi-4">Phi-4 model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-vllm-falcon-180b docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/tiiuae/falcon-180B">Falcon 180B model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization prior to use via an external license agreement through a third party.</p>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>vLLM is a toolkit and library for LLM inference and serving. AMD implements
high-performance custom kernels and modules in vLLM to enhance performance.
See <a class="reference internal" href="#fine-tuning-llms-vllm"><span class="std std-ref">vLLM inference</span></a> and <a class="reference internal" href="#mi300x-vllm-optimization"><span class="std std-ref">vLLM performance optimization</span></a> for
more information.</p>
</div>
</section>
<section id="performance-measurements">
<span id="vllm-benchmark-performance-measurements"></span><h5>Performance measurements<a class="headerlink" href="#performance-measurements" title="Link to this heading">#</a></h5>
<p>To evaluate performance, the
<a class="reference external" href="https://www.amd.com/en/developer/resources/rocm-hub/dev-ai/performance-results.html">Performance results with AMD ROCm software</a>
page provides reference throughput and latency measurements for inferencing popular AI models.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The performance data presented in
<a class="reference external" href="https://www.amd.com/en/developer/resources/rocm-hub/dev-ai/performance-results.html">Performance results with AMD ROCm software</a>
only reflects the latest version of this inference benchmarking environment.
The listed measurements should not be interpreted as the peak performance achievable by AMD Instinct MI325X and MI300X accelerators or ROCm software.</p>
</div>
</section>
<section id="system-validation">
<h5>System validation<a class="headerlink" href="#system-validation" title="Link to this heading">#</a></h5>
<p>Before running AI workloads, it’s important to validate that your AMD hardware is configured
correctly and performing optimally.</p>
<p>If you have already validated your system settings, including aspects like NUMA auto-balancing, you
can skip this step. Otherwise, complete the procedures in the <a class="reference internal" href="#rocm-for-ai-system-optimization"><span class="std std-ref">System validation and
optimization</span></a> guide to properly configure your system settings
before starting training.</p>
<p>To test for optimal performance, consult the recommended <a class="reference internal" href="#rocm-for-ai-system-health-bench"><span class="std std-ref">System health benchmarks</span></a>. This suite of tests will help you verify and fine-tune your
system’s configuration.</p>
<section id="pull-the-docker-image">
<h6>Pull the Docker image<a class="headerlink" href="#pull-the-docker-image" title="Link to this heading">#</a></h6>
<p>Download the <a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">ROCm vLLM Docker image</a>.
Use the following command to pull the Docker image from Docker Hub.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</section>
<section id="benchmarking">
<h6>Benchmarking<a class="headerlink" href="#benchmarking" title="Link to this heading">#</a></h6>
<p>Once the setup is complete, choose between two options to reproduce the
benchmark results:</p>
<div class="model-doc pyt-vllm-llama-3-1-8b docutils container" id="vllm-benchmark-mad">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-20" name="sd-tab-set-10" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-20">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.1-8B">Llama 3.1 8B</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_llama-3.1-8b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_llama-3.1-8b</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float16/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-21" name="sd-tab-set-10" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-21">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m meta-llama/Llama-3.1-8B-Instruct \
    -g $num_gpu \
    -d float16
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the Llama 3.1 8B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mf">3.1</span><span class="o">-</span><span class="mi">8</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float16</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Llama-3.1-8B-Instruct_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the Llama 3.1 8B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float16
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Llama-3.1-8B-Instruct_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-llama-3-1-70b docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-22" name="sd-tab-set-11" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-22">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct">Llama 3.1 70B</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_llama-3.1-70b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_llama-3.1-70b</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float16/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-23" name="sd-tab-set-11" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-23">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m meta-llama/Llama-3.1-70B-Instruct \
    -g $num_gpu \
    -d float16
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the Llama 3.1 70B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mf">3.1</span><span class="o">-</span><span class="mi">70</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float16</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Llama-3.1-70B-Instruct_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the Llama 3.1 70B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>meta-llama/Llama-3.1-70B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float16
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Llama-3.1-70B-Instruct_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-llama-3-1-405b docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-24" name="sd-tab-set-12" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-24">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct">Llama 3.1 405B</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_llama-3.1-405b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_llama-3.1-405b</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float16/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-25" name="sd-tab-set-12" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-25">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m meta-llama/Llama-3.1-405B-Instruct \
    -g $num_gpu \
    -d float16
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the Llama 3.1 405B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mf">3.1</span><span class="o">-</span><span class="mi">405</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float16</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Llama-3.1-405B-Instruct_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the Llama 3.1 405B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>meta-llama/Llama-3.1-405B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float16
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Llama-3.1-405B-Instruct_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-llama-2-7b docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-26" name="sd-tab-set-13" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-26">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf">Llama 2 7B</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_llama-2-7b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_llama-2-7b</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float16/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-27" name="sd-tab-set-13" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-27">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m meta-llama/Llama-2-7b-chat-hf \
    -g $num_gpu \
    -d float16
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the Llama 2 7B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">7</span><span class="n">b</span><span class="o">-</span><span class="n">chat</span><span class="o">-</span><span class="n">hf</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float16</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Llama-2-7b-chat-hf_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the Llama 2 7B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>meta-llama/Llama-2-7b-chat-hf<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float16
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Llama-2-7b-chat-hf_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-llama-2-70b docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-28" name="sd-tab-set-14" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-28">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-70b-chat-hf">Llama 2 70B</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_llama-2-70b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_llama-2-70b</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float16/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-29" name="sd-tab-set-14" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-29">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m meta-llama/Llama-2-70b-chat-hf \
    -g $num_gpu \
    -d float16
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the Llama 2 70B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">70</span><span class="n">b</span><span class="o">-</span><span class="n">chat</span><span class="o">-</span><span class="n">hf</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float16</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Llama-2-70b-chat-hf_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the Llama 2 70B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>meta-llama/Llama-2-70b-chat-hf<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float16
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Llama-2-70b-chat-hf_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-llama-3-1-8b-fp8 docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-30" name="sd-tab-set-15" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-30">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/amd/Llama-3.1-8B-Instruct-FP8-KV">Llama 3.1 8B FP8</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float8</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_llama-3.1-8b_fp8<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_llama-3.1-8b_fp8</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float8/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-31" name="sd-tab-set-15" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-31">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m amd/Llama-3.1-8B-Instruct-FP8-KV \
    -g $num_gpu \
    -d float8
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the Llama 3.1 8B FP8 model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float8</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">amd</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mf">3.1</span><span class="o">-</span><span class="mi">8</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span><span class="o">-</span><span class="n">FP8</span><span class="o">-</span><span class="n">KV</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float8</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float8_vllm_rocm6.4.1/summary/Llama-3.1-8B-Instruct-FP8-KV_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the Llama 3.1 8B FP8 model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float8</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>amd/Llama-3.1-8B-Instruct-FP8-KV<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float8
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float8_vllm_rocm6.4.1/summary/Llama-3.1-8B-Instruct-FP8-KV_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-llama-3-1-70b-fp8 docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-32" name="sd-tab-set-16" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-32">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/amd/Llama-3.1-70B-Instruct-FP8-KV">Llama 3.1 70B FP8</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float8</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_llama-3.1-70b_fp8<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_llama-3.1-70b_fp8</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float8/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-33" name="sd-tab-set-16" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-33">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m amd/Llama-3.1-70B-Instruct-FP8-KV \
    -g $num_gpu \
    -d float8
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the Llama 3.1 70B FP8 model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float8</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">amd</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mf">3.1</span><span class="o">-</span><span class="mi">70</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span><span class="o">-</span><span class="n">FP8</span><span class="o">-</span><span class="n">KV</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float8</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float8_vllm_rocm6.4.1/summary/Llama-3.1-70B-Instruct-FP8-KV_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the Llama 3.1 70B FP8 model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float8</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>amd/Llama-3.1-70B-Instruct-FP8-KV<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float8
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float8_vllm_rocm6.4.1/summary/Llama-3.1-70B-Instruct-FP8-KV_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-llama-3-1-405b-fp8 docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-34" name="sd-tab-set-17" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-34">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/amd/Llama-3.1-405B-Instruct-FP8-KV">Llama 3.1 405B FP8</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float8</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_llama-3.1-405b_fp8<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_llama-3.1-405b_fp8</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float8/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-35" name="sd-tab-set-17" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-35">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m amd/Llama-3.1-405B-Instruct-FP8-KV \
    -g $num_gpu \
    -d float8
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the Llama 3.1 405B FP8 model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float8</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">amd</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mf">3.1</span><span class="o">-</span><span class="mi">405</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span><span class="o">-</span><span class="n">FP8</span><span class="o">-</span><span class="n">KV</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float8</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float8_vllm_rocm6.4.1/summary/Llama-3.1-405B-Instruct-FP8-KV_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the Llama 3.1 405B FP8 model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float8</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>amd/Llama-3.1-405B-Instruct-FP8-KV<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float8
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float8_vllm_rocm6.4.1/summary/Llama-3.1-405B-Instruct-FP8-KV_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-mixtral-8x7b docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-36" name="sd-tab-set-18" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-36">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1">Mixtral MoE 8x7B</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_mixtral-8x7b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_mixtral-8x7b</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float16/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-37" name="sd-tab-set-18" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-37">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m mistralai/Mixtral-8x7B-Instruct-v0.1 \
    -g $num_gpu \
    -d float16
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the Mixtral MoE 8x7B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">mistralai</span><span class="o">/</span><span class="n">Mixtral</span><span class="o">-</span><span class="mi">8</span><span class="n">x7B</span><span class="o">-</span><span class="n">Instruct</span><span class="o">-</span><span class="n">v0</span><span class="mf">.1</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float16</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Mixtral-8x7B-Instruct-v0.1_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the Mixtral MoE 8x7B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>mistralai/Mixtral-8x7B-Instruct-v0.1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float16
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Mixtral-8x7B-Instruct-v0.1_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-mixtral-8x22b docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-38" name="sd-tab-set-19" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-38">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1">Mixtral MoE 8x22B</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_mixtral-8x22b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_mixtral-8x22b</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float16/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-39" name="sd-tab-set-19" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-39">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m mistralai/Mixtral-8x22B-Instruct-v0.1 \
    -g $num_gpu \
    -d float16
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the Mixtral MoE 8x22B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">mistralai</span><span class="o">/</span><span class="n">Mixtral</span><span class="o">-</span><span class="mi">8</span><span class="n">x22B</span><span class="o">-</span><span class="n">Instruct</span><span class="o">-</span><span class="n">v0</span><span class="mf">.1</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float16</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Mixtral-8x22B-Instruct-v0.1_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the Mixtral MoE 8x22B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>mistralai/Mixtral-8x22B-Instruct-v0.1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float16
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Mixtral-8x22B-Instruct-v0.1_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-mistral-7b docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-40" name="sd-tab-set-20" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-40">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3">Mistral 7B</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_mistral-7b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_mistral-7b</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float16/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-41" name="sd-tab-set-20" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-41">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m mistralai/Mistral-7B-Instruct-v0.3 \
    -g $num_gpu \
    -d float16
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the Mistral 7B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">mistralai</span><span class="o">/</span><span class="n">Mistral</span><span class="o">-</span><span class="mi">7</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span><span class="o">-</span><span class="n">v0</span><span class="mf">.3</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float16</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Mistral-7B-Instruct-v0.3_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the Mistral 7B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>mistralai/Mistral-7B-Instruct-v0.3<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float16
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Mistral-7B-Instruct-v0.3_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-mixtral-8x7b-fp8 docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-42" name="sd-tab-set-21" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-42">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/amd/Mixtral-8x7B-Instruct-v0.1-FP8-KV">Mixtral MoE 8x7B FP8</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float8</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_mixtral-8x7b_fp8<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_mixtral-8x7b_fp8</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float8/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-43" name="sd-tab-set-21" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-43">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m amd/Mixtral-8x7B-Instruct-v0.1-FP8-KV \
    -g $num_gpu \
    -d float8
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the Mixtral MoE 8x7B FP8 model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float8</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">amd</span><span class="o">/</span><span class="n">Mixtral</span><span class="o">-</span><span class="mi">8</span><span class="n">x7B</span><span class="o">-</span><span class="n">Instruct</span><span class="o">-</span><span class="n">v0</span><span class="mf">.1</span><span class="o">-</span><span class="n">FP8</span><span class="o">-</span><span class="n">KV</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float8</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float8_vllm_rocm6.4.1/summary/Mixtral-8x7B-Instruct-v0.1-FP8-KV_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the Mixtral MoE 8x7B FP8 model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float8</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>amd/Mixtral-8x7B-Instruct-v0.1-FP8-KV<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float8
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float8_vllm_rocm6.4.1/summary/Mixtral-8x7B-Instruct-v0.1-FP8-KV_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-mixtral-8x22b-fp8 docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-44" name="sd-tab-set-22" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-44">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/amd/Mixtral-8x22B-Instruct-v0.1-FP8-KV">Mixtral MoE 8x22B FP8</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float8</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_mixtral-8x22b_fp8<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_mixtral-8x22b_fp8</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float8/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-45" name="sd-tab-set-22" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-45">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m amd/Mixtral-8x22B-Instruct-v0.1-FP8-KV \
    -g $num_gpu \
    -d float8
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the Mixtral MoE 8x22B FP8 model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float8</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">amd</span><span class="o">/</span><span class="n">Mixtral</span><span class="o">-</span><span class="mi">8</span><span class="n">x22B</span><span class="o">-</span><span class="n">Instruct</span><span class="o">-</span><span class="n">v0</span><span class="mf">.1</span><span class="o">-</span><span class="n">FP8</span><span class="o">-</span><span class="n">KV</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float8</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float8_vllm_rocm6.4.1/summary/Mixtral-8x22B-Instruct-v0.1-FP8-KV_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the Mixtral MoE 8x22B FP8 model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float8</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>amd/Mixtral-8x22B-Instruct-v0.1-FP8-KV<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float8
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float8_vllm_rocm6.4.1/summary/Mixtral-8x22B-Instruct-v0.1-FP8-KV_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-mistral-7b-fp8 docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-46" name="sd-tab-set-23" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-46">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/amd/Mistral-7B-v0.1-FP8-KV">Mistral 7B FP8</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float8</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_mistral-7b_fp8<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_mistral-7b_fp8</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float8/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-47" name="sd-tab-set-23" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-47">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m amd/Mistral-7B-v0.1-FP8-KV \
    -g $num_gpu \
    -d float8
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the Mistral 7B FP8 model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float8</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">amd</span><span class="o">/</span><span class="n">Mistral</span><span class="o">-</span><span class="mi">7</span><span class="n">B</span><span class="o">-</span><span class="n">v0</span><span class="mf">.1</span><span class="o">-</span><span class="n">FP8</span><span class="o">-</span><span class="n">KV</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float8</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float8_vllm_rocm6.4.1/summary/Mistral-7B-v0.1-FP8-KV_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the Mistral 7B FP8 model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float8</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>amd/Mistral-7B-v0.1-FP8-KV<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float8
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float8_vllm_rocm6.4.1/summary/Mistral-7B-v0.1-FP8-KV_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-qwen2-7b docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-48" name="sd-tab-set-24" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-48">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/Qwen/Qwen2-7B-Instruct">Qwen2 7B</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_qwen2-7b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_qwen2-7b</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float16/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-49" name="sd-tab-set-24" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-49">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m Qwen/Qwen2-7B-Instruct \
    -g $num_gpu \
    -d float16
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the Qwen2 7B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">Qwen</span><span class="o">/</span><span class="n">Qwen2</span><span class="o">-</span><span class="mi">7</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float16</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Qwen2-7B-Instruct_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the Qwen2 7B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>Qwen/Qwen2-7B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float16
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Qwen2-7B-Instruct_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-qwen2-72b docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-50" name="sd-tab-set-25" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-50">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/Qwen/Qwen2-72B-Instruct">Qwen2 72B</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_qwen2-72b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_qwen2-72b</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float16/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-51" name="sd-tab-set-25" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-51">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m Qwen/Qwen2-72B-Instruct \
    -g $num_gpu \
    -d float16
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the Qwen2 72B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">Qwen</span><span class="o">/</span><span class="n">Qwen2</span><span class="o">-</span><span class="mi">72</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float16</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Qwen2-72B-Instruct_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the Qwen2 72B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>Qwen/Qwen2-72B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float16
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/Qwen2-72B-Instruct_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-qwq-32b docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-52" name="sd-tab-set-26" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-52">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/Qwen/QwQ-32B">QwQ-32B</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_qwq-32b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_qwq-32b</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float16/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For improved performance, consider enabling <a class="reference internal" href="#mi300x-tunableop"><span class="std std-ref">PyTorch TunableOp</span></a>.
TunableOp automatically explores different implementations and configurations of certain PyTorch
operators to find the fastest one for your hardware.</p>
<p>By default, <code class="docutils literal notranslate"><span class="pre">pyt_vllm_qwq-32b</span></code> runs with TunableOp disabled
(see
<a class="github reference external" href="https://github.com/ROCm/MAD/blob/develop/models.json">ROCm/MAD</a>).
To enable it, include the <code class="docutils literal notranslate"><span class="pre">--tunableop</span> <span class="pre">on</span></code> argument in your
run.</p>
<p>Enabling TunableOp triggers a two-pass run – a warm-up followed
by the performance-collection run.</p>
</div>
</div>
<input id="sd-tab-item-53" name="sd-tab-set-26" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-53">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m Qwen/QwQ-32B \
    -g $num_gpu \
    -d float16
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the QwQ-32B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">Qwen</span><span class="o">/</span><span class="n">QwQ</span><span class="o">-</span><span class="mi">32</span><span class="n">B</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float16</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/QwQ-32B_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the QwQ-32B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>Qwen/QwQ-32B<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float16
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/QwQ-32B_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-dbrx-instruct docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-54" name="sd-tab-set-27" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-54">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/databricks/dbrx-instruct">DBRX Instruct</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_dbrx-instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_dbrx-instruct</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float16/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-55" name="sd-tab-set-27" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-55">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m databricks/dbrx-instruct \
    -g $num_gpu \
    -d float16
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the DBRX Instruct model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">databricks</span><span class="o">/</span><span class="n">dbrx</span><span class="o">-</span><span class="n">instruct</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float16</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/dbrx-instruct_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the DBRX Instruct model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>databricks/dbrx-instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float16
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/dbrx-instruct_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-dbrx-fp8 docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-56" name="sd-tab-set-28" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-56">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/amd/dbrx-instruct-FP8-KV">DBRX Instruct FP8</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float8</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_dbrx_fp8<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_dbrx_fp8</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float8/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-57" name="sd-tab-set-28" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-57">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m amd/dbrx-instruct-FP8-KV \
    -g $num_gpu \
    -d float8
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the DBRX Instruct FP8 model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float8</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">amd</span><span class="o">/</span><span class="n">dbrx</span><span class="o">-</span><span class="n">instruct</span><span class="o">-</span><span class="n">FP8</span><span class="o">-</span><span class="n">KV</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float8</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float8_vllm_rocm6.4.1/summary/dbrx-instruct-FP8-KV_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the DBRX Instruct FP8 model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float8</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>amd/dbrx-instruct-FP8-KV<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float8
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float8_vllm_rocm6.4.1/summary/dbrx-instruct-FP8-KV_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-gemma-2-27b docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-58" name="sd-tab-set-29" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-58">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/google/gemma-2-27b">Gemma 2 27B</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_gemma-2-27b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_gemma-2-27b</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float16/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-59" name="sd-tab-set-29" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-59">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m google/gemma-2-27b \
    -g $num_gpu \
    -d float16
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the Gemma 2 27B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">google</span><span class="o">/</span><span class="n">gemma</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">27</span><span class="n">b</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float16</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/gemma-2-27b_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the Gemma 2 27B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>google/gemma-2-27b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float16
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/gemma-2-27b_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-c4ai-command-r-plus-08-2024 docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-60" name="sd-tab-set-30" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-60">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/CohereForAI/c4ai-command-r-plus-08-2024">C4AI Command R+ 08-2024</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_c4ai-command-r-plus-08-2024<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_c4ai-command-r-plus-08-2024</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float16/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-61" name="sd-tab-set-30" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-61">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m CohereForAI/c4ai-command-r-plus-08-2024 \
    -g $num_gpu \
    -d float16
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the C4AI Command R+ 08-2024 model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">CohereForAI</span><span class="o">/</span><span class="n">c4ai</span><span class="o">-</span><span class="n">command</span><span class="o">-</span><span class="n">r</span><span class="o">-</span><span class="n">plus</span><span class="o">-</span><span class="mi">08</span><span class="o">-</span><span class="mi">2024</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float16</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/c4ai-command-r-plus-08-2024_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the C4AI Command R+ 08-2024 model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>CohereForAI/c4ai-command-r-plus-08-2024<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float16
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/c4ai-command-r-plus-08-2024_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-command-r-plus-fp8 docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-62" name="sd-tab-set-31" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-62">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/amd/c4ai-command-r-plus-FP8-KV">C4AI Command R+ 08-2024 FP8</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float8</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_command-r-plus_fp8<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_command-r-plus_fp8</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float8/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-63" name="sd-tab-set-31" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-63">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m amd/c4ai-command-r-plus-FP8-KV \
    -g $num_gpu \
    -d float8
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the C4AI Command R+ 08-2024 FP8 model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float8</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">amd</span><span class="o">/</span><span class="n">c4ai</span><span class="o">-</span><span class="n">command</span><span class="o">-</span><span class="n">r</span><span class="o">-</span><span class="n">plus</span><span class="o">-</span><span class="n">FP8</span><span class="o">-</span><span class="n">KV</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float8</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float8_vllm_rocm6.4.1/summary/c4ai-command-r-plus-FP8-KV_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the C4AI Command R+ 08-2024 FP8 model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float8</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>amd/c4ai-command-r-plus-FP8-KV<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float8
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float8_vllm_rocm6.4.1/summary/c4ai-command-r-plus-FP8-KV_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-deepseek-moe-16b-chat docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-64" name="sd-tab-set-32" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-64">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/deepseek-ai/deepseek-moe-16b-chat">DeepSeek MoE 16B</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_deepseek-moe-16b-chat<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_deepseek-moe-16b-chat</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float16/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-65" name="sd-tab-set-32" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-65">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m deepseek-ai/deepseek-moe-16b-chat \
    -g $num_gpu \
    -d float16
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the DeepSeek MoE 16B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">deepseek</span><span class="o">-</span><span class="n">ai</span><span class="o">/</span><span class="n">deepseek</span><span class="o">-</span><span class="n">moe</span><span class="o">-</span><span class="mi">16</span><span class="n">b</span><span class="o">-</span><span class="n">chat</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float16</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/deepseek-moe-16b-chat_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the DeepSeek MoE 16B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>deepseek-ai/deepseek-moe-16b-chat<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float16
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/deepseek-moe-16b-chat_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-phi-4 docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-66" name="sd-tab-set-33" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-66">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/microsoft/phi-4">Phi-4</a> model
using one GPU with the :literal:`` data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_phi-4<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_phi-4</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-67" name="sd-tab-set-33" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-67">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m microsoft/phi-4 \
    -g $num_gpu \
    -d 
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the Phi-4 model on eight GPUs with :literal:`` precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">microsoft</span><span class="o">/</span><span class="n">phi</span><span class="o">-</span><span class="mi">4</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> 
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports__vllm_rocm6.4.1/summary/phi-4_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the Phi-4 model on eight GPUs with :literal:`` precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>microsoft/phi-4<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports__vllm_rocm6.4.1/summary/phi-4_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="model-doc pyt-vllm-falcon-180b docutils container">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-68" name="sd-tab-set-34" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-68">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/tiiuae/falcon-180B">Falcon 180B</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_vllm_falcon-180b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_vllm_falcon-180b</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/reports_float16/</span></code>.</p>
<p>Although the <a class="reference internal" href="#vllm-benchmark-available-models"><span class="std std-ref">available models</span></a> are preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-69" name="sd-tab-set-34" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-69">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the vLLM benchmark tool independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/rocm/vllm/rocm6.4.1_vllm_0.9.1_20250715/images/sha256-4a429705fa95a58f6d20aceab43b1b76fa769d57f32d5d28bd3f4e030e2a78ea">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rocm/vllm:rocm6.4.1_vllm_0.9.1_20250715
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/vllm</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/vllm
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">1 or 8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float8</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh \
    -s $test_option \
    -m tiiuae/falcon-180B \
    -g $num_gpu \
    -d float16
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, it’s recommend to run with <code class="docutils literal notranslate"><span class="pre">VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1</span></code>.</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OSError: You are trying to access a gated repo.

# pass your HF_TOKEN
export HF_TOKEN=$your_personal_hf_token
</pre></div>
</div>
</div>
</li>
</ol>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the Falcon 180B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">vllm_benchmark_report</span><span class="o">.</span><span class="n">sh</span> \
    <span class="o">-</span><span class="n">s</span> <span class="n">latency</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">tiiuae</span><span class="o">/</span><span class="n">falcon</span><span class="o">-</span><span class="mi">180</span><span class="n">B</span> \
    <span class="o">-</span><span class="n">g</span> <span class="mi">8</span> \
    <span class="o">-</span><span class="n">d</span> <span class="n">float16</span>
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/falcon-180B_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the Falcon 180B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">float16</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./vllm_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>tiiuae/falcon-180B<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>float16
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_float16_vllm_rocm6.4.1/summary/falcon-180B_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="advanced-usage">
<h5>Advanced usage<a class="headerlink" href="#advanced-usage" title="Link to this heading">#</a></h5>
<p>For information on experimental features and known issues related to ROCm optimization efforts on vLLM,
see the developer’s guide at <a class="github reference external" href="https://github.com/ROCm/vllm/tree/f94ec9beeca1071cc34f9d1e206d8c7f3ac76129/docs/dev-docker">ROCm/vllm</a>.</p>
<section id="reproducing-the-docker-image">
<h6>Reproducing the Docker image<a class="headerlink" href="#reproducing-the-docker-image" title="Link to this heading">#</a></h6>
<p>To reproduce this ROCm/vLLM Docker image release, follow these steps:</p>
<ol class="arabic">
<li><p>Clone the <a class="reference external" href="https://github.com/ROCm/vllm">vLLM repository</a>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/vllm.git
</pre></div>
</div>
</li>
<li><p>Checkout the specific release commit.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>vllm
git<span class="w"> </span>checkout<span class="w"> </span>b432b7a285aa0dcb9677380936ffa74931bb6d6f
</pre></div>
</div>
</li>
<li><p>Build the Docker image. Replace <code class="docutils literal notranslate"><span class="pre">vllm-rocm</span></code> with your desired image tag.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>build<span class="w"> </span>-f<span class="w"> </span>docker/Dockerfile.rocm<span class="w"> </span>-t<span class="w"> </span>vllm-rocm<span class="w"> </span>.
</pre></div>
</div>
</li>
</ol>
</section>
</section>
<section id="known-issues-and-workarounds">
<h5>Known issues and workarounds<a class="headerlink" href="#known-issues-and-workarounds" title="Link to this heading">#</a></h5>
<p>AITER does not support FP8 KV cache yet.</p>
</section>
<section id="further-reading">
<h5>Further reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p>To learn more about the options for latency and throughput benchmark scripts,
see <a class="github reference external" href="https://github.com/ROCm/vllm/tree/main/benchmarks">ROCm/vllm</a>.</p></li>
<li><p>To learn more about MAD and the <code class="docutils literal notranslate"><span class="pre">madengine</span></code> CLI, see the <a class="reference external" href="https://github.com/ROCm/MAD?tab=readme-ov-file#usage-guide">MAD usage guide</a>.</p></li>
<li><p>To learn more about system settings and management practices to configure your system for
AMD Instinct MI300X series accelerators, see <a class="reference external" href="https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/system-optimization/mi300x.html">AMD Instinct MI300X system optimization</a>.</p></li>
<li><p>For application performance optimization strategies for HPC and AI workloads,
including inference with vLLM, see <a class="reference internal" href="#document-how-to/rocm-for-ai/inference-optimization/workload"><span class="doc">AMD Instinct MI300X workload optimization</span></a>.</p></li>
<li><p>To learn how to run community models from Hugging Face on AMD GPUs, see
<a class="reference internal" href="#document-how-to/rocm-for-ai/inference/hugging-face-models"><span class="doc">Running models from Hugging Face</span></a>.</p></li>
<li><p>To learn how to fine-tune LLMs and optimize inference, see
<a class="reference internal" href="#document-how-to/rocm-for-ai/fine-tuning/fine-tuning-and-inference"><span class="doc">Fine-tuning LLMs and inference optimization</span></a>.</p></li>
<li><p>For a list of other ready-made Docker images for AI with ROCm, see
<a class="reference external" href="https://www.amd.com/en/developer/resources/infinity-hub.html#f-amd_hub_category=AI%20%26%20ML%20Models">AMD Infinity Hub</a>.</p></li>
</ul>
</section>
<section id="previous-versions">
<h5>Previous versions<a class="headerlink" href="#previous-versions" title="Link to this heading">#</a></h5>
<p>See <a class="reference internal" href="#document-how-to/rocm-for-ai/inference/benchmark-docker/previous-versions/vllm-history"><span class="doc">vLLM inference performance testing version history</span></a> to find documentation for previous releases
of the <code class="docutils literal notranslate"><span class="pre">ROCm/vllm</span></code> Docker image.</p>
</section>
</section>
<span id="document-how-to/rocm-for-ai/inference/benchmark-docker/pytorch-inference"></span><section id="pytorch-inference-performance-testing">
<h4>PyTorch inference performance testing<a class="headerlink" href="#pytorch-inference-performance-testing" title="Link to this heading">#</a></h4>
<p id="pytorch-inference-benchmark-docker">The <a class="reference external" href="https://hub.docker.com/r/rocm/pytorch/tags">ROCm PyTorch Docker</a> image offers a prebuilt,
optimized environment for testing model inference performance on AMD Instinct™ MI300X series
accelerators. This guide demonstrates how to use the AMD Model Automation and Dashboarding (MAD)
tool with the ROCm PyTorch container to test inference performance on various models efficiently.</p>
<section id="supported-models">
<span id="pytorch-inference-benchmark-available-models"></span><h5>Supported models<a class="headerlink" href="#supported-models" title="Link to this heading">#</a></h5>
<p>The following models are supported for inference performance benchmarking
with PyTorch and ROCm. Some instructions, commands, and recommendations in this
documentation might vary by model – select one to get started.</p>
<div class="container-fluid" id="vllm-benchmark-ud-params-picker">
<div class="row">
<div class="col-2 me-2 model-param-head">Model</div>
<div class="row col-10">
<div class="col-3 model-param" data-param-k="model-group" data-param-v="clip" tabindex="0">CLIP</div>
<div class="col-3 model-param" data-param-k="model-group" data-param-v="chai" tabindex="0">Chai-1</div>
<div class="col-3 model-param" data-param-k="model-group" data-param-v="mochi" tabindex="0">Mochi Video</div>
<div class="col-3 model-param" data-param-k="model-group" data-param-v="wan" tabindex="0">Wan2.1</div>
<div class="col-3 model-param" data-param-k="model-group" data-param-v="janus-pro" tabindex="0">Janus-Pro</div>
</div>
</div>
<div class="row mt-1" style="display: none;">
<div class="col-2 me-2 model-param-head">Model</div>
<div class="row col-10">
<div class="col-12 model-param" data-param-group="clip" data-param-k="model" data-param-v="pyt_clip_inference" tabindex="0">CLIP</div>
<div class="col-12 model-param" data-param-group="chai" data-param-k="model" data-param-v="pyt_chai1_inference" tabindex="0">Chai-1</div>
<div class="col-12 model-param" data-param-group="mochi" data-param-k="model" data-param-v="pyt_mochi_video_inference" tabindex="0">Mochi 1</div>
<div class="col-12 model-param" data-param-group="wan" data-param-k="model" data-param-v="pyt_wan2.1_inference" tabindex="0">Wan2.1</div>
<div class="col-12 model-param" data-param-group="janus-pro" data-param-k="model" data-param-v="pyt_janus_pro_inference" tabindex="0">Janus Pro 7B</div>
</div>
</div>
</div><div class="model-doc pyt-clip-inference docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K">CLIP model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization before use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-chai1-inference docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/chaidiscovery/chai-1">Chai-1 model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization before use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-mochi-video-inference docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/genmo/mochi-1-preview">Mochi 1 model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization before use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-wan2-1-inference docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/Wan-AI/Wan2.1-T2V-14B">Wan2.1 model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization before use via an external license agreement through a third party.</p>
</div>
</div>
<div class="model-doc pyt-janus-pro-inference docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference external" href="https://huggingface.co/deepseek-ai/Janus-Pro-7B">Janus Pro 7B model card on Hugging Face</a> to learn more about your selected model.
Some models require access authorization before use via an external license agreement through a third party.</p>
</div>
</div>
</section>
<section id="system-validation">
<h5>System validation<a class="headerlink" href="#system-validation" title="Link to this heading">#</a></h5>
<p>Before running AI workloads, it’s important to validate that your AMD hardware is configured
correctly and performing optimally.</p>
<p>To optimize performance, disable automatic NUMA balancing. Otherwise, the GPU
might hang until the periodic balancing is finalized. For more information,
see the <a class="reference internal" href="#rocm-for-ai-system-optimization"><span class="std std-ref">system validation steps</span></a>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># disable automatic NUMA balancing</span>
sh<span class="w"> </span>-c<span class="w"> </span><span class="s1">'echo 0 &gt; /proc/sys/kernel/numa_balancing'</span>
<span class="c1"># check if NUMA balancing is disabled (returns 0 if disabled)</span>
cat<span class="w"> </span>/proc/sys/kernel/numa_balancing
<span class="m">0</span>
</pre></div>
</div>
<p>To test for optimal performance, consult the recommended <a class="reference internal" href="#rocm-for-ai-system-health-bench"><span class="std std-ref">System health benchmarks</span></a>. This suite of tests will help you verify and fine-tune your
system’s configuration.</p>
</section>
<section id="pull-the-docker-image">
<h5>Pull the Docker image<a class="headerlink" href="#pull-the-docker-image" title="Link to this heading">#</a></h5>
<div class="model-doc pyt-chai1-inference docutils container">
<p>Use the following command to pull the <a class="reference external" href="https://hub.docker.com/layers/rocm/pytorch/rocm6.2.3_ubuntu22.04_py3.10_pytorch_release_2.3.0_triton_llvm_reg_issue/images/sha256-b736a4239ab38a9d0e448af6d4adca83b117debed00bfbe33846f99c4540f79b">ROCm PyTorch Docker image</a> from Docker Hub.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/pytorch:rocm6.2.3_ubuntu22.04_py3.10_pytorch_release_2.3.0_triton_llvm_reg_issue
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Chai-1 benchmark uses a specifically selected Docker image using ROCm 6.2.3 and PyTorch 2.3.0 to address an accuracy issue.</p>
</div>
</div>
<div class="model-doc pyt-clip-inference pyt-mochi-video-inference pyt-wan2-1-inference pyt-janus-pro-inference docutils container">
<p>Use the following command to pull the <a class="reference external" href="https://hub.docker.com/layers/rocm/pytorch/latest/images/sha256-05b55983e5154f46e7441897d0908d79877370adca4d1fff4899d9539d6c4969">ROCm PyTorch Docker image</a> from Docker Hub.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/pytorch:latest
</pre></div>
</div>
</div>
</section>
<section id="benchmarking">
<span id="pytorch-benchmark-get-started"></span><h5>Benchmarking<a class="headerlink" href="#benchmarking" title="Link to this heading">#</a></h5>
<div class="model-doc pyt-clip-inference docutils container" id="pytorch-inference-benchmark-mad">
<p>To simplify performance testing, the ROCm Model Automation and Dashboarding
(<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) project provides ready-to-use scripts and configuration.
To start, clone the  MAD repository to a local directory and install the required packages on the
host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
<p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K">CLIP</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_clip_inference<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_clip_inference</span></code>. The latency and throughput reports of the
model are collected in <code class="docutils literal notranslate"><span class="pre">perf_pyt_clip_inference.csv</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For improved performance, consider enabling TunableOp. By default,
<code class="docutils literal notranslate"><span class="pre">pyt_clip_inference</span></code> runs with TunableOp disabled (see
<a class="github reference external" href="https://github.com/ROCm/MAD/blob/develop/models.json">ROCm/MAD</a>). To enable
it, include the <code class="docutils literal notranslate"><span class="pre">--tunableop</span> <span class="pre">on</span></code> argument in your run.</p>
<p>Enabling TunableOp triggers a two-pass run – a warm-up followed by the performance-collection run.
Although this might increase the initial training time, it can result in a performance gain.</p>
</div>
</div>
<div class="model-doc pyt-chai1-inference docutils container">
<p>To simplify performance testing, the ROCm Model Automation and Dashboarding
(<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) project provides ready-to-use scripts and configuration.
To start, clone the  MAD repository to a local directory and install the required packages on the
host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
<p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/chaidiscovery/chai-1">Chai-1</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_chai1_inference<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_chai1_inference</span></code>. The latency and throughput reports of the
model are collected in <code class="docutils literal notranslate"><span class="pre">perf_pyt_chai1_inference.csv</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For improved performance, consider enabling TunableOp. By default,
<code class="docutils literal notranslate"><span class="pre">pyt_chai1_inference</span></code> runs with TunableOp disabled (see
<a class="github reference external" href="https://github.com/ROCm/MAD/blob/develop/models.json">ROCm/MAD</a>). To enable
it, include the <code class="docutils literal notranslate"><span class="pre">--tunableop</span> <span class="pre">on</span></code> argument in your run.</p>
<p>Enabling TunableOp triggers a two-pass run – a warm-up followed by the performance-collection run.
Although this might increase the initial training time, it can result in a performance gain.</p>
</div>
</div>
<div class="model-doc pyt-mochi-video-inference docutils container">
<p>To simplify performance testing, the ROCm Model Automation and Dashboarding
(<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) project provides ready-to-use scripts and configuration.
To start, clone the  MAD repository to a local directory and install the required packages on the
host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
<p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/genmo/mochi-1-preview">Mochi 1</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_mochi_video_inference<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_mochi_video_inference</span></code>. The latency and throughput reports of the
model are collected in <code class="docutils literal notranslate"><span class="pre">perf_pyt_mochi_video_inference.csv</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For improved performance, consider enabling TunableOp. By default,
<code class="docutils literal notranslate"><span class="pre">pyt_mochi_video_inference</span></code> runs with TunableOp disabled (see
<a class="github reference external" href="https://github.com/ROCm/MAD/blob/develop/models.json">ROCm/MAD</a>). To enable
it, include the <code class="docutils literal notranslate"><span class="pre">--tunableop</span> <span class="pre">on</span></code> argument in your run.</p>
<p>Enabling TunableOp triggers a two-pass run – a warm-up followed by the performance-collection run.
Although this might increase the initial training time, it can result in a performance gain.</p>
</div>
</div>
<div class="model-doc pyt-wan2-1-inference docutils container">
<p>To simplify performance testing, the ROCm Model Automation and Dashboarding
(<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) project provides ready-to-use scripts and configuration.
To start, clone the  MAD repository to a local directory and install the required packages on the
host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
<p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/Wan-AI/Wan2.1-T2V-14B">Wan2.1</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_wan2.1_inference<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_wan2.1_inference</span></code>. The latency and throughput reports of the
model are collected in <code class="docutils literal notranslate"><span class="pre">perf_pyt_wan2.1_inference.csv</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For improved performance, consider enabling TunableOp. By default,
<code class="docutils literal notranslate"><span class="pre">pyt_wan2.1_inference</span></code> runs with TunableOp disabled (see
<a class="github reference external" href="https://github.com/ROCm/MAD/blob/develop/models.json">ROCm/MAD</a>). To enable
it, include the <code class="docutils literal notranslate"><span class="pre">--tunableop</span> <span class="pre">on</span></code> argument in your run.</p>
<p>Enabling TunableOp triggers a two-pass run – a warm-up followed by the performance-collection run.
Although this might increase the initial training time, it can result in a performance gain.</p>
</div>
</div>
<div class="model-doc pyt-janus-pro-inference docutils container">
<p>To simplify performance testing, the ROCm Model Automation and Dashboarding
(<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) project provides ready-to-use scripts and configuration.
To start, clone the  MAD repository to a local directory and install the required packages on the
host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
<p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/deepseek-ai/Janus-Pro-7B">Janus Pro 7B</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_janus_pro_inference<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_janus_pro_inference</span></code>. The latency and throughput reports of the
model are collected in <code class="docutils literal notranslate"><span class="pre">perf_pyt_janus_pro_inference.csv</span></code>.</p>
</div>
</section>
<section id="further-reading">
<h5>Further reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p>To learn more about MAD and the <code class="docutils literal notranslate"><span class="pre">madengine</span></code> CLI, see the <a class="reference external" href="https://github.com/ROCm/MAD?tab=readme-ov-file#usage-guide">MAD usage guide</a>.</p></li>
<li><p>To learn more about system settings and management practices to configure your system for
AMD Instinct MI300X series accelerators, see <a class="reference external" href="https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/system-optimization/mi300x.html">AMD Instinct MI300X system optimization</a>.</p></li>
<li><p>For application performance optimization strategies for HPC and AI workloads,
including inference with vLLM, see <a class="reference internal" href="#document-how-to/rocm-for-ai/inference-optimization/workload"><span class="doc">AMD Instinct MI300X workload optimization</span></a>.</p></li>
<li><p>To learn how to run LLM models from Hugging Face or your model, see
<a class="reference internal" href="#document-how-to/rocm-for-ai/inference/hugging-face-models"><span class="doc">Running models from Hugging Face</span></a>.</p></li>
<li><p>To learn how to optimize inference on LLMs, see
<a class="reference internal" href="#document-how-to/rocm-for-ai/inference-optimization/index"><span class="doc">Inference optimization</span></a>.</p></li>
<li><p>To learn how to fine-tune LLMs, see
<a class="reference internal" href="#document-how-to/rocm-for-ai/fine-tuning/index"><span class="doc">Fine-tuning LLMs</span></a>.</p></li>
</ul>
</section>
</section>
<span id="document-how-to/rocm-for-ai/inference/benchmark-docker/sglang"></span><section id="sglang-inference-performance-testing">
<h4>SGLang inference performance testing<a class="headerlink" href="#sglang-inference-performance-testing" title="Link to this heading">#</a></h4>
<p id="sglang-benchmark-unified-docker"><a class="reference external" href="https://docs.sglang.ai">SGLang</a> is a high-performance inference and
serving engine for large language models (LLMs) and vision models. The
ROCm-enabled <a class="reference external" href="https://hub.docker.com/layers/lmsysorg/sglang/v0.4.5-rocm630/images/sha256-63d2cb760a237125daf6612464cfe2f395c0784e21e8b0ea37d551cd10d3c951">SGLang Docker image</a>
bundles SGLang with PyTorch, optimized for AMD Instinct MI300X series
accelerators. It includes the following software components:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Software component</p></th>
<th class="head"><p>Version</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ROCm/ROCm">ROCm</a></p></td>
<td><p>6.3.0</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://docs.sglang.ai/index.html">SGLang</a></p></td>
<td><p>0.4.5 (0.4.5-rocm)</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/pytorch/pytorch">PyTorch</a></p></td>
<td><p>2.6.0a0+git8d4926e</p></td>
</tr>
</tbody>
</table>
</div>
<section id="system-validation">
<h5>System validation<a class="headerlink" href="#system-validation" title="Link to this heading">#</a></h5>
<p>Before running AI workloads, it’s important to validate that your AMD hardware is configured
correctly and performing optimally.</p>
<p>If you have already validated your system settings, including aspects like NUMA auto-balancing, you
can skip this step. Otherwise, complete the procedures in the <a class="reference internal" href="#rocm-for-ai-system-optimization"><span class="std std-ref">System validation and
optimization</span></a> guide to properly configure your system settings
before starting training.</p>
<p>To test for optimal performance, consult the recommended <a class="reference internal" href="#rocm-for-ai-system-health-bench"><span class="std std-ref">System health benchmarks</span></a>. This suite of tests will help you verify and fine-tune your
system’s configuration.</p>
<section id="pull-the-docker-image">
<h6>Pull the Docker image<a class="headerlink" href="#pull-the-docker-image" title="Link to this heading">#</a></h6>
<p>Download the <a class="reference external" href="https://hub.docker.com/layers/lmsysorg/sglang/v0.4.5-rocm630/images/sha256-63d2cb760a237125daf6612464cfe2f395c0784e21e8b0ea37d551cd10d3c951">SGLang Docker image</a>.
Use the following command to pull the Docker image from Docker Hub.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>lmsysorg/sglang:v0.4.5-rocm630
</pre></div>
</div>
</section>
<section id="benchmarking">
<h6>Benchmarking<a class="headerlink" href="#benchmarking" title="Link to this heading">#</a></h6>
<p>Once the setup is complete, choose one of the following methods to benchmark inference performance with
<a class="reference external" href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B">DeepSeek-R1-Distill-Qwen-32B</a>.</p>
<div class="model-doc pyt-sglang-deepseek-r1-distill-qwen-32b docutils container" id="sglang-benchmark-mad">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-70" name="sd-tab-set-35" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-70">
MAD-integrated benchmarking</label><div class="sd-tab-content docutils">
<ol class="arabic">
<li><p>Clone the ROCm Model Automation and Dashboarding (<a class="github reference external" href="https://github.com/ROCm/MAD">ROCm/MAD</a>) repository to a local
directory and install the required packages on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Use this command to run the performance benchmark test on the <a class="reference external" href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B">DeepSeek-R1-Distill-Qwen-32B</a> model
using one GPU with the <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> data type on the host machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAD_SECRETS_HFTOKEN</span><span class="o">=</span><span class="s2">"your personal Hugging Face token to access gated models"</span>
madengine<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tags<span class="w"> </span>pyt_sglang_deepseek-r1-distill-qwen-32b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep-model-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--live-output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--timeout<span class="w"> </span><span class="m">28800</span>
</pre></div>
</div>
</li>
</ol>
<p>MAD launches a Docker container with the name
<code class="docutils literal notranslate"><span class="pre">container_ci-pyt_sglang_deepseek-r1-distill-qwen-32b</span></code>. The latency and throughput reports of the
model are collected in the following path: <code class="docutils literal notranslate"><span class="pre">~/MAD/perf_DeepSeek-R1-Distill-Qwen-32B.csv</span></code>.</p>
<p>Although the DeepSeek-R1-Distill-Qwen-32B is preconfigured
to collect latency and throughput performance data, you can also change the benchmarking
parameters. See the standalone benchmarking tab for more information.</p>
</div>
<input id="sd-tab-item-71" name="sd-tab-set-35" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-71">
Standalone benchmarking</label><div class="sd-tab-content docutils">
<p class="rubric">Download the Docker image and required scripts</p>
<ol class="arabic">
<li><p>Run the SGLang benchmark script independently by starting the
<a class="reference external" href="https://hub.docker.com/layers/lmsysorg/sglang/v0.4.5-rocm630/images/sha256-63d2cb760a237125daf6612464cfe2f395c0784e21e8b0ea37d551cd10d3c951">Docker container</a>
as shown in the following snippet.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>lmsysorg/sglang:v0.4.5-rocm630
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">HUGGINGFACE_HUB_CACHE</span><span class="o">=</span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>lmsysorg/sglang:v0.4.5-rocm630
</pre></div>
</div>
</li>
<li><p>In the Docker container, clone the ROCm MAD repository and navigate to the
benchmark scripts directory at <code class="docutils literal notranslate"><span class="pre">~/MAD/scripts/sglang</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/MAD
<span class="nb">cd</span><span class="w"> </span>MAD/scripts/sglang
</pre></div>
</div>
</li>
<li><p>To start the benchmark, use the following command with the appropriate options.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Benchmark options</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="pst-scrollable-table-container"><table class="table table-center">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Name</p></th>
<th class="head"><p class="sd-card-text">Options</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$test_option</span></code></p></td>
<td><p class="sd-card-text">latency</p></td>
<td><p class="sd-card-text">Measure decoding token latency</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p class="sd-card-text">throughput</p></td>
<td><p class="sd-card-text">Measure token generation throughput</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p class="sd-card-text">all</p></td>
<td><p class="sd-card-text">Measure both throughput and latency</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$num_gpu</span></code></p></td>
<td><p class="sd-card-text">8</p></td>
<td><p class="sd-card-text">Number of GPUs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$datatype</span></code></p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">bfloat16</span></code></p></td>
<td><p class="sd-card-text">Data type</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">$dataset</span></code></p></td>
<td><p class="sd-card-text">random</p></td>
<td><p class="sd-card-text">Dataset</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text">The input sequence length, output sequence length, and tensor parallel (TP) are
already configured. You don’t need to specify them with this script.</p>
</div>
</details><p>Command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./sglang_benchmark_report.sh<span class="w"> </span>-s<span class="w"> </span><span class="nv">$test_option</span><span class="w"> </span>-m<span class="w"> </span>deepseek-ai/DeepSeek-R1-Distill-Qwen-32B<span class="w"> </span>-g<span class="w"> </span><span class="nv">$num_gpu</span><span class="w"> </span>-d<span class="w"> </span><span class="nv">$datatype</span><span class="w"> </span><span class="o">[</span>-a<span class="w"> </span><span class="nv">$dataset</span><span class="o">]</span>
</pre></div>
</div>
</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you encounter the following error, pass your access-authorized Hugging
Face token to the gated models.</p>
<div class="highlight-shell-session notranslate"><div class="highlight"><pre><span></span><span class="go">OSError: You are trying to access a gated repo.</span>
<span class="gp"># </span>pass<span class="w"> </span>your<span class="w"> </span>HF_TOKEN
<span class="go">export HF_TOKEN=$your_personal_hf_token</span>
</pre></div>
</div>
</div>
<p class="rubric">Benchmarking examples</p>
<p>Here are some examples of running the benchmark with various options:</p>
<ul>
<li><p>Latency benchmark</p>
<p>Use this command to benchmark the latency of the DeepSeek-R1-Distill-Qwen-32B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./sglang_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>latency<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>deepseek-ai/DeepSeek-R1-Distill-Qwen-32B<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>bfloat16
</pre></div>
</div>
<p>Find the latency report at <code class="docutils literal notranslate"><span class="pre">./reports_bfloat16/summary/DeepSeek-R1-Distill-Qwen-32B_latency_report.csv</span></code>.</p>
</li>
<li><p>Throughput benchmark</p>
<p>Use this command to benchmark the throughput of the DeepSeek-R1-Distill-Qwen-32B model on eight GPUs with <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> precision.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./sglang_benchmark_report.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-s<span class="w"> </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>deepseek-ai/DeepSeek-R1-Distill-Qwen-32B<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-g<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>bfloat16<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-a<span class="w"> </span>random
</pre></div>
</div>
<p>Find the throughput report at <code class="docutils literal notranslate"><span class="pre">./reports_bfloat16/summary/DeepSeek-R1-Distill-Qwen-32B_throughput_report.csv</span></code>.</p>
</li>
</ul>
<style>
mjx-container[jax="CHTML"][display="true"] {
   text-align: left;
   margin: 0;
}
</style><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Throughput is calculated as:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[throughput\_tot = requests \times (\mathsf{\text{input lengths}} + \mathsf{\text{output lengths}}) / elapsed\_time\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[throughput\_gen = requests \times \mathsf{\text{output lengths}} / elapsed\_time\]</div>
</li>
</ul>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="further-reading">
<h5>Further reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p>To learn more about the options for latency and throughput benchmark scripts,
see <a class="github reference external" href="https://github.com/sgl-project/sglang/tree/main/benchmark/blog_v0_2">sgl-project/sglang</a>.</p></li>
<li><p>To learn more about MAD and the <code class="docutils literal notranslate"><span class="pre">madengine</span></code> CLI, see the <a class="reference external" href="https://github.com/ROCm/MAD?tab=readme-ov-file#usage-guide">MAD usage guide</a>.</p></li>
<li><p>To learn more about system settings and management practices to configure your system for
MI300X series accelerators, see <a class="reference external" href="https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/system-optimization/mi300x.html">AMD Instinct MI300X system optimization</a>.</p></li>
<li><p>For application performance optimization strategies for HPC and AI workloads,
including inference with vLLM, see <a class="reference internal" href="#document-how-to/rocm-for-ai/inference-optimization/workload"><span class="doc">AMD Instinct MI300X workload optimization</span></a>.</p></li>
<li><p>To learn how to run community models from Hugging Face on AMD GPUs, see
<a class="reference internal" href="#document-how-to/rocm-for-ai/inference/hugging-face-models"><span class="doc">Running models from Hugging Face</span></a>.</p></li>
<li><p>To learn how to fine-tune LLMs and optimize inference, see
<a class="reference internal" href="#document-how-to/rocm-for-ai/fine-tuning/fine-tuning-and-inference"><span class="doc">Fine-tuning LLMs and inference optimization</span></a>.</p></li>
<li><p>For a list of other ready-made Docker images for AI with ROCm, see
<a class="reference external" href="https://www.amd.com/en/developer/resources/infinity-hub.html#f-amd_hub_category=AI%20%26%20ML%20Models">AMD Infinity Hub</a>.</p></li>
</ul>
</section>
<section id="previous-versions">
<h5>Previous versions<a class="headerlink" href="#previous-versions" title="Link to this heading">#</a></h5>
<p>See <a class="reference internal" href="#document-how-to/rocm-for-ai/inference/benchmark-docker/previous-versions/sglang-history"><span class="doc">SGLang inference performance testing version history</span></a> to find documentation for previous releases
of SGLang inference performance testing.</p>
</section>
</section>
<span id="document-how-to/rocm-for-ai/inference/deploy-your-model"></span><section id="deploying-your-model">
<h4>Deploying your model<a class="headerlink" href="#deploying-your-model" title="Link to this heading">#</a></h4>
<p>ROCm enables inference and deployment for various classes of models including CNN, RNN, LSTM, MLP, and transformers.
This section focuses on deploying transformers-based LLM models.</p>
<p>ROCm supports vLLM and Hugging Face TGI as major LLM-serving frameworks.</p>
<section id="serving-using-vllm">
<span id="rocm-for-ai-serve-vllm"></span><h5>Serving using vLLM<a class="headerlink" href="#serving-using-vllm" title="Link to this heading">#</a></h5>
<p>vLLM is a fast and easy-to-use library for LLM inference and serving. AMD is actively working with the vLLM team to improve performance and support the latest ROCm versions.</p>
<p>See the <a class="reference external" href="https://github.com/vllm-project/vllm">GitHub repository</a> and <a class="reference external" href="https://docs.vllm.ai/">official vLLM documentation</a> for more information.</p>
<p>For guidance on using vLLM with ROCm, refer to <a class="reference external" href="https://docs.vllm.ai/en/latest/getting_started/amd-installation.html">Installation with ROCm</a>.</p>
<section id="vllm-installation">
<h6>vLLM installation<a class="headerlink" href="#vllm-installation" title="Link to this heading">#</a></h6>
<p>vLLM supports two ROCm-capable installation methods. Refer to the official documentation use the following links.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.vllm.ai/en/latest/getting_started/installation/gpu.html?device=rocm#build-image-from-source">Build from source with Docker</a> (recommended)</p></li>
<li><p><a class="reference external" href="https://docs.vllm.ai/en/latest/getting_started/installation/gpu.html?device=rocm#build-wheel-from-source">Build from source</a></p></li>
</ul>
</section>
<section id="vllm-walkthrough">
<h6>vLLM walkthrough<a class="headerlink" href="#vllm-walkthrough" title="Link to this heading">#</a></h6>
<p>Refer to this developer blog for guidance on serving with vLLM <a class="reference external" href="https://rocm.blogs.amd.com/artificial-intelligence/vllm/README.html">Inferencing and serving with vLLM on AMD GPUs — ROCm
Blogs</a></p>
</section>
<section id="validating-vllm-performance">
<h6>Validating vLLM performance<a class="headerlink" href="#validating-vllm-performance" title="Link to this heading">#</a></h6>
<p>ROCm provides a prebuilt optimized Docker image for validating the performance of LLM inference with vLLM
on the MI300X accelerator. The Docker image includes ROCm, vLLM, PyTorch, and tuning files in the CSV
format. For more information, see the guide to
<a class="reference external" href="https://github.com/ROCm/MAD/blob/develop/benchmark/vllm/README.md">LLM inference performance testing with vLLM on the AMD Instinct™ MI300X accelerator</a>
on the ROCm GitHub repository.</p>
</section>
</section>
<section id="serving-using-hugging-face-tgi">
<span id="rocm-for-ai-serve-hugging-face-tgi"></span><h5>Serving using Hugging Face TGI<a class="headerlink" href="#serving-using-hugging-face-tgi" title="Link to this heading">#</a></h5>
<p>The <a class="reference external" href="https://huggingface.co/docs/text-generation-inference/index">Hugging Face Text Generation Inference</a>
(TGI) library is optimized for serving LLMs with low latency. Refer to the <a class="reference external" href="https://huggingface.co/docs/text-generation-inference/quicktour">Quick tour of TGI</a> for more details.</p>
<section id="tgi-installation">
<h6>TGI installation<a class="headerlink" href="#tgi-installation" title="Link to this heading">#</a></h6>
<p>The easiest way to use Hugging Face TGI with ROCm on AMD Instinct accelerators is to use the official Docker image at
<a class="github reference external" href="https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference">huggingface/text-generation-inference</a>.</p>
</section>
<section id="tgi-walkthrough">
<h6>TGI walkthrough<a class="headerlink" href="#tgi-walkthrough" title="Link to this heading">#</a></h6>
<ol class="arabic">
<li><p>Set up the LLM server.</p>
<p>Deploy the Llama2 7B model with TGI using the official Docker image.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">model</span><span class="o">=</span>TheBloke/Llama-2-7B-fp16
<span class="nv">volume</span><span class="o">=</span><span class="nv">$PWD</span>
docker<span class="w"> </span>run<span class="w"> </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span>--group-add<span class="w"> </span>video<span class="w"> </span>--ipc<span class="o">=</span>host<span class="w"> </span>--shm-size<span class="w"> </span>1g<span class="w"> </span>-p<span class="w"> </span><span class="m">8080</span>:80<span class="w"> </span>-v<span class="w"> </span><span class="nv">$volume</span>:/data<span class="w"> </span>--name<span class="w"> </span>tgi_amd<span class="w"> </span>ghcr.io/huggingface/text-generation-inference:1.2-rocm<span class="w"> </span>--model-id<span class="w"> </span><span class="nv">$model</span>
</pre></div>
</div>
</li>
<li><p>Set up the client.</p>
<ol class="loweralpha simple">
<li><p>Open another shell session and run the following command to access the server with the client URL.</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span><span class="m">127</span>.0.0.1:8080/generate<span class="w"> </span><span class="se">\\</span>
-X<span class="w"> </span>POST<span class="w"> </span><span class="se">\\</span>
-d<span class="w"> </span><span class="s1">'{"inputs":"What is Deep</span>
<span class="s1">Learning?","parameters":{"max_new_tokens":20}}'</span><span class="w"> </span><span class="se">\\</span>
-H<span class="w"> </span><span class="s1">'Content-Type: application/json'</span>
</pre></div>
</div>
<ol class="loweralpha simple" start="2">
<li><p>Access the server with request endpoints.</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>request
<span class="nv">PYTHONPATH</span><span class="o">=</span>/usr/lib/python3/dist-packages<span class="w"> </span>python<span class="w"> </span>requests_model.py

<span class="sb">``</span>requests_model.py<span class="sb">``</span><span class="w"> </span>should<span class="w"> </span>look<span class="w"> </span>like:

..<span class="w"> </span>code-block::<span class="w"> </span>python

<span class="w">   </span>import<span class="w"> </span>requests

<span class="w">   </span><span class="nv">headers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>
<span class="w">     </span><span class="s2">"Content-Type"</span>:<span class="w"> </span><span class="s2">"application/json"</span>,
<span class="w">   </span><span class="o">}</span>

<span class="w">   </span><span class="nv">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>
<span class="w">      </span><span class="s1">'inputs'</span>:<span class="w"> </span><span class="s1">'What is Deep Learning?'</span>,
<span class="w">      </span><span class="s1">'parameters'</span>:<span class="w"> </span><span class="o">{</span><span class="w"> </span><span class="s1">'max_new_tokens'</span>:<span class="w"> </span><span class="m">20</span><span class="w"> </span><span class="o">}</span>,
<span class="w">   </span><span class="o">}</span>

<span class="w">   </span><span class="nv">response</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>requests.post<span class="o">(</span><span class="s1">'http://127.0.0.1:8080/generate'</span>,<span class="w"> </span><span class="nv">headers</span><span class="o">=</span>headers,<span class="w"> </span><span class="nv">json</span><span class="o">=</span>data<span class="o">)</span>

<span class="w">   </span>print<span class="o">(</span>response.json<span class="o">())</span>
</pre></div>
</div>
</li>
</ol>
<p>vLLM and Hugging Face TGI are robust solutions for anyone looking to deploy LLMs for applications that demand high
performance, low latency, and scalability.</p>
<p>Visit the topics in <a class="reference internal" href="#document-how-to/rocm-for-ai/index"><span class="doc">Using ROCm for AI</span></a> to learn about other ROCm-aware solutions for AI development.</p>
</section>
</section>
</section>
</div>
</section>
<span id="document-how-to/rocm-for-ai/inference-optimization/index"></span><section id="use-rocm-for-ai-inference-optimization">
<h3>Use ROCm for AI inference optimization<a class="headerlink" href="#use-rocm-for-ai-inference-optimization" title="Link to this heading">#</a></h3>
<p>AI inference optimization is the process of improving the performance of machine learning models and speeding up the inference process. It includes:</p>
<ul class="simple">
<li><p><strong>Quantization</strong>: This involves reducing the precision of model weights and activations while maintaining acceptable accuracy levels. Reduced precision improves inference efficiency because lower precision data requires less storage and better utilizes the hardware’s computation power.</p></li>
<li><p><strong>Kernel optimization</strong>: This technique involves optimizing computation kernels to exploit the underlying hardware capabilities. For example, the kernels can be optimized to use multiple GPU cores or utilize specialized hardware like tensor cores to accelerate the computations.</p></li>
<li><p><strong>Libraries</strong>: Libraries such as Flash Attention, xFormers, and PyTorch TunableOp are used to accelerate deep learning models and improve the performance of inference workloads.</p></li>
<li><p><strong>Hardware acceleration</strong>: Hardware acceleration techniques, like GPUs for AI inference, can significantly improve performance due to their parallel processing capabilities.</p></li>
<li><p><strong>Pruning</strong>: This involves removing unnecessary connections, layers, or weights from a pre-trained model while maintaining acceptable accuracy levels, resulting in a smaller model that requires fewer computational resources to run inference.</p></li>
</ul>
<p>Utilizing these optimization techniques with the ROCm™ software platform can significantly reduce inference time, improve performance, and reduce the cost of your AI applications.</p>
<p>Throughout the following topics, this guide discusses optimization techniques for inference workloads.</p>
<ul class="simple">
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/inference-optimization/model-quantization"><span class="doc">Model quantization</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/inference-optimization/model-acceleration-libraries"><span class="doc">Model acceleration libraries</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/inference-optimization/optimizing-with-composable-kernel"><span class="doc">Optimizing with Composable Kernel</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/inference-optimization/optimizing-triton-kernel"><span class="doc">Optimizing Triton kernels</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/inference-optimization/profiling-and-debugging"><span class="doc">Profiling and debugging</span></a></p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/inference-optimization/workload"><span class="doc">Workload tuning</span></a></p></li>
</ul>
<div class="toctree-wrapper compound">
<span id="document-how-to/rocm-for-ai/inference-optimization/model-quantization"></span><section id="model-quantization-techniques">
<h4>Model quantization techniques<a class="headerlink" href="#model-quantization-techniques" title="Link to this heading">#</a></h4>
<p>Quantization reduces the model size compared to its native full-precision version, making it easier to fit large models
onto accelerators or GPUs with limited memory usage. This section explains how to perform LLM quantization using AMD Quark, GPTQ
and bitsandbytes on AMD Instinct hardware.</p>
<section id="amd-quark">
<span id="quantize-llms-quark"></span><h5>AMD Quark<a class="headerlink" href="#amd-quark" title="Link to this heading">#</a></h5>
<p><a class="reference external" href="https://quark.docs.amd.com/latest/">AMD Quark</a> offers the leading efficient and scalable quantization solution tailored to AMD Instinct GPUs. It supports <code class="docutils literal notranslate"><span class="pre">FP8</span></code> and <code class="docutils literal notranslate"><span class="pre">INT8</span></code> quantization for activations, weights, and KV cache,
including <code class="docutils literal notranslate"><span class="pre">FP8</span></code> attention. For very large models, it employs a two-level <code class="docutils literal notranslate"><span class="pre">INT4-FP8</span></code> scheme—storing weights in <code class="docutils literal notranslate"><span class="pre">INT4</span></code> while computing with <code class="docutils literal notranslate"><span class="pre">FP8</span></code>—for nearly 4× compression without sacrificing accuracy.
Quark scales efficiently across multiple GPUs, efficiently handling ultra-large models like Llama-3.1-405B. Quantized <code class="docutils literal notranslate"><span class="pre">FP8</span></code> models like Llama, Mixtral, and Grok-1 are available under the <a class="reference external" href="https://huggingface.co/collections/amd/quark-quantized-ocp-fp8-models-66db7936d18fcbaf95d4405c">AMD organization on Hugging Face</a>, and can be deployed directly via <a class="reference external" href="https://github.com/vllm-project/vllm/tree/main/vllm">vLLM</a>.</p>
<section id="installing-quark">
<h6>Installing Quark<a class="headerlink" href="#installing-quark" title="Link to this heading">#</a></h6>
<p>The latest release of Quark can be installed with pip</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>amd-quark
</pre></div>
</div>
<p>For detailed installation instructions, refer to the <a class="reference external" href="https://quark.docs.amd.com/latest/install.html">Quark documentation</a>.</p>
</section>
<section id="using-quark-for-quantization">
<h6>Using Quark for quantization<a class="headerlink" href="#using-quark-for-quantization" title="Link to this heading">#</a></h6>
<ol class="arabic">
<li><p>First, load the pre-trained model and its corresponding tokenizer using the Hugging Face <code class="docutils literal notranslate"><span class="pre">transformers</span></code> library.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">MODEL_ID</span> <span class="o">=</span> <span class="s2">"meta-llama/Llama-2-70b-chat-hf"</span>
<span class="n">MAX_SEQ_LEN</span> <span class="o">=</span> <span class="mi">512</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">MODEL_ID</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_ID</span><span class="p">,</span> <span class="n">model_max_length</span><span class="o">=</span><span class="n">MAX_SEQ_LEN</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
</pre></div>
</div>
</li>
<li><p>Prepare the calibration DataLoader (static quantization requires calibration data).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">NUM_CALIBRATION_DATA</span> <span class="o">=</span> <span class="mi">512</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"mit-han-lab/pile-val-backup"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">"validation"</span><span class="p">)</span>
<span class="n">text_data</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"text"</span><span class="p">][:</span><span class="n">NUM_CALIBRATION_DATA</span><span class="p">]</span>

<span class="n">tokenized_outputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
<span class="n">text_data</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_SEQ_LEN</span>
<span class="p">)</span>
<span class="n">calib_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
<span class="n">tokenized_outputs</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Define the quantization configuration. See the comments in the following code snippet for descriptions of each configuration option.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch.quantization</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span><span class="n">Config</span><span class="p">,</span> <span class="n">QuantizationConfig</span><span class="p">,</span>
                                     <span class="n">FP8E4M3PerTensorSpec</span><span class="p">)</span>

<span class="c1"># Define fp8/per-tensor/static spec.</span>
<span class="n">FP8_PER_TENSOR_SPEC</span> <span class="o">=</span> <span class="n">FP8E4M3PerTensorSpec</span><span class="p">(</span><span class="n">observer_method</span><span class="o">=</span><span class="s2">"min_max"</span><span class="p">,</span>
    <span class="n">is_dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">to_quantization_spec</span><span class="p">()</span>

<span class="c1"># Define global quantization config, input tensors and weight apply FP8_PER_TENSOR_SPEC.</span>
<span class="n">global_quant_config</span> <span class="o">=</span> <span class="n">QuantizationConfig</span><span class="p">(</span><span class="n">input_tensors</span><span class="o">=</span><span class="n">FP8_PER_TENSOR_SPEC</span><span class="p">,</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">FP8_PER_TENSOR_SPEC</span><span class="p">)</span>

<span class="c1"># Define quantization config for kv-cache layers, output tensors apply FP8_PER_TENSOR_SPEC.</span>
<span class="n">KV_CACHE_SPEC</span> <span class="o">=</span> <span class="n">FP8_PER_TENSOR_SPEC</span>
<span class="n">kv_cache_layer_names_for_llama</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"*k_proj"</span><span class="p">,</span> <span class="s2">"*v_proj"</span><span class="p">]</span>
<span class="n">kv_cache_quant_config</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span> <span class="p">:</span>
    <span class="n">QuantizationConfig</span><span class="p">(</span><span class="n">input_tensors</span><span class="o">=</span><span class="n">global_quant_config</span><span class="o">.</span><span class="n">input_tensors</span><span class="p">,</span>
                       <span class="n">weight</span><span class="o">=</span><span class="n">global_quant_config</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                       <span class="n">output_tensors</span><span class="o">=</span><span class="n">KV_CACHE_SPEC</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">kv_cache_layer_names_for_llama</span><span class="p">}</span>
<span class="n">layer_quant_config</span> <span class="o">=</span> <span class="n">kv_cache_quant_config</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="n">EXCLUDE_LAYERS</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"lm_head"</span><span class="p">]</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="n">Config</span><span class="p">(</span>
    <span class="n">global_quant_config</span><span class="o">=</span><span class="n">global_quant_config</span><span class="p">,</span>
    <span class="n">layer_quant_config</span><span class="o">=</span><span class="n">layer_quant_config</span><span class="p">,</span>
    <span class="n">kv_cache_quant_config</span><span class="o">=</span><span class="n">kv_cache_quant_config</span><span class="p">,</span>
    <span class="n">exclude</span><span class="o">=</span><span class="n">EXCLUDE_LAYERS</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Quantize the model and export</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelQuantizer</span><span class="p">,</span> <span class="n">ModelExporter</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch.export</span><span class="w"> </span><span class="kn">import</span> <span class="n">ExporterConfig</span><span class="p">,</span> <span class="n">JsonExporterConfig</span>

<span class="c1"># Apply quantization.</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">ModelQuantizer</span><span class="p">(</span><span class="n">quant_config</span><span class="p">)</span>
<span class="n">quant_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">quantize_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">calib_dataloader</span><span class="p">)</span>

<span class="c1"># Freeze quantized model to export.</span>
<span class="n">freezed_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Define export config.</span>
<span class="n">LLAMA_KV_CACHE_GROUP</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"*k_proj"</span><span class="p">,</span> <span class="s2">"*v_proj"</span><span class="p">]</span>
<span class="n">export_config</span> <span class="o">=</span> <span class="n">ExporterConfig</span><span class="p">(</span><span class="n">json_export_config</span><span class="o">=</span><span class="n">JsonExporterConfig</span><span class="p">())</span>
<span class="n">export_config</span><span class="o">.</span><span class="n">json_export_config</span><span class="o">.</span><span class="n">kv_cache_group</span> <span class="o">=</span> <span class="n">LLAMA_KV_CACHE_GROUP</span>

<span class="n">EXPORT_DIR</span> <span class="o">=</span> <span class="n">MODEL_ID</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"/"</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s2">"-w-fp8-a-fp8-kvcache-fp8-pertensor"</span>
<span class="n">exporter</span> <span class="o">=</span> <span class="n">ModelExporter</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">export_config</span><span class="p">,</span> <span class="n">export_dir</span><span class="o">=</span><span class="n">EXPORT_DIR</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">exporter</span><span class="o">.</span><span class="n">export_safetensors_model</span><span class="p">(</span><span class="n">freezed_model</span><span class="p">,</span>
        <span class="n">quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="evaluating-the-quantized-model-with-vllm">
<h6>Evaluating the quantized model with vLLM<a class="headerlink" href="#evaluating-the-quantized-model-with-vllm" title="Link to this heading">#</a></h6>
<p>The exported Quark-quantized model can be loaded directly by vLLM for inference. You need to specify the model path and inform vLLM about the quantization method (<code class="docutils literal notranslate"><span class="pre">quantization='quark'</span></code>) and the KV cache data type (<code class="docutils literal notranslate"><span class="pre">kv_cache_dtype='fp8'</span></code>).
Use the <code class="docutils literal notranslate"><span class="pre">LLM</span></code> interface to load the model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">vllm</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">SamplingParamsinterface</span>

<span class="c1"># Sample prompts.</span>
<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">"Hello, my name is"</span><span class="p">,</span>
    <span class="s2">"The president of the United States is"</span><span class="p">,</span>
    <span class="s2">"The capital of France is"</span><span class="p">,</span>
    <span class="s2">"The future of AI is"</span><span class="p">,</span>
<span class="p">]</span>
<span class="c1"># Create a sampling params object.</span>
<span class="n">sampling_params</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>

<span class="c1"># Create an LLM.</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"Llama-2-70b-chat-hf-w-fp8-a-fp8-kvcache-fp8-pertensor"</span><span class="p">,</span>
          <span class="n">kv_cache_dtype</span><span class="o">=</span><span class="s1">'fp8'</span><span class="p">,</span><span class="n">quantization</span><span class="o">=</span><span class="s1">'quark'</span><span class="p">)</span>
<span class="c1"># Generate texts from the prompts. The output is a list of RequestOutput objects</span>
<span class="c1"># that contain the prompt, generated text, and other information.</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">)</span>
<span class="c1"># Print the outputs.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Generated Outputs:</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">"-"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">prompt</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Prompt:    </span><span class="si">{</span><span class="n">prompt</span><span class="si">!r}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Output:    </span><span class="si">{</span><span class="n">generated_text</span><span class="si">!r}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
</pre></div>
</div>
<p>You can also evaluate the quantized model’s accuracy on standard benchmarks using the <a class="reference external" href="https://github.com/EleutherAI/lm-evaluation-harness">lm-evaluation-harness</a>. Pass the necessary vLLM arguments to <code class="docutils literal notranslate"><span class="pre">lm_eval</span></code> via <code class="docutils literal notranslate"><span class="pre">--model_args</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>lm_eval<span class="w"> </span>--model<span class="w"> </span>vllm<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model_args<span class="w"> </span><span class="nv">pretrained</span><span class="o">=</span>Llama-2-70b-chat-hf-w-fp8-a-fp8-kvcache-fp8-pertensor,kv_cache_dtype<span class="o">=</span><span class="s1">'fp8'</span>,quantization<span class="o">=</span><span class="s1">'quark'</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--tasks<span class="w"> </span>gsm8k
</pre></div>
</div>
<p>This provides a standardized way to measure the performance impact of quantization.
.. _fine-tune-llms-gptq:</p>
</section>
</section>
<section id="gptq">
<h5>GPTQ<a class="headerlink" href="#gptq" title="Link to this heading">#</a></h5>
<p>GPTQ is a post-training quantization technique where each row of the weight matrix is quantized independently to find a
version of the weights that minimizes error. These weights are quantized to <code class="docutils literal notranslate"><span class="pre">int4</span></code> but are restored to <code class="docutils literal notranslate"><span class="pre">fp16</span></code> on the
fly during inference. This can save your memory usage by a factor of four. A speedup in inference is expected because
inference of GPTQ models uses a lower bit width, which takes less time to communicate.</p>
<p>Before setting up the GPTQ configuration in Transformers, ensure the <a class="reference external" href="https://github.com/AutoGPTQ/AutoGPTQ">AutoGPTQ</a> library
is installed.</p>
<section id="installing-autogptq">
<h6>Installing AutoGPTQ<a class="headerlink" href="#installing-autogptq" title="Link to this heading">#</a></h6>
<p>The AutoGPTQ library implements the GPTQ algorithm.</p>
<ol class="arabic">
<li><p>Use the following command to install the latest stable release of AutoGPTQ from pip.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># This will install pre-built wheel for a specific ROCm version.</span>

pip<span class="w"> </span>install<span class="w"> </span>auto-gptq<span class="w"> </span>--no-build-isolation<span class="w"> </span>--extra-index-url<span class="w"> </span>https://huggingface.github.io/autogptq-index/whl/rocm573/
</pre></div>
</div>
<p>Or, install AutoGPTQ from source for the appropriate ROCm version (for example, ROCm 6.1).</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Clone the source code.</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/AutoGPTQ/AutoGPTQ.git
<span class="nb">cd</span><span class="w"> </span>AutoGPTQ

<span class="c1"># Speed up the compilation by specifying PYTORCH_ROCM_ARCH to target device.</span>
<span class="nv">PYTORCH_ROCM_ARCH</span><span class="o">=</span>gfx942<span class="w"> </span><span class="nv">ROCM_VERSION</span><span class="o">=</span><span class="m">6</span>.1<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>.

<span class="c1"># Show the package after the installation</span>
</pre></div>
</div>
</li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">show</span> <span class="pre">auto-gptq</span></code> to print information for the installed <code class="docutils literal notranslate"><span class="pre">auto-gptq</span></code> package. Its output should look like
this:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>Name:<span class="w"> </span>auto-gptq
Version:<span class="w"> </span><span class="m">0</span>.8.0.dev0+rocm6.1
...
</pre></div>
</div>
</li>
</ol>
</section>
<section id="using-gptq-with-autogptq">
<h6>Using GPTQ with AutoGPTQ<a class="headerlink" href="#using-gptq-with-autogptq" title="Link to this heading">#</a></h6>
<ol class="arabic">
<li><p>Run the following code snippet.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">TextGenerationPipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">auto_gptq</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoGPTQForCausalLM</span><span class="p">,</span> <span class="n">BaseQuantizeConfig</span>
<span class="n">base_model_name</span> <span class="o">=</span> <span class="s2">"NousResearch/Llama-2-7b-hf"</span>
<span class="n">quantized_model_name</span> <span class="o">=</span> <span class="s2">"llama-2-7b-hf-gptq"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model_name</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">examples</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">tokenizer</span><span class="p">(</span>
        <span class="s2">"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm."</span>
    <span class="p">)</span>
<span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span>
</pre></div>
</div>
<p>The resulting examples should be a list of dictionaries whose keys are <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> and <code class="docutils literal notranslate"><span class="pre">attention_mask</span></code>.</p>
</li>
<li><p>Set up the quantization configuration using the following snippet.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">quantize_config</span> <span class="o">=</span> <span class="n">BaseQuantizeConfig</span><span class="p">(</span>
    <span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>               <span class="c1"># quantize model to 4-bit</span>
    <span class="n">group_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>       <span class="c1"># it is recommended to set the value to 128</span>
    <span class="n">desc_act</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Load the non-quantized model using the AutoGPTQ class and run the quantization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import auto_gptq class.</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">auto_gptq</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoGPTQForCausalLM</span>

<span class="c1"># Load non-quantized model.</span>
<span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoGPTQForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model_name</span><span class="p">,</span> <span class="n">quantize_config</span><span class="p">,</span> <span class="n">device_map</span> <span class="o">=</span> <span class="s2">"auto"</span><span class="p">)</span>
<span class="n">base_model</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span>

<span class="c1"># Save quantized model.</span>
<span class="n">base_model</span><span class="o">.</span><span class="n">save_quantized</span><span class="p">(</span><span class="n">quantized_model_name</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="using-gptq-with-hugging-face-transformers">
<h6>Using GPTQ with Hugging Face Transformers<a class="headerlink" href="#using-gptq-with-hugging-face-transformers" title="Link to this heading">#</a></h6>
<ol class="arabic">
<li><p>To perform a GPTQ quantization using Hugging Face Transformers, you need to create a <code class="docutils literal notranslate"><span class="pre">GPTQConfig</span></code> instance and set the
number of bits to quantize to, and a dataset to calibrate the weights.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">GPTQConfig</span>

<span class="n">base_model_name</span> <span class="o">=</span> <span class="s2">" NousResearch/Llama-2-7b-hf"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model_name</span><span class="p">)</span>
<span class="n">gptq_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="s2">"c4"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Load a model to quantize using <code class="docutils literal notranslate"><span class="pre">AutoModelForCausalLM</span></code> and pass the
<code class="docutils literal notranslate"><span class="pre">gptq_config</span></code> to its <code class="docutils literal notranslate"><span class="pre">from_pretained</span></code> method. Set <code class="docutils literal notranslate"><span class="pre">device_map=”auto”</span></code> to
automatically offload the model to available GPU resources.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">quantized_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                        <span class="n">base_model_name</span><span class="p">,</span>
                        <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span>
                        <span class="n">quantization_config</span><span class="o">=</span><span class="n">gptq_config</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Once the model is quantized, you can push the model and tokenizer to Hugging Face Hub for easy share and access.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">quantized_model</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s2">"llama-2-7b-hf-gptq"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s2">"llama-2-7b-hf-gptq"</span><span class="p">)</span>
</pre></div>
</div>
<p>Or, you can save the model locally using the following snippet.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">quantized_model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">"llama-2-7b-gptq"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">"llama-2-7b-gptq"</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="exllama-v2-support">
<h6>ExLlama-v2 support<a class="headerlink" href="#exllama-v2-support" title="Link to this heading">#</a></h6>
<p>ExLlama is a Python/C++/CUDA implementation of the Llama model that is
designed for faster inference with 4-bit GPTQ weights. The ExLlama
kernel is activated by default when users create a <code class="docutils literal notranslate"><span class="pre">GPTQConfig</span></code> object. To
boost inference speed even further on Instinct accelerators, use the ExLlama-v2
kernels by configuring the <code class="docutils literal notranslate"><span class="pre">exllama_config</span></code> parameter as the following.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">GPTQConfig</span>
<span class="c1">#pretrained_model_dir = "meta-llama/Llama-2-7b"</span>
<span class="n">base_model_name</span> <span class="o">=</span> <span class="s2">"NousResearch/Llama-2-7b-hf"</span>
<span class="n">gptq_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="s2">"c4"</span><span class="p">,</span> <span class="n">exllama_config</span><span class="o">=</span><span class="p">{</span><span class="s2">"version"</span><span class="p">:</span><span class="mi">2</span><span class="p">})</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                        <span class="n">base_model_name</span><span class="p">,</span>
                        <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span>
                        <span class="n">quantization_config</span><span class="o">=</span><span class="n">gptq_config</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="bitsandbytes">
<h5>bitsandbytes<a class="headerlink" href="#bitsandbytes" title="Link to this heading">#</a></h5>
<p>The <a class="reference external" href="https://github.com/ROCm/bitsandbytes">ROCm-aware bitsandbytes</a> library is
a lightweight Python wrapper around CUDA custom functions, in particular 8-bit optimizer, matrix multiplication, and
8-bit and 4-bit quantization functions. The library includes quantization primitives for 8-bit and 4-bit operations
through <code class="docutils literal notranslate"><span class="pre">bitsandbytes.nn.Linear8bitLt</span></code> and <code class="docutils literal notranslate"><span class="pre">bitsandbytes.nn.Linear4bit</span></code> and 8-bit optimizers through the
<code class="docutils literal notranslate"><span class="pre">bitsandbytes.optim</span></code> module. These modules are supported on AMD Instinct accelerators.</p>
<section id="installing-bitsandbytes">
<h6>Installing bitsandbytes<a class="headerlink" href="#installing-bitsandbytes" title="Link to this heading">#</a></h6>
<ol class="arabic">
<li><p>To install bitsandbytes for ROCm 6.0 (and later), use the following commands.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Clone the github repo</span>
git<span class="w"> </span>clone<span class="w"> </span>--recurse<span class="w"> </span>https://github.com/ROCm/bitsandbytes.git
<span class="nb">cd</span><span class="w"> </span>bitsandbytes
git<span class="w"> </span>checkout<span class="w"> </span>rocm_enabled_multi_backend

<span class="c1"># Install dependencies</span>
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements-dev.txt

<span class="c1"># Use -DBNB_ROCM_ARCH to specify target GPU arch</span>
cmake<span class="w"> </span>-DBNB_ROCM_ARCH<span class="o">=</span><span class="s2">"gfx942"</span><span class="w"> </span>-DCOMPUTE_BACKEND<span class="o">=</span>hip<span class="w"> </span>-S<span class="w"> </span>.

<span class="c1"># Compile the project</span>
make

<span class="c1"># Install</span>
python<span class="w"> </span>setup.py<span class="w"> </span>install
</pre></div>
</div>
</li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">show</span> <span class="pre">bitsandbytes</span></code> to show the information about the installed bitsandbytes package. Its output should
look like the following.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>Name:<span class="w"> </span>bitsandbytes
Version:<span class="w"> </span><span class="m">0</span>.44.0.dev0
...
</pre></div>
</div>
</li>
</ol>
</section>
<section id="using-bitsandbytes-primitives">
<h6>Using bitsandbytes primitives<a class="headerlink" href="#using-bitsandbytes-primitives" title="Link to this heading">#</a></h6>
<p>To get started with bitsandbytes primitives, use the following code as reference.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">bitsandbytes</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">bnb</span>

<span class="c1"># Use Int8 Matrix Multiplication</span>
<span class="n">bnb</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">6.0</span><span class="p">)</span>

<span class="c1"># Use bitsandbytes 8-bit Optimizers</span>
<span class="n">adam</span> <span class="o">=</span> <span class="n">bnb</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam8bit</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.995</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="using-bitsandbytes-with-hugging-face-transformers">
<h6>Using bitsandbytes with Hugging Face Transformers<a class="headerlink" href="#using-bitsandbytes-with-hugging-face-transformers" title="Link to this heading">#</a></h6>
<p>To load a Transformers model in 4-bit, set <code class="docutils literal notranslate"><span class="pre">load_in_4bit=true</span></code> in <code class="docutils literal notranslate"><span class="pre">BitsAndBytesConfig</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">base_model_name</span> <span class="o">=</span> <span class="s2">"NousResearch/Llama-2-7b-hf"</span>
<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">bnb_model_4bit</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="n">base_model_name</span><span class="p">,</span>
        <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span>
        <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>

<span class="c1"># Check the memory footprint with get_memory_footprint method</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bnb_model_4bit</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">())</span>
</pre></div>
</div>
<p>To load a model in 8-bit for inference, use the <code class="docutils literal notranslate"><span class="pre">load_in_8bit</span></code> option.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">base_model_name</span> <span class="o">=</span> <span class="s2">"NousResearch/Llama-2-7b-hf"</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model_name</span><span class="p">)</span>
<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_8bit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model_name</span><span class="p">)</span>
<span class="n">bnb_model_8bit</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="n">base_model_name</span><span class="p">,</span>
        <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span>
        <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">"What is a large language model?"</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<span id="document-how-to/rocm-for-ai/inference-optimization/model-acceleration-libraries"></span><section id="model-acceleration-libraries">
<h4>Model acceleration libraries<a class="headerlink" href="#model-acceleration-libraries" title="Link to this heading">#</a></h4>
<p>This section discusses model acceleration techniques and libraries to improve memory efficiency and performance.</p>
<section id="flash-attention-2">
<span id="acceleration-flash-attention"></span><h5>Flash Attention 2<a class="headerlink" href="#flash-attention-2" title="Link to this heading">#</a></h5>
<p>Flash Attention is a technique designed to reduce memory movements between GPU SRAM and high-bandwidth memory (HBM). By
using a tiling approach, Flash Attention 2 improves memory locality in the nested loops of query, key, and value
computations within the Attention modules of LLMs. These modules include Multi-Head Attention (MHA), Group-Query
Attention (GQA), and Multi-Query Attention (MQA). This reduction in memory movements significantly decreases the
time-to-first-token (TTFT) latency for large batch sizes and long prompt sequences, thereby enhancing overall
performance.</p>
<img alt="Attention module of a large language module utilizing tiling" class="align-center" src="_images/attention-module.png"/>
<section id="installing-flash-attention-2">
<h6>Installing Flash Attention 2<a class="headerlink" href="#installing-flash-attention-2" title="Link to this heading">#</a></h6>
<p>ROCm provides two different implementations of Flash Attention 2 modules. They can be deployed interchangeably:</p>
<ul class="simple">
<li><p>ROCm <a class="reference external" href="https://github.com/ROCm/composable_kernel/tree/develop/example/01_gemm">Composable Kernel</a>
(CK) Flash Attention 2</p></li>
<li><p><a class="reference external" href="https://triton-lang.org/main/index.html">OpenAI Triton</a> Flash Attention 2</p></li>
</ul>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-72" name="sd-tab-set-36" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-72">
CK Flash Attention 2</label><div class="sd-tab-content docutils">
<p>To install CK Flash Attention 2, use the following commands.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install from source</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/flash-attention.git
<span class="nb">cd</span><span class="w"> </span>flash-attention/
<span class="nv">GPU_ARCHS</span><span class="o">=</span>gfx942<span class="w"> </span>python<span class="w"> </span>setup.py<span class="w"> </span>install<span class="w"> </span><span class="c1">#MI300 series</span>
</pre></div>
</div>
<p>Hugging Face Transformers can easily deploy the CK Flash Attention 2 module by passing an argument
<code class="docutils literal notranslate"><span class="pre">attn_implementation="flash_attention_2"</span></code> in the <code class="docutils literal notranslate"><span class="pre">from_pretrained</span></code> class.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda:0"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"NousResearch/Meta-Llama-3-8B"</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s1">'Today is'</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'pt'</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">model_eager</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">attn_implementation</span><span class="o">=</span><span class="s2">"eager"</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_ckFAv2</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">attn_implementation</span><span class="o">=</span><span class="s2">"flash_attention_2"</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"eager GQA: "</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">model_eager</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">10</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"ckFAv2 GQA: "</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">model_ckFAv2</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">10</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

<span class="c1">#  eager GQA:  Today is the day of the Lord, and we are the</span>
<span class="c1"># ckFAv2 GQA: Today is the day of the Lord, and we are the</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-73" name="sd-tab-set-36" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-73">
Triton Flash Attention 2</label><div class="sd-tab-content docutils">
<p>The Triton Flash Attention 2 module is implemented in Python and uses OpenAI’s JIT compiler. This module has been
upstreamed into the vLLM serving toolkit, discussed in :doc:’llm-inference-frameworks’.</p>
<ol class="arabic">
<li><p>To install Triton Flash Attention 2 and run the benchmark, use the following commands.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install from the source</span>
pip<span class="w"> </span>uninstall<span class="w"> </span>pytorch-triton-rocm<span class="w"> </span>triton<span class="w"> </span>-y
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/triton.git
<span class="nb">cd</span><span class="w"> </span>triton/python
<span class="nv">GPU_ARCHS</span><span class="o">=</span>gfx942<span class="w"> </span>python<span class="w"> </span>setup.py<span class="w"> </span>install<span class="w"> </span><span class="c1">#MI300 series</span>
pip<span class="w"> </span>install<span class="w"> </span>matplotlib<span class="w"> </span>pandas
</pre></div>
</div>
</li>
<li><p>To test, run the Triton Flash Attention 2 performance benchmark.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test the triton FA v2 kernel</span>
python<span class="w"> </span>https://github.com/ROCm/triton/blob/triton-mlir/python/perf-kernels/flash-attention.py
<span class="c1"># Results (Okay to release TFLOPS number ???)</span>
fused-attention-fwd-d128:
<span class="w">    </span>BATCH<span class="w">    </span>HQ<span class="w">    </span>HK<span class="w">  </span>N_CTX_Q<span class="w">  </span>N_CTX_K<span class="w">      </span>TFLOPS
<span class="m">0</span><span class="w">    </span><span class="m">16</span>.0<span class="w">  </span><span class="m">16</span>.0<span class="w">  </span><span class="m">16</span>.0<span class="w">   </span><span class="m">1024</span>.0<span class="w">   </span><span class="m">1024</span>.0<span class="w">  </span><span class="m">287</span>.528411
<span class="m">1</span><span class="w">     </span><span class="m">8</span>.0<span class="w">  </span><span class="m">16</span>.0<span class="w">  </span><span class="m">16</span>.0<span class="w">   </span><span class="m">2048</span>.0<span class="w">   </span><span class="m">2048</span>.0<span class="w">  </span><span class="m">287</span>.490806
<span class="m">2</span><span class="w">     </span><span class="m">4</span>.0<span class="w">  </span><span class="m">16</span>.0<span class="w">  </span><span class="m">16</span>.0<span class="w">   </span><span class="m">4096</span>.0<span class="w">   </span><span class="m">4096</span>.0<span class="w">  </span><span class="m">345</span>.966031
<span class="m">3</span><span class="w">     </span><span class="m">2</span>.0<span class="w">  </span><span class="m">16</span>.0<span class="w">  </span><span class="m">16</span>.0<span class="w">   </span><span class="m">8192</span>.0<span class="w">   </span><span class="m">8192</span>.0<span class="w">  </span><span class="m">361</span>.369510
<span class="m">4</span><span class="w">     </span><span class="m">1</span>.0<span class="w">  </span><span class="m">16</span>.0<span class="w">  </span><span class="m">16</span>.0<span class="w">  </span><span class="m">16384</span>.0<span class="w">  </span><span class="m">16384</span>.0<span class="w">  </span><span class="m">356</span>.873720
<span class="m">5</span><span class="w">     </span><span class="m">2</span>.0<span class="w">  </span><span class="m">48</span>.0<span class="w">  </span><span class="m">48</span>.0<span class="w">   </span><span class="m">1024</span>.0<span class="w">   </span><span class="m">1024</span>.0<span class="w">  </span><span class="m">216</span>.916235
<span class="m">6</span><span class="w">     </span><span class="m">2</span>.0<span class="w">  </span><span class="m">48</span>.0<span class="w">  </span><span class="m">48</span>.0<span class="w">   </span><span class="m">2048</span>.0<span class="w">   </span><span class="m">1024</span>.0<span class="w">  </span><span class="m">271</span>.027578
<span class="m">7</span><span class="w">     </span><span class="m">2</span>.0<span class="w">  </span><span class="m">48</span>.0<span class="w">  </span><span class="m">48</span>.0<span class="w">   </span><span class="m">4096</span>.0<span class="w">   </span><span class="m">8192</span>.0<span class="w">  </span><span class="m">337</span>.367372
<span class="m">8</span><span class="w">     </span><span class="m">2</span>.0<span class="w">  </span><span class="m">48</span>.0<span class="w">  </span><span class="m">48</span>.0<span class="w">   </span><span class="m">8192</span>.0<span class="w">   </span><span class="m">4096</span>.0<span class="w">  </span><span class="m">363</span>.481649
<span class="m">9</span><span class="w">     </span><span class="m">2</span>.0<span class="w">  </span><span class="m">48</span>.0<span class="w">  </span><span class="m">48</span>.0<span class="w">  </span><span class="m">16384</span>.0<span class="w">   </span><span class="m">8192</span>.0<span class="w">  </span><span class="m">375</span>.013622
<span class="m">10</span><span class="w">    </span><span class="m">8</span>.0<span class="w">  </span><span class="m">16</span>.0<span class="w">  </span><span class="m">16</span>.0<span class="w">   </span><span class="m">1989</span>.0<span class="w">  </span><span class="m">15344</span>.0<span class="w">  </span><span class="m">321</span>.791333
<span class="m">11</span><span class="w">    </span><span class="m">4</span>.0<span class="w">  </span><span class="m">16</span>.0<span class="w">  </span><span class="m">16</span>.0<span class="w">   </span><span class="m">4097</span>.0<span class="w">    </span><span class="m">163</span>.0<span class="w">  </span><span class="m">122</span>.104888
<span class="m">12</span><span class="w">    </span><span class="m">2</span>.0<span class="w">  </span><span class="m">16</span>.0<span class="w">  </span><span class="m">16</span>.0<span class="w">   </span><span class="m">8122</span>.0<span class="w">   </span><span class="m">2159</span>.0<span class="w">  </span><span class="m">337</span>.060283
<span class="m">13</span><span class="w">    </span><span class="m">1</span>.0<span class="w">  </span><span class="m">16</span>.0<span class="w">  </span><span class="m">16</span>.0<span class="w">  </span><span class="m">16281</span>.0<span class="w">      </span><span class="m">7</span>.0<span class="w">    </span><span class="m">5</span>.234012
<span class="m">14</span><span class="w">    </span><span class="m">2</span>.0<span class="w">  </span><span class="m">48</span>.0<span class="w">  </span><span class="m">48</span>.0<span class="w">   </span><span class="m">1021</span>.0<span class="w">   </span><span class="m">1020</span>.0<span class="w">  </span><span class="m">214</span>.657425
<span class="m">15</span><span class="w">    </span><span class="m">2</span>.0<span class="w">  </span><span class="m">48</span>.0<span class="w">  </span><span class="m">48</span>.0<span class="w">   </span><span class="m">2001</span>.0<span class="w">   </span><span class="m">2048</span>.0<span class="w">  </span><span class="m">314</span>.429118
<span class="m">16</span><span class="w">    </span><span class="m">2</span>.0<span class="w">  </span><span class="m">48</span>.0<span class="w">  </span><span class="m">48</span>.0<span class="w">   </span><span class="m">3996</span>.0<span class="w">   </span><span class="m">9639</span>.0<span class="w">  </span><span class="m">330</span>.411368
<span class="m">17</span><span class="w">    </span><span class="m">2</span>.0<span class="w">  </span><span class="m">48</span>.0<span class="w">  </span><span class="m">48</span>.0<span class="w">   </span><span class="m">8181</span>.0<span class="w">   </span><span class="m">1021</span>.0<span class="w">  </span><span class="m">324</span>.614980
</pre></div>
</div>
</li>
</ol>
</div>
</div>
</section>
</section>
<section id="xformers">
<h5>xFormers<a class="headerlink" href="#xformers" title="Link to this heading">#</a></h5>
<p>xFormers also improves the performance of attention modules. Although xFormers attention performs very
similarly to Flash Attention 2 due to its tiling behavior of query, key, and value, it’s widely used for LLMs and
Stable Diffusion models with the Hugging Face Diffusers library.</p>
<section id="installing-ck-xformers">
<h6>Installing CK xFormers<a class="headerlink" href="#installing-ck-xformers" title="Link to this heading">#</a></h6>
<p>Use the following commands to install CK xFormers.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install from source</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ROCm/xformers.git
<span class="nb">cd</span><span class="w"> </span>xformers/
git<span class="w"> </span>submodule<span class="w"> </span>update<span class="w"> </span>--init<span class="w"> </span>--recursive
<span class="nv">PYTORCH_ROCM_ARCH</span><span class="o">=</span>gfx942<span class="w"> </span>python<span class="w"> </span>setup.py<span class="w"> </span>install<span class="w"> </span><span class="c1">#Instinct MI300-series</span>
</pre></div>
</div>
</section>
</section>
<section id="pytorch-built-in-acceleration">
<h5>PyTorch built-in acceleration<a class="headerlink" href="#pytorch-built-in-acceleration" title="Link to this heading">#</a></h5>
<p><a class="reference external" href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">PyTorch compilation
mode</a>
synthesizes the model into a graph and then lowers it to prime
operators. These operators are compiled using TorchInductor, which uses
OpenAI Triton as a building block for GPU acceleration. One advantage of
PyTorch compilation mode is that its GPU kernels are written in Python,
making modifying and extending them easier. PyTorch compilation mode
often delivers higher performance, as model operations are fused before
runtime, which allows for easy deployment of high-performance kernels.</p>
<section id="pytorch-compilation">
<h6>PyTorch compilation<a class="headerlink" href="#pytorch-compilation" title="Link to this heading">#</a></h6>
<p>To utilize the PyTorch compilation mode, specific layers of the model
must be explicitly assigned as compilation targets. In the case of LLM,
where autoregressive token decoding generates dynamically changing
key/value sizes, limiting the key/value size to a static dimension,
<code class="docutils literal notranslate"><span class="pre">max_cache_length</span></code>, is necessary to utilize the performance benefits
of the PyTorch compilation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sample script to run LLM with the static key-value cache and PyTorch compilation</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">StaticCache</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda:0"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"TOKENIZERS_PARALLELISM"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"NousResearch/Meta-Llama-3-8B"</span>
<span class="n">prompts</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">prompts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"New york city is where "</span>
<span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">decode_one_tokens</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cur_token</span><span class="p">,</span> <span class="n">input_pos</span><span class="p">,</span> <span class="n">cache_position</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">cur_token</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">=</span><span class="n">input_pos</span><span class="p">,</span> <span class="n">cache_position</span><span class="o">=</span><span class="n">cache_position</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">new_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">new_token</span>

<span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>

<span class="c1"># Static key-value cache</span>
<span class="n">max_cache_length</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">model</span><span class="o">.</span><span class="n">_setup_cache</span><span class="p">(</span><span class="n">StaticCache</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">max_cache_len</span><span class="o">=</span><span class="n">max_cache_length</span><span class="p">)</span>
<span class="n">cache_position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">+</span> <span class="n">max_new_tokens</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">generated_ids</span><span class="p">[:,</span> <span class="n">cache_position</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">cache_position</span><span class="o">=</span><span class="n">cache_position</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>

<span class="c1"># torch compilation</span>
<span class="n">decode_one_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">decode_one_tokens</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"max-autotune-no-cudagraphs"</span><span class="p">,</span><span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">generated_ids</span><span class="p">[:,</span> <span class="n">seq_length</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_token</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">cache_position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">seq_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">sdp_kernel</span><span class="p">(</span><span class="n">enable_flash</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">enable_mem_efficient</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">enable_math</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="n">next_token</span> <span class="o">=</span> <span class="n">decode_one_tokens</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">next_token</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="kc">None</span><span class="p">,</span> <span class="n">cache_position</span><span class="p">)</span>
            <span class="n">generated_ids</span><span class="p">[:,</span> <span class="n">cache_position</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_token</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="n">cache_position</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</section>
<section id="pytorch-tunableop">
<span id="fine-tuning-llms-pytorch-tunableop"></span><h6>PyTorch TunableOp<a class="headerlink" href="#pytorch-tunableop" title="Link to this heading">#</a></h6>
<p>ROCm PyTorch (2.2.0 and later) allows users to use high-performance ROCm
GEMM kernel libraries through PyTorch’s built-in TunableOp options.
This enables users to automatically pick up the best-performing GEMM
kernels from <a class="reference external" href="https://rocm.docs.amd.com/projects/rocBLAS/en/latest/index.html" title="(in rocBLAS Documentation v4.4.1)"><span class="xref std std-doc">rocBLAS</span></a> and <a class="reference external" href="https://rocm.docs.amd.com/projects/hipBLASLt/en/latest/index.html" title="(in hipBLASLt Documentation v0.12.1)"><span class="xref std std-doc">hipBLASLt</span></a> libraries during runtime.</p>
<p>During warm-up runs or offline profiling steps, users can create a GEMM Table
that enumerates the kernel information. During the model’s run, the best-performing kernel substitutes
<code class="docutils literal notranslate"><span class="pre">torch.nn.functional.linear(input,</span> <span class="pre">weight,</span> <span class="pre">bias=None)</span></code> with the kernel specified in the GEMM table. The
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/cuda/tunable/README.md">Tunable GitHub</a>
page describes the options.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># To turn on TunableOp, simply set this environment variable</span>
<span class="n">export</span> <span class="n">PYTORCH_TUNABLEOP_ENABLED</span><span class="o">=</span><span class="mi">1</span>

<span class="c1"># Python</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">Out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Out</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>

<span class="c1"># tunableop_results0.csv</span>
<span class="n">Validator</span><span class="p">,</span><span class="n">PT_VERSION</span><span class="p">,</span><span class="mf">2.4.0</span>
<span class="n">Validator</span><span class="p">,</span><span class="n">ROCM_VERSION</span><span class="p">,</span><span class="mf">6.1.0.0</span><span class="o">-</span><span class="mi">82</span><span class="o">-</span><span class="mi">5</span><span class="n">fabb4c</span>
<span class="n">Validator</span><span class="p">,</span><span class="n">HIPBLASLT_VERSION</span><span class="p">,</span><span class="mf">0.7.0</span><span class="o">-</span><span class="mi">1549</span><span class="n">b021</span>
<span class="n">Validator</span><span class="p">,</span><span class="n">GCN_ARCH_NAME</span><span class="p">,</span><span class="n">gfx942</span><span class="p">:</span><span class="n">sramecc</span><span class="o">+</span><span class="p">:</span><span class="n">xnack</span><span class="o">-</span>
<span class="n">Validator</span><span class="p">,</span><span class="n">ROCBLAS_VERSION</span><span class="p">,</span><span class="mf">4.1.0</span><span class="o">-</span><span class="n">cefa4a9b</span><span class="o">-</span><span class="n">dirty</span>
<span class="n">GemmTunableOp_float_TN</span><span class="p">,</span><span class="n">tn_200_100_20</span><span class="p">,</span><span class="n">Gemm_Rocblas_32323</span><span class="p">,</span><span class="mf">0.00669595</span>
</pre></div>
</div>
<img alt="GEMM and TunableOp" class="align-center" src="_images/tunableop.png"/>
<p>Learn more about optimizing kernels with TunableOp in
<a class="reference internal" href="#mi300x-tunableop"><span class="std std-ref">Optimizing Triton kernels</span></a>.</p>
</section>
</section>
<section id="fbgemm-and-fbgemm-gpu">
<h5>FBGEMM and FBGEMM_GPU<a class="headerlink" href="#fbgemm-and-fbgemm-gpu" title="Link to this heading">#</a></h5>
<p>FBGEMM (Facebook General Matrix Multiplication) is a low-precision, high-performance CPU kernel library
for matrix-matrix multiplications and convolutions. It is used for server-side inference
and as a back end for PyTorch quantized operators. FBGEMM offers optimized on-CPU performance for reduced precision calculations,
strong performance on native tensor formats, and the ability to generate
high-performance shape- and size-specific kernels at runtime.</p>
<p>FBGEMM_GPU collects several high-performance PyTorch GPU operator libraries
for use in training and inference. It provides efficient table-batched embedding functionality,
data layout transformation, and quantization support.</p>
<p>For more information about FBGEMM and FBGEMM_GPU, see the <a class="reference external" href="https://github.com/pytorch/FBGEMM">PyTorch FBGEMM GitHub</a>
and the <a class="reference external" href="https://pytorch.org/FBGEMM/">PyTorch FBGEMM documentation</a>.
The <a class="reference external" href="https://engineering.fb.com/2018/11/07/ml-applications/fbgemm/">Meta blog post about FBGEMM</a> provides
additional background about the library.</p>
<section id="installing-fbgemm-gpu">
<h6>Installing FBGEMM_GPU<a class="headerlink" href="#installing-fbgemm-gpu" title="Link to this heading">#</a></h6>
<p>Installing FBGEMM_GPU consists of the following steps:</p>
<ul class="simple">
<li><p>Set up an isolated Miniconda environment</p></li>
<li><p>Install ROCm using Docker or the <a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/install-methods/package-manager-index.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">package manager</span></a></p></li>
<li><p>Install the nightly <a class="reference external" href="https://pytorch.org/">PyTorch</a> build</p></li>
<li><p>Complete the pre-build and build tasks</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>FBGEMM_GPU doesn’t require the installation of FBGEMM. To optionally install
FBGEMM, see the <a class="reference external" href="https://pytorch.org/FBGEMM/fbgemm/development/BuildInstructions.html">FBGEMM install instructions</a>.</p>
</div>
<section id="set-up-the-miniconda-environment">
<h6 aria-level="7">Set up the Miniconda environment<a class="headerlink" href="#set-up-the-miniconda-environment" title="Link to this heading">#</a></h6>
<p>To install Miniconda, use the following commands.</p>
<ol class="arabic">
<li><p>Install a <a class="reference external" href="https://docs.anaconda.com/miniconda/">Miniconda environment</a> for reproducible builds.
All subsequent commands run inside this environment.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">PLATFORM_NAME</span><span class="o">=</span><span class="s2">"</span><span class="k">$(</span>uname<span class="w"> </span>-s<span class="k">)</span><span class="s2">-</span><span class="k">$(</span>uname<span class="w"> </span>-m<span class="k">)</span><span class="s2">"</span>

<span class="c1"># Set the Miniconda prefix directory</span>
<span class="nv">miniconda_prefix</span><span class="o">=</span><span class="nv">$HOME</span>/miniconda

<span class="c1"># Download the Miniconda installer</span>
wget<span class="w"> </span>-q<span class="w"> </span><span class="s2">"https://repo.anaconda.com/miniconda/Miniconda3-latest-</span><span class="si">${</span><span class="nv">PLATFORM_NAME</span><span class="si">}</span><span class="s2">.sh"</span><span class="w"> </span>-O<span class="w"> </span>miniconda.sh

<span class="c1"># Run the installer</span>
bash<span class="w"> </span>miniconda.sh<span class="w"> </span>-b<span class="w"> </span>-p<span class="w"> </span><span class="s2">"</span><span class="nv">$miniconda_prefix</span><span class="s2">"</span><span class="w"> </span>-u

<span class="c1"># Load the shortcuts</span>
.<span class="w"> </span>~/.bashrc

<span class="c1"># Run updates</span>
conda<span class="w"> </span>update<span class="w"> </span>-n<span class="w"> </span>base<span class="w"> </span>-c<span class="w"> </span>defaults<span class="w"> </span>-y<span class="w"> </span>conda
</pre></div>
</div>
</li>
<li><p>Create a Miniconda environment with Python 3.12:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">env_name</span><span class="o">=</span>&lt;ENV<span class="w"> </span>NAME&gt;
<span class="nv">python_version</span><span class="o">=</span><span class="m">3</span>.12

<span class="c1"># Create the environment</span>
conda<span class="w"> </span>create<span class="w"> </span>-y<span class="w"> </span>--name<span class="w"> </span><span class="si">${</span><span class="nv">env_name</span><span class="si">}</span><span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="s2">"</span><span class="si">${</span><span class="nv">python_version</span><span class="si">}</span><span class="s2">"</span>

<span class="c1"># Upgrade PIP and pyOpenSSL package</span>
conda<span class="w"> </span>run<span class="w"> </span>-n<span class="w"> </span><span class="si">${</span><span class="nv">env_name</span><span class="si">}</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>pip
conda<span class="w"> </span>run<span class="w"> </span>-n<span class="w"> </span><span class="si">${</span><span class="nv">env_name</span><span class="si">}</span><span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>pyOpenSSL&gt;22.1.0
</pre></div>
</div>
</li>
<li><p>Install additional build tools:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>-n<span class="w"> </span><span class="si">${</span><span class="nv">env_name</span><span class="si">}</span><span class="w"> </span>-y<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>click<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>cmake<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>hypothesis<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>jinja2<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>make<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>ncurses<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>ninja<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>numpy<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>scikit-build<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>wheel
</pre></div>
</div>
</li>
</ol>
</section>
<section id="install-the-rocm-components">
<h6 aria-level="7">Install the ROCm components<a class="headerlink" href="#install-the-rocm-components" title="Link to this heading">#</a></h6>
<p>FBGEMM_GPU can run in a ROCm Docker container or in conjunction with the full ROCm installation.
The Docker method is recommended because it requires fewer steps and provides a stable environment.</p>
<p>To run FBGEMM_GPU in the Docker container, pull the <a class="reference external" href="https://hub.docker.com/r/rocm/rocm-terminal">Minimal Docker image for ROCm</a>.
This image includes all preinstalled ROCm packages required to integrate FBGEMM. To pull
and run the ROCm Docker image, use this command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run for ROCm 6.2.0</span>
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>--network<span class="o">=</span>host<span class="w"> </span>--shm-size<span class="w"> </span>16G<span class="w"> </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span>--ipc<span class="o">=</span>host<span class="w"> </span>rocm/rocm-terminal:6.2<span class="w"> </span>/bin/bash
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <a class="reference external" href="https://hub.docker.com/r/rocm/dev-ubuntu-20.04">Full Docker image for ROCm</a>, which includes all
ROCm packages, can also be used. However, it results in a very large container, so the minimal
Docker image is recommended.</p>
</div>
<p>You can also install ROCm using the package manager. FBGEMM_GPU requires the installation of the full ROCm package.
For more information, see <a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/detailed-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">the ROCm installation guide</span></a>.
The ROCm package also requires the <a class="reference external" href="https://rocm.docs.amd.com/projects/MIOpen/en/latest/index.html" title="(in MIOpen Documentation v3.4.0)"><span class="xref std std-doc">MIOpen</span></a> component as a dependency.
To install MIOpen, use the <code class="docutils literal notranslate"><span class="pre">apt</span> <span class="pre">install</span></code> command.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>apt<span class="w"> </span>install<span class="w"> </span>hipify-clang<span class="w"> </span>miopen-hip<span class="w"> </span>miopen-hip-dev
</pre></div>
</div>
</section>
<section id="install-pytorch">
<h6 aria-level="7">Install PyTorch<a class="headerlink" href="#install-pytorch" title="Link to this heading">#</a></h6>
<p>Install <a class="reference external" href="https://pytorch.org/">PyTorch</a> using <code class="docutils literal notranslate"><span class="pre">pip</span></code> for the most reliable and consistent results.</p>
<ol class="arabic">
<li><p>Install the nightly PyTorch build using <code class="docutils literal notranslate"><span class="pre">pip</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install the latest nightly, ROCm variant</span>
conda<span class="w"> </span>run<span class="w"> </span>-n<span class="w"> </span><span class="si">${</span><span class="nv">env_name</span><span class="si">}</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--pre<span class="w"> </span>torch<span class="w"> </span>--index-url<span class="w"> </span>https://download.pytorch.org/whl/nightly/rocm6.2/
</pre></div>
</div>
</li>
<li><p>Ensure PyTorch loads correctly. Verify the version and variant of the installation using an <code class="docutils literal notranslate"><span class="pre">import</span></code> test.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Ensure that the package loads properly</span>
conda<span class="w"> </span>run<span class="w"> </span>-n<span class="w"> </span><span class="si">${</span><span class="nv">env_name</span><span class="si">}</span><span class="w"> </span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">"import torch.distributed"</span>

<span class="c1"># Verify the version and variant of the installation</span>
conda<span class="w"> </span>run<span class="w"> </span>-n<span class="w"> </span><span class="si">${</span><span class="nv">env_name</span><span class="si">}</span><span class="w"> </span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">"import torch; print(torch.__version__)"</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="perform-the-prebuild-and-build">
<h6 aria-level="7">Perform the prebuild and build<a class="headerlink" href="#perform-the-prebuild-and-build" title="Link to this heading">#</a></h6>
<ol class="arabic">
<li><p>Clone the FBGEMM repository and the relevant submodules. Use <code class="docutils literal notranslate"><span class="pre">pip</span></code> to install the
components in <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code>. Run the following commands inside the Miniconda environment.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Select a version tag</span>
<span class="nv">FBGEMM_VERSION</span><span class="o">=</span>v0.8.0

<span class="c1"># Clone the repo along with its submodules</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/pytorch/FBGEMM.git<span class="w"> </span>--branch<span class="o">=</span>v0.8.0<span class="w"> </span>--recursive<span class="w"> </span>fbgemm_<span class="si">${</span><span class="nv">FBGEMM_VERSION</span><span class="si">}</span>

<span class="c1"># Install additional required packages for building and testing</span>
<span class="nb">cd</span><span class="w"> </span>fbgemm_<span class="si">${</span><span class="nv">FBGEMM_VERSION</span><span class="si">}</span>/fbgemm_gpu
pip<span class="w"> </span>install<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Clear the build cache to remove stale build information.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># !! Run in fbgemm_gpu/ directory inside the Conda environment !!</span>

python<span class="w"> </span>setup.py<span class="w"> </span>clean
</pre></div>
</div>
</li>
<li><p>Set the wheel build variables, including the package name, Python version tag, and Python platform name.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the package name depending on the build variant</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">package_name</span><span class="o">=</span>fbgemm_gpu_rocm

<span class="c1"># Set the Python version tag.  It should follow the convention `py&lt;major&gt;&lt;minor&gt;`,</span>
<span class="c1"># for example, Python 3.12 --&gt; py312</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">python_tag</span><span class="o">=</span>py312

<span class="c1"># Determine the processor architecture</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">ARCH</span><span class="o">=</span><span class="k">$(</span>uname<span class="w"> </span>-m<span class="k">)</span>

<span class="c1"># Set the Python platform name for the Linux case</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">python_plat_name</span><span class="o">=</span><span class="s2">"manylinux2014_</span><span class="si">${</span><span class="nv">ARCH</span><span class="si">}</span><span class="s2">"</span>
</pre></div>
</div>
</li>
<li><p>Build FBGEMM_GPU for the ROCm platform. Set <code class="docutils literal notranslate"><span class="pre">ROCM_PATH</span></code> to the path to your ROCm installation.
Run these commands from the <code class="docutils literal notranslate"><span class="pre">fbgemm_gpu/</span></code> directory inside the Miniconda environment.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># !! Run in the fbgemm_gpu/ directory inside the Conda environment !!</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">ROCM_PATH</span><span class="o">=</span>&lt;/path/to/rocm&gt;

<span class="c1"># Build for the target architecture of the ROCm device installed on the machine (for example, 'gfx942;gfx90a')</span>
<span class="c1"># See :doc:`The Linux system requirements &lt;../../reference/system-requirements&gt;` for a list of supported GPUs.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PYTORCH_ROCM_ARCH</span><span class="o">=</span><span class="k">$(</span><span class="si">${</span><span class="nv">ROCM_PATH</span><span class="si">}</span>/bin/rocminfo<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>-o<span class="w"> </span>-m<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="s1">'gfx.*'</span><span class="k">)</span>

<span class="c1"># Build the wheel artifact only</span>
python<span class="w"> </span>setup.py<span class="w"> </span>bdist_wheel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--package_variant<span class="o">=</span>rocm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--python-tag<span class="o">=</span><span class="s2">"</span><span class="si">${</span><span class="nv">python_tag</span><span class="si">}</span><span class="s2">"</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--plat-name<span class="o">=</span><span class="s2">"</span><span class="si">${</span><span class="nv">python_plat_name</span><span class="si">}</span><span class="s2">"</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>-DHIP_ROOT_DIR<span class="o">=</span><span class="s2">"</span><span class="si">${</span><span class="nv">ROCM_PATH</span><span class="si">}</span><span class="s2">"</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>-DCMAKE_C_FLAGS<span class="o">=</span><span class="s2">"-DTORCH_USE_HIP_DSA"</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>-DCMAKE_CXX_FLAGS<span class="o">=</span><span class="s2">"-DTORCH_USE_HIP_DSA"</span>

<span class="c1"># Build and install the library into the Conda environment</span>
python<span class="w"> </span>setup.py<span class="w"> </span>install<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--package_variant<span class="o">=</span>rocm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>-DHIP_ROOT_DIR<span class="o">=</span><span class="s2">"</span><span class="si">${</span><span class="nv">ROCM_PATH</span><span class="si">}</span><span class="s2">"</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>-DCMAKE_C_FLAGS<span class="o">=</span><span class="s2">"-DTORCH_USE_HIP_DSA"</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>-DCMAKE_CXX_FLAGS<span class="o">=</span><span class="s2">"-DTORCH_USE_HIP_DSA"</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>
<section id="post-build-validation">
<h6>Post-build validation<a class="headerlink" href="#post-build-validation" title="Link to this heading">#</a></h6>
<p>After building FBGEMM_GPU, run some verification checks to ensure the build is correct. Continue
to run all commands inside the <code class="docutils literal notranslate"><span class="pre">fbgemm_gpu/</span></code> directory inside the Miniconda environment.</p>
<ol class="arabic">
<li><p>The build process generates many build artifacts and C++ templates, so
it is important to confirm no undefined symbols remain.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># !! Run in fbgemm_gpu/ directory inside the Conda environment !!</span>

<span class="c1"># Locate the built .SO file</span>
<span class="nv">fbgemm_gpu_lib_path</span><span class="o">=</span><span class="k">$(</span>find<span class="w"> </span>.<span class="w"> </span>-name<span class="w"> </span>fbgemm_gpu_py.so<span class="k">)</span>

<span class="c1"># Check that the undefined symbols don't include fbgemm_gpu-defined functions</span>
nm<span class="w"> </span>-gDCu<span class="w"> </span><span class="s2">"</span><span class="si">${</span><span class="nv">fbgemm_gpu_lib_path</span><span class="si">}</span><span class="s2">"</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>sort
</pre></div>
</div>
</li>
<li><p>Verify the referenced version number of <code class="docutils literal notranslate"><span class="pre">GLIBCXX</span></code> and the presence of certain function symbols:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># !! Run in fbgemm_gpu/ directory inside the Conda environment !!</span>

<span class="c1"># Locate the built .SO file</span>
<span class="nv">fbgemm_gpu_lib_path</span><span class="o">=</span><span class="k">$(</span>find<span class="w"> </span>.<span class="w"> </span>-name<span class="w"> </span>fbgemm_gpu_py.so<span class="k">)</span>

<span class="c1"># Note the versions of GLIBCXX referenced by the .SO</span>
<span class="c1"># The libstdc++.so.6 available on the install target must support these versions</span>
objdump<span class="w"> </span>-TC<span class="w"> </span><span class="s2">"</span><span class="si">${</span><span class="nv">fbgemm_gpu_lib_path</span><span class="si">}</span><span class="s2">"</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>GLIBCXX<span class="w"> </span><span class="p">|</span><span class="w"> </span>sed<span class="w"> </span><span class="s1">'s/.*GLIBCXX_\([.0-9]*\).*/GLIBCXX_\1/g'</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>sort<span class="w"> </span>-Vu<span class="w"> </span><span class="p">|</span><span class="w"> </span>cat

<span class="c1"># Test for the existence of a given function symbol in the .SO</span>
nm<span class="w"> </span>-gDC<span class="w"> </span><span class="s2">"</span><span class="si">${</span><span class="nv">fbgemm_gpu_lib_path</span><span class="si">}</span><span class="s2">"</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span><span class="s2">" fbgemm_gpu::merge_pooled_embeddings("</span>
nm<span class="w"> </span>-gDC<span class="w"> </span><span class="s2">"</span><span class="si">${</span><span class="nv">fbgemm_gpu_lib_path</span><span class="si">}</span><span class="s2">"</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span><span class="s2">" fbgemm_gpu::jagged_2d_to_dense("</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="testing-fbgemm">
<h6>Testing FBGEMM<a class="headerlink" href="#testing-fbgemm" title="Link to this heading">#</a></h6>
<p>FBGEMM includes tests and benchmarks to validate performance. To run these tests,
you must use ROCm 5.7 or a more recent version on the host and container. To run FBGEMM tests,
follow these instructions:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># !! Run inside the Conda environment !!</span>

<span class="c1"># From the /fbgemm_gpu/ directory</span>
<span class="nb">cd</span><span class="w"> </span><span class="nb">test</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">FBGEMM_TEST_WITH_ROCM</span><span class="o">=</span><span class="m">1</span>
<span class="c1"># Enable for debugging failed kernel executions</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">HIP_LAUNCH_BLOCKING</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># Run the test</span>
python<span class="w"> </span>-m<span class="w"> </span>pytest<span class="w"> </span>-v<span class="w"> </span>-rsx<span class="w"> </span>-s<span class="w"> </span>-W<span class="w"> </span>ignore::pytest.PytestCollectionWarning<span class="w"> </span>split_table_batched_embeddings_test.py
</pre></div>
</div>
<p>To run the FBGEMM_GPU <code class="docutils literal notranslate"><span class="pre">uvm</span></code> test, use these commands. These tests only support the AMD MI210 and
more recent accelerators.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run this inside the Conda environment from the /fbgemm_gpu/ directory</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">HSA_XNACK</span><span class="o">=</span><span class="m">1</span>
<span class="nb">cd</span><span class="w"> </span><span class="nb">test</span>

python<span class="w"> </span>-m<span class="w"> </span>pytest<span class="w"> </span>-v<span class="w"> </span>-rsx<span class="w"> </span>-s<span class="w"> </span>-W<span class="w"> </span>ignore::pytest.PytestCollectionWarning<span class="w"> </span>./uvm/uvm_test.py
</pre></div>
</div>
</section>
</section>
</section>
<span id="document-how-to/rocm-for-ai/inference-optimization/optimizing-with-composable-kernel"></span><section class="tex2jax_ignore mathjax_ignore" id="optimizing-with-composable-kernel">
<h4>Optimizing with Composable Kernel<a class="headerlink" href="#optimizing-with-composable-kernel" title="Link to this heading">#</a></h4>
<p>The AMD ROCm Composable Kernel (CK) library provides a programming model for writing performance-critical kernels for machine learning workloads. It generates a general-purpose kernel during the compilation phase through a C++ template, enabling developers to achieve operation fusions on different data precisions.</p>
<p>This article gives a high-level overview of CK General Matrix Multiplication (GEMM) kernel based on the design example of <code class="docutils literal notranslate"><span class="pre">03_gemm_bias_relu</span></code>. It also outlines the steps to construct the kernel and run it. Moreover, the article provides a detailed implementation of running SmoothQuant quantized INT8 models on AMD Instinct MI300X accelerators using CK.</p>
<section id="high-level-overview-a-ck-gemm-instance">
<h5>High-level overview: a CK GEMM instance<a class="headerlink" href="#high-level-overview-a-ck-gemm-instance" title="Link to this heading">#</a></h5>
<p>GEMM is a fundamental block in linear algebra, machine learning, and deep neural networks. It is defined as the operation:
<span class="math notranslate nohighlight">\(E = α \times (A \times B) + β \times (D)\)</span>, with A and B as matrix inputs, α and β as scalar inputs, and D as a pre-existing matrix.
Take the commonly used linear transformation in a fully connected layer as an example. These terms correspond to input activation (A), weight (B), bias (D), and output (E), respectively. The example employs a <code class="docutils literal notranslate"><span class="pre">DeviceGemmMultipleD_Xdl_CShuffle</span></code> struct from CK library as the fundamental instance to explore the compute capability of AMD Instinct accelerators for the computation of GEMM. The implementation of the instance contains two phases:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#template-parameter-definition">Template parameter definition</a></p></li>
<li><p><a class="reference internal" href="#instantiating-and-running-the-templated-kernel">Instantiating and running the templated kernel</a></p></li>
</ul>
<section id="template-parameter-definition">
<h6>Template parameter definition<a class="headerlink" href="#template-parameter-definition" title="Link to this heading">#</a></h6>
<p>The template parameters of the instance are grouped into four parameter types:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#matrix-data-precision"><span class="std std-ref">Parameters for determining matrix data precision</span></a></p></li>
<li><p><a class="reference internal" href="#matrix-data-layout"><span class="std std-ref">Parameters for determining matrix data layout</span></a></p></li>
<li><p><a class="reference internal" href="#matrix-element-operation"><span class="std std-ref">Parameters for determining extra operations on matrix elements</span></a></p></li>
<li><p><a class="reference internal" href="#tunable-parameters"><span class="std std-ref">Performance-oriented tunable parameters</span></a></p></li>
</ul>
<!-- 
================
 ### Figure 2
================ -->
<figure class="align-default" id="id5">
<img alt="_images/ck-template_parameters.jpg" src="_images/ck-template_parameters.jpg"/>
<figcaption>
<p><span class="caption-text">The template parameters of the selected GEMM kernel are classified into four groups. These template parameter groups should be defined properly before running the instance.</span><a class="headerlink" href="#id5" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="matrix-data-precision">
<span id="id1"></span><h6 aria-level="7">Matrix data precision<a class="headerlink" href="#matrix-data-precision" title="Link to this heading">#</a></h6>
<p>A, B, D, and E are defined as half-precision floating-point datatypes. The multiply-add results of matrix A and B are added with a pre-existing matrix D (half-precision), and the final GEMM results are also half-precision floating-points.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">ADataType</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="n">F16</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">BDataType</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="n">F16</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">AccDataType</span><span class="w">      </span><span class="o">=</span><span class="w"> </span><span class="n">F32</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">CShuffleDataType</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">F16</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">DDataType</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="n">F16</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">EDataType</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="n">F16</span><span class="p">;</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">ADataType</span></code> and <code class="docutils literal notranslate"><span class="pre">BDataType</span></code> denote the data precision of the A and B input matrices. <code class="docutils literal notranslate"><span class="pre">AccDataType</span></code> determines the data precision used for representing the multiply-add results of A and B elements. These results are stored in a <code class="docutils literal notranslate"><span class="pre">CShuffle</span></code> module in local data share (LDS), a low-latency and high-bandwidth explicitly-addressed memory used for synchronization within a workgroup LDS for later use.</p>
<p><code class="docutils literal notranslate"><span class="pre">CShuffleDataType</span></code> denotes the data precision of <code class="docutils literal notranslate"><span class="pre">CShuffle</span></code> in LDS.</p>
<p><code class="docutils literal notranslate"><span class="pre">DDataType</span></code> denotes the data precision of the pre-existing D matrix stored in GPU global memory, while <code class="docutils literal notranslate"><span class="pre">EDatatype</span></code> denotes the data precision of the final output. The CK kernel supports a fusion strategy so that <code class="docutils literal notranslate"><span class="pre">CShuffle</span></code> can be added with a single pre-existing matrix in the same GPU kernel for better performance.</p>
</section>
<section id="matrix-data-layout">
<span id="id2"></span><h6 aria-level="7">Matrix data layout<a class="headerlink" href="#matrix-data-layout" title="Link to this heading">#</a></h6>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">ALayout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Row</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">BLayout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Col</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">DLayout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Row</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">ELayout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Row</span><span class="p">;</span>
</pre></div>
</div>
<p>Following the convention of various linear algebra libraries, CK assumes that the input matrix A is an M x K matrix, meaning the matrix has M rows and K columns. Similarly, matrix B is assumed to be K x N, meaning it has K rows and N columns. In computing, row-major order and column-major order are commonly used ways to store matrices in linear storage. After understanding the matrix storage pattern, the underlying optimized memory access manner can be applied to achieve better performance depending on the storage ordering of these matrices.</p>
</section>
<section id="matrix-element-operation">
<span id="id3"></span><h6 aria-level="7">Matrix element operation<a class="headerlink" href="#matrix-element-operation" title="Link to this heading">#</a></h6>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">AElementOp</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="n">PassThrough</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">BElementOp</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="n">PassThrough</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">CDEElementOp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">AddRelu</span><span class="p">;</span>
</pre></div>
</div>
<p>CK supports the pre-processing of the matrix before calculating GEMM, that is, <code class="docutils literal notranslate"><span class="pre">C</span> <span class="pre">=</span> <span class="pre">AElementOp(A)</span> <span class="pre">*</span> <span class="pre">BElementOp(B)</span></code>. It similarly supports the post-processing of GEMM results the same way, that is, <code class="docutils literal notranslate"><span class="pre">E</span> <span class="pre">=</span> <span class="pre">CDEElementOp(C,</span> <span class="pre">D)</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">AElementOp</span></code> and <code class="docutils literal notranslate"><span class="pre">BElementOp</span></code> determine the operation applied to matrix A and B separately before GEMM, which is achieved by binding the operation with a C++ struct function.</p>
<p>The above <code class="docutils literal notranslate"><span class="pre">PassThrough</span></code> denotes no operations are performed on the target matrix. <code class="docutils literal notranslate"><span class="pre">CDEELementOp</span></code> determines the operations applied to <code class="docutils literal notranslate"><span class="pre">CShuffle</span></code> output and matrix D. The following binding struct <code class="docutils literal notranslate"><span class="pre">AddRelu</span></code> shows an example of adding the <code class="docutils literal notranslate"><span class="pre">CShuffle</span></code> output and matrix D, and ReLU (Rectified Linear Unit) operations to the addition result. It then passes the results to matrix E.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">AddRelu</span>
<span class="p">{</span>
<span class="w">    </span><span class="n">__host__</span><span class="w"> </span><span class="n">__device__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="k">operator</span><span class="p">()(</span><span class="n">ck</span><span class="o">::</span><span class="n">half_t</span><span class="o">&amp;</span><span class="w"> </span><span class="n">e</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">half_t</span><span class="o">&amp;</span><span class="w"> </span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">half_t</span><span class="o">&amp;</span><span class="w"> </span><span class="n">d</span><span class="p">)</span><span class="w"> </span><span class="k">const</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">half_t</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">d</span><span class="p">;</span>
<span class="w">        </span><span class="n">e</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</pre></div>
</div>
</section>
<section id="tunable-parameters">
<span id="id4"></span><h6 aria-level="7">Tunable parameters<a class="headerlink" href="#tunable-parameters" title="Link to this heading">#</a></h6>
<p>The CK instance includes a series of tunable template parameters to control the parallel granularity of the workload to achieve load balancing on different hardware platforms.</p>
<p>These parameters include Block Size, M/N/K Per Block, M/N per XDL, AK1, BK1, etc.</p>
<ul class="simple">
<li><p>Block Size determines the number of threads in the thread block.</p></li>
<li><p>M/N/K Per Block determines the size of tile that each thread block is responsible for calculating.</p></li>
<li><p>M/N Per XDL refers to M/N size for Instinct accelerator Matrix Fused Multiply Add (MFMA) instructions operating on a per-wavefront basis.</p></li>
<li><p>A/B K1 is related to the data type. It can be any value ranging from 1 to K Per Block. To achieve the optimal load/store performance, 128bit per load is suggested. In addition, the A/B loading parameters must be changed accordingly to match the A/B K1 value; otherwise, it will result in compilation errors.</p></li>
</ul>
<p>Conditions for achieving computational load balancing on different hardware platforms can vary.</p>
</section>
</section>
<section id="instantiating-and-running-the-templated-kernel">
<h6>Instantiating and running the templated kernel<a class="headerlink" href="#instantiating-and-running-the-templated-kernel" title="Link to this heading">#</a></h6>
<p>After determining the template parameters, we instantiate the kernel with actual arguments. Do one of the following:</p>
<ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">GetDeviceBuffer</span></code> from CK’s custom struct <code class="docutils literal notranslate"><span class="pre">DeviceMem</span></code> to pass the element values of the matrices that need to be calculated.</p></li>
<li><p>Allocate device buffer via <code class="docutils literal notranslate"><span class="pre">hipMalloc</span></code>. Ensure the device buffer size can fit the matrix size.</p></li>
<li><p>Pass matrix elements through the <code class="docutils literal notranslate"><span class="pre">data_ptr</span></code> method in the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> object if the matrix to be calculated is of <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> type.</p></li>
</ul>
<p>The row and column, and stride information of input matrices are also passed to the instance. For batched GEMM, you must pass in additional batch count and batch stride values. The extra operations for pre and post-processing are also passed with an actual argument; for example, α and β for GEMM scaling operations. Afterward, the instantiated kernel is launched by the invoker, as illustrated in Figure 3.</p>
<!-- 
================
 ### Figure 3
================ -->
<figure class="align-default" id="id6">
<img alt="_images/ck-kernel_launch.jpg" src="_images/ck-kernel_launch.jpg"/>
<figcaption>
<p><span class="caption-text">Templated kernel launching consists of kernel instantiation, making arguments by passing in actual application parameters, creating an invoker, and running the instance through the invoker.</span><a class="headerlink" href="#id6" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="developing-fused-int8-kernels-for-smoothquant-models">
<h5>Developing fused INT8 kernels for SmoothQuant models<a class="headerlink" href="#developing-fused-int8-kernels-for-smoothquant-models" title="Link to this heading">#</a></h5>
<p><a class="reference external" href="https://github.com/mit-han-lab/smoothquant">SmoothQuant</a> (SQ) is a quantization algorithm that enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLM. The required GPU kernel functionalities used to accelerate the inference of SQ models on Instinct accelerators are shown in the following table.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id7">
<caption><span class="caption-text">Functionalities used to implement SmoothQuant model inference.</span><a class="headerlink" href="#id7" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Functionality descriptions</p></th>
<th class="head"><p>Corresponding wrappers</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><span class="math notranslate nohighlight">\(E = α \times (A \times B) + β \times (D)\)</span>, where A, B, D, E are INT8 2-D tensors;</p></td>
<td><p>E = Linear_ABDE_I8(A, B, D, <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span>)</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><span class="math notranslate nohighlight">\(E = RELU (α \times (A \times B) + β \times (D))\)</span>, where A, B, D, E are INT8 2-D tensors;</p></td>
<td><p>E = Linear_ReLU_ABDE_I8(A, B, D, <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span>)</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><span class="math notranslate nohighlight">\(E = α \times (A \times B) + β \times (D)\)</span>, where A, B are INT8 2-D tensors, D and E are FP32 2-D tensors;</p></td>
<td><p>E = Linear_AB_I8_DE_F32(A, B, D, <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span>)</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><span class="math notranslate nohighlight">\(E = α \times (A \times B)\)</span>, where A, B, E are INT8 3-D tensors;</p></td>
<td><p>E = BMM_ABE_I8(A, B, <span class="math notranslate nohighlight">\(\alpha\)</span>)</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><span class="math notranslate nohighlight">\(E = α \times (A \times B)\)</span>, where A, B are INT8 3-D tensors, E is FP32 3-D tensor;</p></td>
<td><p>E = BMM_AB_I8_E_F32(A, B, <span class="math notranslate nohighlight">\(\alpha\)</span>)</p></td>
</tr>
</tbody>
</table>
</div>
<section id="operation-flow-analysis">
<h6>Operation flow analysis<a class="headerlink" href="#operation-flow-analysis" title="Link to this heading">#</a></h6>
<p>The following section discusses the analysis of the operation flow of <code class="docutils literal notranslate"><span class="pre">Linear_ReLU_ABDE_I8</span></code>. The rest of the wrappers in Table 1 can be analyzed similarly.</p>
<p>The first operation in the process is to perform the multiplication of input matrices A and B. The resulting matrix C is then scaled with α to obtain T1. At the same time, the process performs a scaling operation on D elements to obtain T2. Afterward, the process performs matrix addition between T1 and T2, element activation calculation using ReLU, and element rounding sequentially. The operations to generate E1, E2, and E are encapsulated and completed by a user-defined template function in CK (given in the next sub-section). This template function is integrated into the fundamental instance directly during the compilation phase so that all these steps can be fused in a single GPU kernel.</p>
<!-- 
================
 ### Figure 4
================ -->
<figure class="align-default" id="id8">
<img alt="_images/ck-operation_flow.jpg" src="_images/ck-operation_flow.jpg"/>
<figcaption>
<p><span class="caption-text">Operation flow.</span><a class="headerlink" href="#id8" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The CK library contains many fundamental instances that implement different functions. Familiarize yourself with the names of various CK instances and determine whether they meet the target functional requirements.</p>
<p>Second, consider whether the format of input data meets your actual calculation needs. For SQ models, the 8-bit integer data format (INT8) is applied for matrix calculations.</p>
<p>Third, consider the platform for implementing CK instances. The instances suffixed with <code class="docutils literal notranslate"><span class="pre">xdl</span></code> only run on AMD Instinct accelerators after being compiled and cannot run on Radeon-series GPUs. This is due to the underlying device-specific instruction sets for implementing these basic instances.</p>
<p>Here, we use <a class="reference external" href="https://github.com/ROCm/composable_kernel/tree/develop/example/24_batched_gemm">DeviceBatchedGemmMultiD_Xdl</a> as the fundamental instance to implement the functionalities in the previous table.</p>
<!-- 
================
 ### Figure 5
================ -->
<figure class="align-default" id="id9">
<img alt="_images/ck-root_instance.jpg" src="_images/ck-root_instance.jpg"/>
<figcaption>
<p><span class="caption-text">Use the ‘DeviceBatchedGemmMultiD_Xdl’ instance as a root.</span><a class="headerlink" href="#id9" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The <code class="docutils literal notranslate"><span class="pre">DeviceBatchedGemmMultiD_Xdl</span></code> instance realizes the batched GEMM <code class="docutils literal notranslate"><span class="pre">BMM_ABE_I8</span></code> and <code class="docutils literal notranslate"><span class="pre">BMM_AB_I8_E_F32</span></code> kernels directly by using the proper input and output data precision types.</p>
<p>Based on the two batched GEMM kernels, GEMM kernel <code class="docutils literal notranslate"><span class="pre">Linear_ABDE_I8</span></code> and <code class="docutils literal notranslate"><span class="pre">Linear_AB_I8_DE_F32</span></code> can be implemented by expanding their input 2-D tensors to 3-D tensors. Then, the 3-D output tensors produced by the root instance are squeezed back to 2-D output tensors before returning back.</p>
<p>For example, unsqueeze A (M, K) to A (1, M, K) before assigning it into the root instance and squeeze E (1, M, N) to (M, N) after the calculations of the root instance return back. <code class="docutils literal notranslate"><span class="pre">Linear_ReLU_ABDE_I8</span></code> is implemented by adding a ReLU operation on the result output of <code class="docutils literal notranslate"><span class="pre">Linear_ABDE_I8</span></code>.</p>
</section>
<section id="developing-the-complete-function">
<h6>Developing the complete function<a class="headerlink" href="#developing-the-complete-function" title="Link to this heading">#</a></h6>
<p>The inference of SQ quantized models relies on using PyTorch and Transformer libraries, and a tensor type is used to represent matrices and vectors in <code class="docutils literal notranslate"><span class="pre">torch</span></code>, the C++ data types in CK need to be replaced with the <code class="docutils literal notranslate"><span class="pre">torch::tensor</span></code> type. The data types of the input and output matrices should be a <code class="docutils literal notranslate"><span class="pre">tensor</span></code> type.</p>
<p>In GEMM, the A and B inputs are two-dimensional matrices, and the required input matrices of the selected fundamental CK instance are three-dimensional matrices. Therefore, we must convert the input 2-D tensors to 3-D tensors, by using <code class="docutils literal notranslate"><span class="pre">tensor</span></code>’s <code class="docutils literal notranslate"><span class="pre">unsqueeze()</span></code> method before passing these matrices to the instance. For batched GEMM in the preceding table, ignore this step.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Function input and output </span>
<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">linear_relu_abde_i8</span><span class="p">(</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">A_</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">B_</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">D_</span><span class="p">,</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">alpha</span><span class="p">,</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">beta</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">  </span><span class="c1">// Convert torch::Tensor A_ (M, K) to torch::Tensor A (1, M, K) </span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A_</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Convert torch::Tensor B_ (K, N) to torch::Tensor A (1, K, N) </span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">B_</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="p">...</span>
</pre></div>
</div>
<p>As shown in the following code block, we obtain M, N, and K values using input tensor size values. This stride size information is used to reshape the input vector D and allocate the storage space of tensor E. Stride reflects the exact size of continuous elements in memory, which are passed as important parameters to the fundamental instance for GPU kernel use.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// Return the batch count from the size of dimension 0</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">batch_count</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Return the M, N, K from the size of dimension 1 &amp; 2</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">M</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">B</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Initialize the stride size for A, B, D and E</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">stride_A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">K</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">stride_B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">K</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">stride_D0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">N</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">stride_E</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">N</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Initialize the stride size for batched A, B, D and E</span>
<span class="w">  </span><span class="kt">long</span><span class="w"> </span><span class="kt">long</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">batch_stride_A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">M</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">K</span><span class="p">;</span>
<span class="w">  </span><span class="kt">long</span><span class="w"> </span><span class="kt">long</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">batch_stride_B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="p">;</span>
<span class="w">  </span><span class="kt">long</span><span class="w"> </span><span class="kt">long</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">batch_stride_D0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">M</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="p">;</span>
<span class="w">  </span><span class="kt">long</span><span class="w"> </span><span class="kt">long</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">batch_stride_E</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">M</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Convert the tensor of 2-D to 3-D</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">D</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">D_</span><span class="p">.</span><span class="n">view</span><span class="p">({</span><span class="mi">1</span><span class="p">,</span><span class="mi">-1</span><span class="p">}).</span><span class="n">repeat</span><span class="p">({</span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">});</span>

<span class="w">  </span><span class="c1">// Allocate memory for E</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">empty</span><span class="p">({</span><span class="n">batch_count</span><span class="p">,</span><span class="w"> </span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">},</span><span class="w"> </span>
<span class="w">       </span><span class="n">torch</span><span class="o">::</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kInt8</span><span class="p">).</span><span class="n">device</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">device</span><span class="p">()));</span>
</pre></div>
</div>
<p>In the following code block, <code class="docutils literal notranslate"><span class="pre">ADataType</span></code>, <code class="docutils literal notranslate"><span class="pre">BDataType</span></code> and <code class="docutils literal notranslate"><span class="pre">D0DataType</span></code> are used to denote the data precision of the input tensors A, B and D, respectively. <code class="docutils literal notranslate"><span class="pre">EDataType</span></code> is used to denote the data precision of output tensor E. These parameters are specified to <code class="docutils literal notranslate"><span class="pre">I8</span></code> data format (8-bit integer data format) to meet the kernel’s design requirements.</p>
<p><code class="docutils literal notranslate"><span class="pre">AccDataType</span></code> determines the data precision used to represent the multiply-add results of A and B elements. Generally, a larger range data type is applied to store the multiply-add results of A and B to avoid result overflow; <code class="docutils literal notranslate"><span class="pre">I32</span></code> is applied in this case. The <code class="docutils literal notranslate"><span class="pre">CShuffleDataType</span> <span class="pre">I32</span></code> data type indicates that the multiply-add results continue to be stored in LDS as an <code class="docutils literal notranslate"><span class="pre">I32</span></code> data format. All of this is implemented through the following code block.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// Data precision </span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">ADataType</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="n">I8</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">BDataType</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="n">I8</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">AccDataType</span><span class="w">      </span><span class="o">=</span><span class="w"> </span><span class="n">I32</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">CShuffleDataType</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">I32</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">D0DataType</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="n">I8</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">DsDataType</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">Tuple</span><span class="o">&lt;</span><span class="n">D0DataType</span><span class="o">&gt;</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">EDataType</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="n">I8</span><span class="p">;</span>
</pre></div>
</div>
<p>Following the convention of various linear algebra libraries, row-major and column-major orders are used to denote the ways of storing matrices in linear storage. The advantage of specifying matrix B as column major is that all the relevant matrix elements are stored continuously in GPU global memory when a row in A is multiplied by a column in B, which can help GPU achieve data consistency access to improve access performance.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// Specify tensor order</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">ALayout</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">RowMajor</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">BLayout</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">ColumnMajor</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">D0Layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">RowMajor</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">DsLayout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">Tuple</span><span class="o">&lt;</span><span class="n">D0Layout</span><span class="o">&gt;</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">ELayout</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">RowMajor</span><span class="p">;</span>
</pre></div>
</div>
<p>In CK, <code class="docutils literal notranslate"><span class="pre">PassThrough</span></code> is a struct denoting if an operation is applied to the tensor it binds to. To fuse the operations between E1, E2, and E introduced in section <a class="reference internal" href="#operation-flow-analysis">Operation flow analysis</a>, we define a custom C++ struct, <code class="docutils literal notranslate"><span class="pre">ScaleScaleAddRelu</span></code>, and bind it to <code class="docutils literal notranslate"><span class="pre">CDEELementOp</span></code>. It determines the operations that will be applied to <code class="docutils literal notranslate"><span class="pre">CShuffle</span></code> (A×B results), tensor D, α, and β.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// No operations bound to the elements of A and B </span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">AElementOp</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="n">PassThrough</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">BElementOp</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="n">PassThrough</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Operations bound to the elements of C, D and E</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">CDEElementOp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ScaleScaleAddRelu</span><span class="p">;</span>
</pre></div>
</div>
<p>In the binding struct, <code class="docutils literal notranslate"><span class="pre">operator()</span></code> performs an addition operation between <code class="docutils literal notranslate"><span class="pre">CShuffle</span></code> and matrix D, a ReLU operation on the addition results, and a rounding operation on the output elements. It then returns the results to E.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">ScaleScaleAddRelu</span><span class="w"> </span><span class="p">{</span>

<span class="w">  </span><span class="k">template</span><span class="w"> </span><span class="o">&lt;&gt;</span>
<span class="w">  </span><span class="n">__host__</span><span class="w"> </span><span class="n">__device__</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="kt">void</span>
<span class="w">  </span><span class="k">operator</span><span class="p">()</span><span class="o">&lt;</span><span class="n">I8</span><span class="p">,</span><span class="w"> </span><span class="n">I32</span><span class="p">,</span><span class="w"> </span><span class="n">I8</span><span class="o">&gt;</span><span class="p">(</span><span class="n">I8</span><span class="o">&amp;</span><span class="w"> </span><span class="n">e</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">I32</span><span class="o">&amp;</span><span class="w"> </span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">I8</span><span class="o">&amp;</span><span class="w"> </span><span class="n">d</span><span class="p">)</span><span class="w"> </span><span class="k">const</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">      </span><span class="c1">// Scale AxB result with alpha</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">F32</span><span class="w"> </span><span class="n">c_scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">type_convert</span><span class="o">&lt;</span><span class="n">F32</span><span class="o">&gt;</span><span class="p">(</span><span class="n">c</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">alpha</span><span class="p">;</span>

<span class="w">      </span><span class="c1">// Scale D with beta</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">F32</span><span class="w"> </span><span class="n">d_scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">type_convert</span><span class="o">&lt;</span><span class="n">F32</span><span class="o">&gt;</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">beta</span><span class="p">;</span>

<span class="w">      </span><span class="c1">// Perform addition operation</span>
<span class="w">      </span><span class="n">F32</span><span class="w"> </span><span class="n">temp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">c_scale</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">d_scale</span><span class="p">;</span>
<span class="w">      </span>
<span class="w">      </span><span class="c1">// Perform RELU operation</span>
<span class="w">      </span><span class="n">temp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">temp</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="n">temp</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">      </span><span class="c1">// Perform rounding operation </span>
<span class="w">      </span><span class="n">temp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">temp</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">127</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="mi">127</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">temp</span><span class="p">;</span>
<span class="w">      </span>
<span class="w">      </span><span class="c1">// Return to E</span>
<span class="w">      </span><span class="n">e</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">type_convert</span><span class="o">&lt;</span><span class="n">I8</span><span class="o">&gt;</span><span class="p">(</span><span class="n">temp</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">    </span>
<span class="w">  </span><span class="n">F32</span><span class="w"> </span><span class="n">alpha</span><span class="p">;</span>
<span class="w">  </span><span class="n">F32</span><span class="w"> </span><span class="n">beta</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>
</div>
<p>The original input tensors need to be padded to meet GPU tile-based parallelism.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">static</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">GemmDefault</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">tensor_operation</span><span class="o">::</span><span class="n">device</span><span class="o">::</span><span class="n">GemmSpecialization</span><span class="o">::</span><span class="n">MNKPadding</span><span class="p">;</span>
</pre></div>
</div>
<p>The template parameters of the target fundamental instance are initialized with the above parameters and includes default tunable parameters. For specific tuning methods, see <a class="reference internal" href="#tunable-parameters">Tunable parameters</a>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">DeviceOpInstance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">tensor_operation</span><span class="o">::</span><span class="n">device</span><span class="o">::</span><span class="n">DeviceBatchedGemmMultiD_Xdl</span><span class="o">&lt;</span><span class="w"> </span>
<span class="w">    </span><span class="c1">// Tensor layout</span>
<span class="w">    </span><span class="n">ALayout</span><span class="p">,</span><span class="w"> </span><span class="n">BLayout</span><span class="p">,</span><span class="w"> </span><span class="n">DsLayout</span><span class="p">,</span><span class="w"> </span><span class="n">ELayout</span><span class="p">,</span><span class="w"> </span>
<span class="w">    </span><span class="c1">// Tensor data type</span>
<span class="w">    </span><span class="n">ADataType</span><span class="p">,</span><span class="w"> </span><span class="n">BDataType</span><span class="p">,</span><span class="w"> </span><span class="n">AccDataType</span><span class="p">,</span><span class="w"> </span><span class="n">CShuffleDataType</span><span class="p">,</span><span class="w"> </span><span class="n">DsDataType</span><span class="p">,</span><span class="w"> </span><span class="n">EDataType</span><span class="p">,</span><span class="w">  </span>
<span class="w">    </span><span class="c1">// Tensor operation</span>
<span class="w">    </span><span class="n">AElementOp</span><span class="p">,</span><span class="w">  </span><span class="n">BElementOp</span><span class="p">,</span><span class="w"> </span><span class="n">CDEElementOp</span><span class="p">,</span><span class="w">  </span>
<span class="w">    </span><span class="c1">// Padding strategy  </span>
<span class="w">    </span><span class="n">GemmDefault</span><span class="p">,</span>
<span class="w">    </span><span class="c1">// Tunable parameters        </span>
<span class="w">    </span><span class="n">tunable</span><span class="w"> </span><span class="n">parameters</span><span class="o">&gt;</span><span class="p">;</span>
</pre></div>
</div>
<p>Return the address of the first element of tensors:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">A_ref</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="n">ADataType</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">B_ref</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">B</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="n">BDataType</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">D0_ref</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">D</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="n">D0DataType</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">E_ref</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">E</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="n">EDataType</span><span class="o">&gt;</span><span class="p">();</span>
</pre></div>
</div>
<p>The fundamental instance is then initialized and run with actual arguments:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">device_op</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="n">DeviceOpInstance</span><span class="p">{};</span>
<span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">invoker</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">device_op</span><span class="p">.</span><span class="n">MakeInvoker</span><span class="p">();</span>
<span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">argument</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">device_op</span><span class="p">.</span><span class="n">MakeArgument</span><span class="p">(</span>
<span class="w">    </span><span class="n">A_ref</span><span class="p">,</span><span class="w"> </span><span class="n">B_ref</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="n">D0_ref</span><span class="p">},</span><span class="w"> </span><span class="n">E_ref</span><span class="p">,</span>
<span class="w">    </span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="p">,</span>
<span class="w">    </span><span class="n">batch_count</span><span class="p">,</span>
<span class="w">    </span><span class="n">stride_A</span><span class="p">,</span><span class="w"> </span><span class="n">stride_B</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="n">stride_D0</span><span class="p">},</span><span class="w"> </span><span class="n">stride_E</span><span class="p">,</span>
<span class="w">    </span><span class="n">batch_stride_A</span><span class="p">,</span><span class="w"> </span><span class="n">batch_stride_B</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="n">batch_stride_D0</span><span class="p">},</span><span class="w"> </span><span class="n">batch_stride_E</span><span class="p">,</span>
<span class="w">    </span><span class="n">AElementOp</span><span class="p">{},</span><span class="w"> </span><span class="n">BElementOp</span><span class="p">{},</span><span class="w"> </span><span class="n">CDEElementOp</span><span class="p">{</span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">});</span>

<span class="n">invoker</span><span class="p">.</span><span class="n">Run</span><span class="p">(</span><span class="n">argument</span><span class="p">,</span><span class="w"> </span><span class="n">StreamConfig</span><span class="p">{</span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">});</span>
</pre></div>
</div>
<p>The output of the fundamental instance is a calculated batched matrix E (batch, M, N). Before the return, it needs to be converted to a 2-D matrix if a normal GEMM result is required.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Convert (1, M, N) to (M, N) </span>
<span class="k">return</span><span class="w"> </span><span class="n">E</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="binding-to-python">
<h6>Binding to Python<a class="headerlink" href="#binding-to-python" title="Link to this heading">#</a></h6>
<p>Since these functions are written in C++ and <code class="docutils literal notranslate"><span class="pre">torch::Tensor</span></code>, you can use <code class="docutils literal notranslate"><span class="pre">pybind11</span></code> to bind the functions and import them as Python modules. For the example, the necessary binding code for exposing the functions in the table spans but a few lines.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/extension.h&gt;</span>

<span class="n">PYBIND11_MODULE</span><span class="p">(</span><span class="n">TORCH_EXTENSION_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">){</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"linear_ab_i8_de_f32"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">linear_ab_i8_de_f32</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"linear_relu_abde_i8"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">linear_relu_abde_i8</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"linear_abde_i8"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">linear_abde_i8</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"bmm_abe_i8"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">bmm_abe_i8</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"bmm_ab_i8_e_f32"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">bmm_ab_i8_e_f32</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Build the C++ extension by writing a <code class="docutils literal notranslate"><span class="pre">setup.py</span></code> script that uses <code class="docutils literal notranslate"><span class="pre">setuptools</span></code> to compile the C++ code. A reference implementation of the <code class="docutils literal notranslate"><span class="pre">setup.py</span></code> script is as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">setuptools</span><span class="w"> </span><span class="kn">import</span> <span class="n">setup</span><span class="p">,</span> <span class="n">find_packages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">cpp_extension</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.cpp_extension</span><span class="w"> </span><span class="kn">import</span> <span class="n">BuildExtension</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"CC"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"hipcc"</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"CXX"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"hipcc"</span>

<span class="n">sources</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">'torch_int/kernels/linear.cpp'</span><span class="p">,</span>
    <span class="s1">'torch_int/kernels/bmm.cpp'</span><span class="p">,</span>
    <span class="s1">'torch_int/kernels/pybind.cpp'</span><span class="p">,</span> 
<span class="p">]</span>

<span class="n">include_dirs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'torch_int/kernels/include'</span><span class="p">]</span>
<span class="n">extra_link_args</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'libutility.a'</span><span class="p">]</span>
<span class="n">extra_compile_args</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'-O3'</span><span class="p">,</span><span class="s1">'-DNDEBUG'</span><span class="p">,</span> <span class="s1">'-std=c++17'</span><span class="p">,</span> <span class="s1">'--offload-arch=gfx942'</span><span class="p">,</span> <span class="s1">'-DCK_ENABLE_INT8'</span><span class="p">,</span> <span class="s1">'-D__HIP_PLATFORM_AMD__=1'</span><span class="p">]</span>

<span class="n">setup</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">'torch_int'</span><span class="p">,</span>
    <span class="n">ext_modules</span><span class="o">=</span><span class="p">[</span>
        <span class="n">cpp_extension</span><span class="o">.</span><span class="n">CUDAExtension</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">'torch_int.rocm'</span><span class="p">,</span>
            <span class="n">sources</span><span class="o">=</span><span class="n">sources</span><span class="p">,</span>
            <span class="n">include_dirs</span><span class="o">=</span><span class="n">include_dirs</span><span class="p">,</span>
            <span class="n">extra_link_args</span><span class="o">=</span><span class="n">extra_link_args</span><span class="p">,</span>
            <span class="n">extra_compile_args</span><span class="o">=</span><span class="n">extra_compile_args</span>
            <span class="p">),</span>
    <span class="p">],</span>
    <span class="n">cmdclass</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">'build_ext'</span><span class="p">:</span> <span class="n">BuildExtension</span><span class="o">.</span><span class="n">with_options</span><span class="p">(</span><span class="n">use_ninja</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="p">},</span>
    <span class="n">packages</span><span class="o">=</span><span class="n">find_packages</span><span class="p">(</span>
        <span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="s1">'notebook'</span><span class="p">,</span> <span class="s1">'scripts'</span><span class="p">,</span> <span class="s1">'tests'</span><span class="p">]),</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Run <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">setup.py</span> <span class="pre">install</span></code> to build and install the extension. It should look something like Figure 6:</p>
<!-- 
================
 ### Figure 6
================ -->
<figure class="align-default" id="id10">
<img alt="_images/ck-compilation.jpg" src="_images/ck-compilation.jpg"/>
<figcaption>
<p><span class="caption-text">Compilation and installation of the INT8 kernels.</span><a class="headerlink" href="#id10" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="int8-model-inference-and-performance">
<h6>INT8 model inference and performance<a class="headerlink" href="#int8-model-inference-and-performance" title="Link to this heading">#</a></h6>
<p>The implementation architecture of running SmoothQuant models on MI300X GPUs is illustrated in Figure 7, where (a) shows the decoder layer composition components of the target model, (b) shows the major implementation class for the decoder layer components, and (c) denotes the underlying GPU kernels implemented by CK instance.</p>
<!-- 
================
 ### Figure 7
================ -->
<figure class="align-default" id="id11">
<img alt="_images/ck-inference_flow.jpg" src="_images/ck-inference_flow.jpg"/>
<figcaption>
<p><span class="caption-text">The implementation architecture of running SmoothQuant models on AMD MI300X accelerators.</span><a class="headerlink" href="#id11" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>For the target <a class="reference external" href="https://huggingface.co/mit-han-lab/opt-13b-smoothquant">SQ quantized model</a>, each decoder layer contains three major components: attention calculation, layer normalization, and linear transformation in fully connected layers.  The corresponding implementation classes for these components are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Int8OPTAttention</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">W8A8B8O8LinearReLU</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">W8A8BF32OF32Linear</span></code></p></li>
</ul>
<p>These classes’ underlying implementation logits will harness the functions in previous table. Note that for the example, the <code class="docutils literal notranslate"><span class="pre">LayerNormQ</span></code> module is implemented by the torch native module.</p>
<p>Testing environment:
The hardware platform used for testing equips with 256 AMD EPYC 9534 64-Core Processor, 8 AMD Instinct MI300X accelerators and 1.5T memory. The testing was done in a publicly available Docker image from Docker Hub:
<a class="reference external" href="https://hub.docker.com/layers/rocm/pytorch/rocm6.1_ubuntu22.04_py3.10_pytorch_2.1.2/images/sha256-f6ea7cee8aae299c7f6368187df7beed29928850c3929c81e6f24b34271d652b"><code class="docutils literal notranslate"><span class="pre">rocm/pytorch:rocm6.1_ubuntu22.04_py3.10_pytorch_2.1.2</span></code></a></p>
<p>The tested models are OPT-1.3B, 2.7B, 6.7B and 13B FP16 models and the corresponding SmoothQuant INT8 OPT models were obtained from Hugging Face.</p>
<p>Note that since the default values were used for the tunable parameters of the fundamental instance, the performance of the INT8 kernel is suboptimal.</p>
<p>Figure 8 shows the performance comparisons between the original FP16 and the SmoothQuant-quantized INT8 models on a single MI300X accelerator. The GPU memory footprints of SmoothQuant-quantized models are significantly reduced. It also indicates the per-sample inference latency is significantly reduced for all SmoothQuant-quantized OPT models (illustrated in (b)). Notably, the performance of the CK instance-based INT8 kernel steadily improves with an increase in model size.</p>
<!-- 
================
 ### Figure 8
================ -->
<figure class="align-default" id="id12">
<img alt="_images/ck-comparisons.jpg" src="_images/ck-comparisons.jpg"/>
<figcaption>
<p><span class="caption-text">Performance comparisons between the original FP16 and the SmoothQuant-quantized INT8 models on a single MI300X accelerator.</span><a class="headerlink" href="#id12" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>For accuracy comparisons between the original FP16 and INT8 models, the evaluation is done by using the first 1,000 samples from the LAMBADA dataset’s validation set. We employ the same Last Token Prediction Accuracy method introduced in <a class="reference external" href="https://github.com/mit-han-lab/smoothquant/blob/main/examples/smoothquant_opt_real_int8_demo.ipynb">SmoothQuant Real-INT8 Inference for PyTorch</a> as our evaluation metric. The comparison results are shown in Table 2.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id13">
<caption><span class="caption-text">The inference accuracy comparisons of SmoothQuant quantized models on Instinct MI300X.</span><a class="headerlink" href="#id13" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Models</p></th>
<th class="head"><p>Hugging Face FP16 model accuracy</p></th>
<th class="head"><p>SmoothQuant quantized INT8 model accuracy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>opt-1.3B</p></td>
<td><p>0.72</p></td>
<td><p>0.70</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>opt-2.7B</p></td>
<td><p>0.76</p></td>
<td><p>0.75</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>opt-6.7B</p></td>
<td><p>0.80</p></td>
<td><p>0.79</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>opt-13B</p></td>
<td><p>0.79</p></td>
<td><p>0.77</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="conclusion">
<h5>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h5>
<p>CK provides a rich set of template parameters for generating flexible accelerated computing kernels for difference application scenarios.</p>
<p>CK supports multiple instruction sets of AMD Instinct GPUs, operator fusion and different data precisions. Its composability helps users quickly construct operator performance verification.</p>
<p>With CK, you can build more effective AI applications with higher flexibility and better performance on different AMD accelerator platforms.</p>
</section>
</section>
<span id="document-how-to/rocm-for-ai/inference-optimization/optimizing-triton-kernel"></span><section id="optimizing-triton-kernels">
<h4>Optimizing Triton kernels<a class="headerlink" href="#optimizing-triton-kernels" title="Link to this heading">#</a></h4>
<p>This section introduces the general steps for
<a class="reference external" href="https://openai.com/index/triton/">Triton</a> kernel optimization. Broadly,
Triton kernel optimization is similar to <a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/how-to/performance_guidelines.html" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-doc">HIP</span></a>
and CUDA kernel optimization.</p>
<p>Refer to the
<a class="reference internal" href="#mi300x-triton-kernel-performance-optimization"><span class="std std-ref">Triton kernel performance optimization</span></a>
section of the <a class="reference internal" href="#document-how-to/rocm-for-ai/inference-optimization/workload"><span class="doc">AMD Instinct MI300X workload optimization</span></a> guide
for detailed information.</p>
<p>Triton kernel performance optimization includes the following topics.</p>
<ul class="simple">
<li><p><a class="reference internal" href="#mi300x-autotunable-kernel-config"><span class="std std-ref">Auto-tunable kernel configurations</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-mlir-analysis"><span class="std std-ref">MLIR analysis</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-assembly-analysis"><span class="std std-ref">ISA assembly analysis</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-torchinductor-tuning"><span class="std std-ref">PyTorch inductor max-autotune tuning knobs</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-compute-kernel-occ"><span class="std std-ref">Compute the occupancy of a kernel</span></a></p></li>
</ul>
</section>
<span id="document-how-to/rocm-for-ai/inference-optimization/profiling-and-debugging"></span><section id="profiling-and-debugging">
<h4>Profiling and debugging<a class="headerlink" href="#profiling-and-debugging" title="Link to this heading">#</a></h4>
<p>This section provides an index for further documentation on profiling and
debugging tools and their common usage patterns.</p>
<p>See <a class="reference internal" href="#mi300x-profiling-start"><span class="std std-ref">AMD Instinct MI300X™ workload optimization</span></a>
for a conceptual summary of the workload profiling workflow for ROCm applications
on AMD hardware – including fine-tuning LLMs.</p>
<p>There, you’ll find information on higher-level and kernel-level profiling tools
as well as other profiling and debugging suggestions.</p>
<ul class="simple">
<li><p><a class="reference internal" href="#mi300x-pytorch-profiler"><span class="std std-ref">PyTorch Profiler</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-profiling-tools"><span class="std std-ref">ROCm profiling tools</span></a></p>
<ul>
<li><p><a class="reference internal" href="#mi300x-rocprof"><span class="std std-ref">ROCProfiler</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-rocprof-compute"><span class="std std-ref">ROCm Compute Profiler</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-rocprof-systems"><span class="std std-ref">ROCm Systems Profiler</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#mi300x-rocr-debug-agent"><span class="std std-ref">ROCr Debug Agent</span></a></p></li>
</ul>
</section>
<span id="document-how-to/rocm-for-ai/inference-optimization/workload"></span><section id="amd-instinct-mi300x-workload-optimization">
<h4>AMD Instinct MI300X workload optimization<a class="headerlink" href="#amd-instinct-mi300x-workload-optimization" title="Link to this heading">#</a></h4>
<p>This document provides guidelines for optimizing the performance of AMD
Instinct™ MI300X accelerators, with a particular focus on GPU kernel
programming, high-performance computing (HPC), and deep learning operations
using PyTorch. It delves into specific workloads such as
<a class="reference internal" href="#mi300x-vllm-optimization"><span class="std std-ref">model inference</span></a>, offering strategies to
enhance efficiency.</p>
<p>The following topics highlight <a class="reference internal" href="#mi300x-auto-tune"><span class="std std-ref">auto-tunable configurations</span></a>
that streamline optimization as well as advanced techniques like
<a class="reference internal" href="#mi300x-triton-kernel-performance-optimization"><span class="std std-ref">Triton kernel optimization</span></a> for
meticulous tuning.</p>
<section id="workload-tuning-strategy">
<h5>Workload tuning strategy<a class="headerlink" href="#workload-tuning-strategy" title="Link to this heading">#</a></h5>
<p>By following a structured approach, you can systematically address
performance issues and enhance the efficiency of your workloads on AMD Instinct
MI300X accelerators.</p>
<section id="measure-the-current-workload">
<h6>Measure the current workload<a class="headerlink" href="#measure-the-current-workload" title="Link to this heading">#</a></h6>
<p>Begin by evaluating the performance of your workload in its current state. This
involves running benchmarks and collecting performance data to establish a
baseline. Understanding how your workload behaves under different conditions
provides critical insights into where improvements are needed.</p>
</section>
<section id="mi300x-profiling-start">
<span id="identify-tuning-requirements"></span><h6>Identify tuning requirements<a class="headerlink" href="#mi300x-profiling-start" title="Link to this heading">#</a></h6>
<p>Analyze the collected performance data to identify areas where tuning is
required. This could involve detecting bottlenecks in CPU, GPU, memory, or data
transfer. Understanding these requirements will help direct your optimization
efforts more effectively.</p>
<p>Profiling is a fundamental step in workload tuning. It allows you to gather
detailed information about how your workload utilizes system resources, and
where potential inefficiencies lie. Profiling tools can provide insights into
both high-level and granular performance metrics. See <a class="reference internal" href="#mi300x-profiling-tools"><span class="std std-ref">Profiling tools</span></a>.</p>
<section id="high-level-profiling-tools">
<h6 aria-level="7">High-level profiling tools<a class="headerlink" href="#high-level-profiling-tools" title="Link to this heading">#</a></h6>
<p>For a broad overview, use tools like the
<a class="reference internal" href="#mi300x-pytorch-profiler"><span class="std std-ref">PyTorch Profiler</span></a>, which helps in
understanding how PyTorch operations are executed and where time is spent. This
is particularly useful for developers new to workload tuning, as it provides a
comprehensive view without requiring in-depth knowledge of lower-level
operations.</p>
</section>
<section id="kernel-level-profiling-tools">
<h6 aria-level="7">Kernel-level profiling tools<a class="headerlink" href="#kernel-level-profiling-tools" title="Link to this heading">#</a></h6>
<p>When profiling indicates that GPUs are a performance bottleneck, delve deeper
into kernel-level profiling. Tools such as the
<a class="reference internal" href="#mi300x-rocr-debug-agent"><span class="std std-ref">ROCr Debug Agent</span></a>,
<a class="reference internal" href="#mi300x-rocprof"><span class="std std-ref">ROCProfiler</span></a>, and
<a class="reference internal" href="#mi300x-rocprof-compute"><span class="std std-ref">ROCm Compute Profiler</span></a> offer detailed insights
into GPU kernel execution. These tools can help isolate problematic GPU
operations and provide data needed for targeted optimizations.</p>
</section>
</section>
<section id="analyze-and-tune">
<h6>Analyze and tune<a class="headerlink" href="#analyze-and-tune" title="Link to this heading">#</a></h6>
<p>Based on the insights gained from profiling, focus your tuning efforts on the
identified bottlenecks. This might involve optimizing specific kernel
operations, adjusting memory access patterns, or modifying computational
algorithms.</p>
<p>The following subsections discuss optimization ranging from high-level and more
automated strategies to more involved, hands-on optimization.</p>
<section id="optimize-model-inference-with-vllm">
<h6 aria-level="7">Optimize model inference with vLLM<a class="headerlink" href="#optimize-model-inference-with-vllm" title="Link to this heading">#</a></h6>
<p>vLLM provides tools and techniques specifically designed for efficient model
inference on AMD Instinct MI300X accelerators. See <a class="reference internal" href="#fine-tuning-llms-vllm"><span class="std std-ref">vLLM inference</span></a>
for installation guidance. Optimizing performance with vLLM
involves configuring tensor parallelism, leveraging advanced features, and
ensuring efficient execution. Here’s how to optimize vLLM performance:</p>
<ul class="simple">
<li><p>Tensor parallelism: Configure the
<a class="reference internal" href="#mi300x-vllm-multiple-gpus"><span class="std std-ref">tensor-parallel-size parameter</span></a> to distribute
tensor computations across multiple GPUs. Adjust parameters such as
<code class="docutils literal notranslate"><span class="pre">batch-size</span></code>, <code class="docutils literal notranslate"><span class="pre">input-len</span></code>, and <code class="docutils literal notranslate"><span class="pre">output-len</span></code> based on your workload.</p></li>
<li><p>Configuration for vLLM: Set <a class="reference internal" href="#mi300x-vllm-optimization"><span class="std std-ref">parameters</span></a>
according to workload requirements. Benchmark performance to understand
characteristics and identify bottlenecks.</p></li>
<li><p>Benchmarking and performance metrics: Measure latency and throughput to
evaluate performance.</p></li>
</ul>
</section>
<section id="auto-tunable-configurations">
<span id="mi300x-auto-tune"></span><h6 aria-level="7">Auto-tunable configurations<a class="headerlink" href="#auto-tunable-configurations" title="Link to this heading">#</a></h6>
<p>Auto-tunable configurations can significantly streamline performance
optimization by automatically adjusting parameters based on workload
characteristics. For example:</p>
<ul class="simple">
<li><p>PyTorch: Utilize <a class="reference internal" href="#mi300x-torchinductor-tuning"><span class="std std-ref">PyTorch’s built-in auto-tuning features</span></a>,
such as the <a class="reference internal" href="#mi300x-tunableop"><span class="std std-ref">TunableOp</span></a> module, which helps in
optimizing operation performance by exploring different configurations.</p></li>
<li><p>MIOpen: Leverage <a class="reference internal" href="#mi300x-miopen-tuning"><span class="std std-ref">MIOpen’s auto-tuning capabilities</span></a>
for convolutional operations and other primitives to find optimal settings for
your specific hardware.</p></li>
<li><p>Triton: Use <a class="reference internal" href="#mi300x-autotunable-kernel-config"><span class="std std-ref">Triton’s auto-tuning features</span></a>
to explore various kernel configurations and automatically select the
best-performing ones.</p></li>
</ul>
</section>
<section id="manual-tuning">
<h6 aria-level="7">Manual tuning<a class="headerlink" href="#manual-tuning" title="Link to this heading">#</a></h6>
<p>Advanced developers can manually adjust parameters and configurations to
optimize performance. Both Triton and HIP involve manual tuning aspects.</p>
<ul class="simple">
<li><p>ROCm libraries: Optimize GPU performance by adjusting various parameters and
configurations within <a class="reference internal" href="#mi300x-rocm-library-tuning"><span class="std std-ref">ROCm libraries</span></a>. This
approach involves hands-on optimization to maximize efficiency for specific
workloads.</p></li>
<li><p>Triton: Tune Triton kernels by adjusting parameters tailored to
your workload to
<a class="reference internal" href="#mi300x-triton-gpu-utilization"><span class="std std-ref">optimize GPU resource utilization</span></a> and
better <a class="reference internal" href="#mi300x-assembly-analysis"><span class="std std-ref">leverage specific hardware features</span></a>.</p></li>
<li><p>HIP: Profile and <a class="reference internal" href="#mi300x-hip-optimization"><span class="std std-ref">optimize HIP kernels</span></a> by
optimizing parallel execution, memory access patterns, and other aspects.</p></li>
</ul>
</section>
</section>
<section id="iterate-and-validate">
<h6>Iterate and validate<a class="headerlink" href="#iterate-and-validate" title="Link to this heading">#</a></h6>
<p>Optimization is an iterative process. After applying tuning changes, re-profile
the workload to validate improvements and ensure that the changes have had the
desired effect. Continuous iteration helps refine the performance gains and
address any new bottlenecks that may emerge.</p>
<p>ROCm provides a prebuilt optimized Docker image that has everything required to implement
the LLM inference tips in this section. It includes ROCm, PyTorch, and vLLM.
For more information, see <a class="reference internal" href="#document-how-to/rocm-for-ai/inference/benchmark-docker/vllm"><span class="doc">vLLM inference performance testing</span></a>.</p>
</section>
</section>
<section id="profiling-tools">
<span id="mi300x-profiling-tools"></span><h5>Profiling tools<a class="headerlink" href="#profiling-tools" title="Link to this heading">#</a></h5>
<p>AMD profiling tools provide valuable insights into how efficiently your
application utilizes hardware and help diagnose potential bottlenecks that
contribute to poor performance. Developers targeting AMD GPUs have multiple
tools available depending on their specific profiling needs.</p>
<ul class="simple">
<li><p>ROCProfiler tool collects kernel execution performance
metrics. For more information, see the
<a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler/en/latest/index.html" title="(in rocprofiler Documentation v2.0.0)"><span class="xref std std-doc">ROCProfiler</span></a>
documentation.</p></li>
<li><p>ROCm Compute Profiler builds upon ROCProfiler but provides more guided analysis.
For more information, see
<a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler-compute/en/latest/index.html" title="(in ROCm Compute Profiler v3.1.1)"><span class="xref std std-doc">ROCm Compute Profiler documentation</span></a>.</p></li>
</ul>
<p>Refer to <a class="reference internal" href="#document-how-to/rocm-for-ai/inference-optimization/profiling-and-debugging"><span class="doc">Profiling and debugging</span></a>
to explore commonly used profiling tools and their usage patterns.</p>
<p>Once performance bottlenecks are identified, you can implement an informed workload
tuning strategy. If kernels are the bottleneck, consider:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#mi300x-tunableop"><span class="std std-ref">Auto-tuning in PyTorch with TunableOp</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-miopen-tuning"><span class="std std-ref">Auto-tuning in MIOpen</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-autotunable-kernel-config"><span class="std std-ref">Triton auto-tunable kernel configurations</span></a></p></li>
</ul>
<p>If auto-tuning does not meet your requirements, consider
<a class="reference internal" href="#mi300x-triton-kernel-performance-optimization"><span class="std std-ref">Triton kernel performance optimization</span></a>.</p>
<p>If the issue is multi-GPU scale-out, try
<a class="reference internal" href="#mi300x-rccl"><span class="std std-ref">RCCL tuning and configuration</span></a>.</p>
<p>This section discusses profiling and debugging tools and some of their common usage patterns with ROCm applications.</p>
<section id="pytorch-profiler">
<span id="mi300x-pytorch-profiler"></span><h6>PyTorch Profiler<a class="headerlink" href="#pytorch-profiler" title="Link to this heading">#</a></h6>
<p><a class="reference external" href="https://pytorch.org/docs/stable/profiler.html">PyTorch Profiler</a> can be invoked inside Python scripts, letting you
collect CPU and GPU performance metrics while the script is running. See the <a class="reference external" href="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html">PyTorch Profiler tutorial</a> for more information.</p>
<p>You can then visualize and view these metrics using an open-source profile visualization tool like
<a class="reference external" href="https://ui.perfetto.dev">Perfetto UI</a>.</p>
<ol class="arabic">
<li><p>Use the following snippet to invoke PyTorch Profiler in your code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.models</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">models</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.profiler</span><span class="w"> </span><span class="kn">import</span> <span class="n">profile</span><span class="p">,</span> <span class="n">record_function</span><span class="p">,</span> <span class="n">ProfilerActivity</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="k">with</span> <span class="n">profile</span><span class="p">(</span><span class="n">activities</span><span class="o">=</span><span class="p">[</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span> <span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">])</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">record_function</span><span class="p">(</span><span class="s2">"model_inference"</span><span class="p">):</span>
        <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">prof</span><span class="o">.</span><span class="n">export_chrome_trace</span><span class="p">(</span><span class="s2">"resnet18_profile.json"</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Profile results in <code class="docutils literal notranslate"><span class="pre">resnet18_profile.json</span></code> can be viewed by the Perfetto visualization tool. Go to
<a class="reference external" href="https://ui.perfetto.dev">https://ui.perfetto.dev</a> and import the file. In your Perfetto visualization, you’ll see that the upper section
shows transactions denoting the CPU activities that launch GPU kernels while the lower section shows the actual GPU
activities where it processes the <code class="docutils literal notranslate"><span class="pre">resnet18</span></code> inferences layer by layer.</p>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="_images/perfetto-trace.svg"><img alt="_images/perfetto-trace.svg" src="_images/perfetto-trace.svg" style="width: 800px;"/>
</a>
<figcaption>
<p><span class="caption-text">Perfetto trace visualization example.</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
</li>
</ol>
</section>
<section id="rocm-profiling-tools">
<h6>ROCm profiling tools<a class="headerlink" href="#rocm-profiling-tools" title="Link to this heading">#</a></h6>
<p>Heterogenous systems, where programs run on both CPUs and GPUs, introduce additional complexities. Understanding the
critical path and kernel execution is all the more important. So, performance tuning is a necessary component in the
benchmarking process.</p>
<p>With AMD’s profiling tools, developers are able to gain important insight into how efficiently their application is
using hardware resources and effectively diagnose potential bottlenecks contributing to poor performance. Developers
working with AMD Instinct accelerators have multiple tools depending on their specific profiling needs; these include:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#mi300x-rocprof"><span class="std std-ref">ROCProfiler</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-rocprof-compute"><span class="std std-ref">ROCm Compute Profiler</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-rocprof-systems"><span class="std std-ref">ROCm Systems Profiler</span></a></p></li>
</ul>
<section id="rocprofiler">
<span id="mi300x-rocprof"></span><h6 aria-level="7">ROCProfiler<a class="headerlink" href="#rocprofiler" title="Link to this heading">#</a></h6>
<p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler/en/latest/index.html" title="(in rocprofiler Documentation v2.0.0)"><span class="xref std std-doc">ROCProfiler</span></a> is primarily a low-level API for accessing and extracting GPU hardware performance
metrics, commonly called <em>performance counters</em>. These counters quantify the performance of the underlying architecture
showcasing which pieces of the computational pipeline and memory hierarchy are being utilized.</p>
<p>Your ROCm installation contains a script or executable command called <code class="docutils literal notranslate"><span class="pre">rocprof</span></code> which provides the ability to list all
available hardware counters for your specific accelerator or GPU, and run applications while collecting counters during
their execution.</p>
<p>This <code class="docutils literal notranslate"><span class="pre">rocprof</span></code> utility also depends on the <a class="reference external" href="https://rocm.docs.amd.com/projects/roctracer/en/latest/index.html" title="(in roctracer Documentation v4.1.0)"><span class="xref std std-doc">ROCTracer and ROC-TX libraries</span></a>, giving it the
ability to collect timeline traces of the accelerator software stack as well as user-annotated code regions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">rocprof</span></code> is a CLI-only utility where inputs and outputs take the form of text and CSV files. These
formats provide a raw view of the data and puts the onus on the user to parse and analyze. <code class="docutils literal notranslate"><span class="pre">rocprof</span></code>
gives the user full access and control of raw performance profiling data, but requires extra effort to analyze the
collected data.</p>
</div>
</section>
<section id="rocm-compute-profiler">
<span id="mi300x-rocprof-compute"></span><h6 aria-level="7">ROCm Compute Profiler<a class="headerlink" href="#rocm-compute-profiler" title="Link to this heading">#</a></h6>
<p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler-compute/en/latest/index.html" title="(in ROCm Compute Profiler v3.1.1)"><span class="xref std std-doc">ROCm Compute Profiler</span></a> is a system performance profiler for high-performance computing (HPC) and
machine learning (ML) workloads using Instinct accelerators. Under the hood, ROCm Compute Profiler uses
<a class="reference internal" href="#mi300x-rocprof"><span class="std std-ref">ROCProfiler</span></a> to collect hardware performance counters. The ROCm Compute Profiler tool performs
system profiling based on all approved hardware counters for Instinct
accelerator architectures. It provides high level performance analysis features including System Speed-of-Light, IP
block Speed-of-Light, Memory Chart Analysis, Roofline Analysis, Baseline Comparisons, and more.</p>
<p>ROCm Compute Profiler takes the guesswork out of profiling by removing the need to provide text input files with lists of counters
to collect and analyze raw CSV output files as is the case with ROCProfiler. Instead, ROCm Compute Profiler automates the collection
of all available hardware counters in one command and provides graphical interfaces to help users understand and
analyze bottlenecks and stressors for their computational workloads on AMD Instinct accelerators.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>ROCm Compute Profiler collects hardware counters in multiple passes, and will therefore re-run the application during each pass
to collect different sets of metrics.</p>
</div>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="_images/rocprof-compute-analysis.png"><img alt="_images/rocprof-compute-analysis.png" src="_images/rocprof-compute-analysis.png" style="width: 800px;"/>
</a>
<figcaption>
<p><span class="caption-text">ROCm Compute Profiler memory chart analysis panel.</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>In brief, ROCm Compute Profiler provides details about hardware activity for a particular GPU kernel. It also supports both
a web-based GUI or command-line analyzer, depending on your preference.</p>
</section>
<section id="rocm-systems-profiler">
<span id="mi300x-rocprof-systems"></span><h6 aria-level="7">ROCm Systems Profiler<a class="headerlink" href="#rocm-systems-profiler" title="Link to this heading">#</a></h6>
<p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler-systems/en/latest/index.html" title="(in rocprofiler-systems v1.0.2)"><span class="xref std std-doc">ROCm Systems Profiler</span></a> is a comprehensive profiling and tracing tool for parallel applications,
including HPC and ML packages, written in C, C++, Fortran, HIP, OpenCL, and Python which execute on the CPU or CPU and
GPU. It is capable of gathering the performance information of functions through any combination of binary
instrumentation, call-stack sampling, user-defined regions, and Python interpreter hooks.</p>
<p>ROCm Systems Profiler supports interactive visualization of comprehensive traces in the web browser in addition to high-level
summary profiles with <code class="docutils literal notranslate"><span class="pre">mean/min/max/stddev</span></code> statistics. Beyond runtime
information, ROCm Systems Profiler supports the collection of system-level metrics such as CPU frequency, GPU temperature, and GPU
utilization. Process and thread level metrics such as memory usage, page faults, context switches, and numerous other
hardware counters are also included.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When analyzing the performance of an application, it is best not to assume you know where the performance
bottlenecks are and why they are happening. ROCm Systems Profiler is the ideal tool for characterizing where optimization would
have the greatest impact on the end-to-end execution of the application and to discover what else is happening on the
system during a performance bottleneck.</p>
</div>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="_images/rocprof-systems-timeline.png"><img alt="_images/rocprof-systems-timeline.png" src="_images/rocprof-systems-timeline.png" style="width: 800px;"/>
</a>
<figcaption>
<p><span class="caption-text">ROCm Systems Profiler timeline trace example.</span><a class="headerlink" href="#id4" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
</section>
<section id="vllm-performance-optimization">
<span id="mi300x-vllm-optimization"></span><h5>vLLM performance optimization<a class="headerlink" href="#vllm-performance-optimization" title="Link to this heading">#</a></h5>
<p>vLLM is a high-throughput and memory efficient inference and serving engine for large language models that has gained traction in the AI community for
its performance and ease of use. See <a class="reference internal" href="#fine-tuning-llms-vllm"><span class="std std-ref">vLLM inference</span></a> for a primer on vLLM with ROCm.</p>
<section id="performance-environment-variables">
<h6>Performance environment variables<a class="headerlink" href="#performance-environment-variables" title="Link to this heading">#</a></h6>
<p>The following performance tips are not <em>specific</em> to vLLM – they are general
but relevant in this context. You can tune the following vLLM parameters to
achieve optimal request latency and throughput performance.</p>
<ul class="simple">
<li><p>As described in <a class="reference external" href="https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/system-optimization/mi300x.html#environment-variables">Environment variables (MI300X)</a>,
the environment variable <code class="docutils literal notranslate"><span class="pre">HIP_FORCE_DEV_KERNARG</span></code> can improve vLLM
performance. Set it to <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">HIP_FORCE_DEV_KERNARG=1</span></code>.</p></li>
<li><p>Set the <a class="reference internal" href="#mi300x-rccl"><span class="std std-ref">RCCL environment variable</span></a> <code class="docutils literal notranslate"><span class="pre">NCCL_MIN_NCHANNELS</span></code>
to <code class="docutils literal notranslate"><span class="pre">112</span></code> to increase the number of channels on MI300X to potentially improve
performance.</p></li>
<li><p>Set the environment variable <code class="docutils literal notranslate"><span class="pre">TORCH_BLAS_PREFER_HIPBLASLT=1</span></code> to use hipBLASLt to improve performance.</p></li>
</ul>
</section>
<section id="auto-tuning-using-pytorch-tunableop">
<h6>Auto-tuning using PyTorch TunableOp<a class="headerlink" href="#auto-tuning-using-pytorch-tunableop" title="Link to this heading">#</a></h6>
<p>Since vLLM is based on the PyTorch framework, PyTorch TunableOp can be used for auto-tuning.
You can run auto-tuning with TunableOp in two simple steps without modifying your code:</p>
<ul>
<li><p>Enable TunableOp and tuning. Optionally, enable verbose mode:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">PYTORCH_TUNABLEOP_ENABLED</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">PYTORCH_TUNABLEOP_VERBOSE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>your_vllm_script.sh
</pre></div>
</div>
</li>
<li><p>Enable TunableOp and disable tuning and measure.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">PYTORCH_TUNABLEOP_ENABLED</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">PYTORCH_TUNABLEOP_TUNING</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>your_vllm_script.sh
</pre></div>
</div>
</li>
</ul>
<p>Learn more about TunableOp in the <a class="reference internal" href="#mi300x-tunableop"><span class="std std-ref">PyTorch TunableOp</span></a> section.</p>
</section>
<section id="performance-tuning-based-on-vllm-engine-configurations">
<h6>Performance tuning based on vLLM engine configurations<a class="headerlink" href="#performance-tuning-based-on-vllm-engine-configurations" title="Link to this heading">#</a></h6>
<p>The following subsections describe vLLM-specific configurations for performance tuning.
You can tune the following vLLM parameters to achieve optimal performance.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tensor_parallel_size</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpu_memory_utilization</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dtype</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">enforce_eager</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kv_cache_dtype</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input_len</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_len</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_num_seqs</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_scheduler_steps</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_model_len</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">enable_chunked_prefill</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">distributed_executor_backend</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_seq_len_to_capture</span></code></p></li>
</ul>
<p>Refer to <a class="reference external" href="https://docs.vllm.ai/en/latest/models/performance.html">vLLM documentation</a>
for additional performance tips. <a class="reference internal" href="#fine-tuning-llms-vllm"><span class="std std-ref">vLLM inference</span></a> describes vLLM
usage with ROCm.</p>
<p>ROCm provides a prebuilt optimized Docker image for validating the performance
of LLM inference with vLLM on MI300X series accelerators. The Docker image includes
ROCm, vLLM, and PyTorch. For more information, see
<a class="reference internal" href="#document-how-to/rocm-for-ai/inference/benchmark-docker/vllm"><span class="doc">vLLM inference performance testing</span></a>.</p>
</section>
<section id="evaluating-performance-by-throughput-measurement">
<span id="mi300x-vllm-throughput-measurement"></span><h6>Evaluating performance by throughput measurement<a class="headerlink" href="#evaluating-performance-by-throughput-measurement" title="Link to this heading">#</a></h6>
<p>This tuning guide evaluates the performance of LLM inference workloads by measuring throughput in tokens per second (TPS). Throughput can be assessed using both real-world and synthetic data, depending on your evaluation goals.</p>
<p>Refer to the benchmarking script located at <code class="docutils literal notranslate"><span class="pre">benchmarks/benchmark_throughput.py</span></code> in the <a class="reference external" href="https://github.com/ROCm/vllm/blob/main/benchmarks/benchmark_throughput.py">vLLM repository</a>.
Use this script to measure throughput effectively. You can assess throughput using real-world and synthetic data, depending on your evaluation goals.</p>
<ul>
<li><p>For realistic performance evaluation, you can use datasets like Hugging Face’s
<code class="docutils literal notranslate"><span class="pre">ShareGPT_V3_unfiltered_cleaned_split.json</span></code>. This dataset includes real-world conversational
data, making it a good representation of typical use cases for language models. Download it using
the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json
</pre></div>
</div>
</li>
<li><p>For standardized benchmarking, you can set fixed input and output token
lengths. Synthetic prompts provide consistent benchmarking runs, making it
easier to compare performance across different models or configurations.
Additionally, a controlled environment simplifies analysis.</p></li>
</ul>
<p>By balancing real-world data and synthetic data approaches, you can get a well-rounded understanding of model performance in varied scenarios.</p>
</section>
<section id="maximizing-vllm-instances-on-a-single-node">
<span id="mi300x-vllm-single-node"></span><h6>Maximizing vLLM instances on a single node<a class="headerlink" href="#maximizing-vllm-instances-on-a-single-node" title="Link to this heading">#</a></h6>
<p>The general guideline is to maximize per-node throughput by running as many vLLM instances as possible.
However, running too many instances might lead to insufficient memory for the KV-cache, which can affect performance.</p>
<p>The Instinct MI300X accelerator is equipped with 192GB of HBM3 memory capacity and bandwidth.
For models that fit in one GPU – to maximize the accumulated throughput – you can run as many as eight vLLM instances
simultaneously on one MI300X node (with eight GPUs). To do so, use the GPU isolation environment
variable <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code>.</p>
<p>For example, this script runs eight instances of vLLM for throughput benchmarking at the same time
with a model that can fit in one GPU:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="k">for</span><span class="w"> </span>i<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="k">$(</span>seq<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">7</span><span class="k">)</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">    </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="s2">"</span><span class="nv">$i</span><span class="s2">"</span><span class="w"> </span>python3<span class="w"> </span>/app/vllm/benchmarks/benchmark_throughput.py<span class="w"> </span>-tp<span class="w"> </span><span class="m">1</span><span class="w"> </span>--dataset<span class="w"> </span><span class="s2">"/path/to/dataset/ShareGPT_V3_unfiltered_cleaned_split.json"</span><span class="w"> </span>--model<span class="w"> </span>/path/to/model<span class="w"> </span><span class="p">&amp;</span>
<span class="k">done</span>
</pre></div>
</div>
<p>The total throughput achieved by running <code class="docutils literal notranslate"><span class="pre">N</span></code> instances of vLLM is generally much higher than running a
single vLLM instance across <code class="docutils literal notranslate"><span class="pre">N</span></code> GPUs simultaneously (that is, configuring <code class="docutils literal notranslate"><span class="pre">tensor_parallel_size</span></code> as N or
using the <code class="docutils literal notranslate"><span class="pre">-tp</span></code> N option, where <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">&lt;</span> <span class="pre">N</span> <span class="pre">≤</span> <span class="pre">8</span></code>).</p>
<p>vLLM on MI300X accelerators can run a variety of model weights, including Llama 2 (7b, 13b, 70b), Llama 3 (8b, 70b), Qwen2 (7b, 72b), Mixtral-8x7b, Mixtral-8x22b, and so on.
Notable configurations include Llama2-70b and Llama3-70b models on a single MI300X GPU, and the Llama3.1 405b model can fit on one single node with 8 MI300X GPUs.</p>
</section>
<section id="configure-the-gpu-memory-utilization-parameter">
<span id="mi300x-vllm-gpu-memory-utilization"></span><h6>Configure the gpu_memory_utilization parameter<a class="headerlink" href="#configure-the-gpu-memory-utilization-parameter" title="Link to this heading">#</a></h6>
<p>There are two ways to increase throughput by configuring <code class="docutils literal notranslate"><span class="pre">gpu-memory-utilization</span></code> parameter.</p>
<ol class="arabic">
<li><p>Increase <code class="docutils literal notranslate"><span class="pre">gpu-memory-utilization</span></code> to improve the throughput for a single instance as long as
it does not incur HIP or CUDA Out Of Memory. The default <code class="docutils literal notranslate"><span class="pre">gpu-memory-utilization</span></code> is 0.9.
You can set it to <code class="docutils literal notranslate"><span class="pre">&gt;0.9</span></code> and <code class="docutils literal notranslate"><span class="pre">&lt;1</span></code>.</p>
<p>For example, below benchmarking command set the <code class="docutils literal notranslate"><span class="pre">gpu-memory-utilization</span></code> as 0.98, or 98%.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>/vllm-workspace/benchmarks/benchmark_throughput.py<span class="w"> </span>--gpu-memory-utilization<span class="w"> </span><span class="m">0</span>.98<span class="w"> </span>--input-len<span class="w"> </span><span class="m">1024</span><span class="w"> </span>--output-len<span class="w"> </span><span class="m">128</span><span class="w"> </span>--model<span class="w"> </span>/path/to/model
</pre></div>
</div>
</li>
<li><p>Decrease <code class="docutils literal notranslate"><span class="pre">gpu-memory-utilization</span></code> to maximize the number of vLLM instances on the same GPU.</p>
<p>Specify GPU memory utilization to run as many instances of vLLM as possible on a single
GPU. However, too many instances can result in no memory for KV-cache. For small models, run
multiple instances of vLLM on the same GPU by specifying a smaller <code class="docutils literal notranslate"><span class="pre">gpu-memory-utilization</span></code> – as
long as it would not cause HIP Out Of Memory.</p>
<p>For example, run two instances of the Llama3-8b model at the same time on a single GPU by specifying
<code class="docutils literal notranslate"><span class="pre">--gpu-memory-utilization</span></code> to 0.4 (40%) as follows (on GPU <code class="docutils literal notranslate"><span class="pre">0</span></code>):</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>python3<span class="w"> </span>/vllm-workspace/benchmarks/benchmark_throughput.py<span class="w"> </span>--gpu-memory-utilization<span class="w"> </span><span class="m">0</span>.4
--dataset<span class="w"> </span><span class="s2">"/path/to/dataset/ShareGPT_V3_unfiltered_cleaned_split.json"</span><span class="w"> </span>--model<span class="w"> </span>/path/to/model<span class="w"> </span><span class="p">&amp;</span>

<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>python3<span class="w"> </span>/vllm-workspace/benchmarks/benchmark_throughput.py<span class="w"> </span>--gpu-memory-utilization<span class="w"> </span><span class="m">0</span>.4
--dataset<span class="w"> </span><span class="s2">"/path/to/dataset/ShareGPT_V3_unfiltered_cleaned_split.json"</span><span class="w"> </span>--model<span class="w"> </span>/path/to/model<span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
</li>
</ol>
<p>See <a class="reference internal" href="#vllm-engine-args"><span class="std std-ref">vLLM engine arguments</span></a> for other performance suggestions.</p>
</section>
<section id="run-vllm-on-multiple-gpus">
<span id="mi300x-vllm-multiple-gpus"></span><h6>Run vLLM on multiple GPUs<a class="headerlink" href="#run-vllm-on-multiple-gpus" title="Link to this heading">#</a></h6>
<p>The two main reasons to use multiple GPUs are:</p>
<ul class="simple">
<li><p>The model size is too big to run vLLM using one GPU as it results HIP Out of Memory.</p></li>
<li><p>To achieve better latency when using a single GPU is not desirable.</p></li>
</ul>
<p>To run one vLLM instance on multiple GPUs, use the <code class="docutils literal notranslate"><span class="pre">-tp</span></code> or <code class="docutils literal notranslate"><span class="pre">--tensor-parallel-size</span></code> option to
specify multiple GPUs. Optionally, use the <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> environment variable to specify
the GPUs.</p>
<p>For example, you can use two GPUs to start an API server on port 8000:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>vllm.entrypoints.api_server<span class="w"> </span>--model<span class="w"> </span>/path/to/model<span class="w"> </span>--dtype
float16<span class="w"> </span>-tp<span class="w"> </span><span class="m">2</span><span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
<p>To achieve both latency and throughput performance for serving, you can run multiple API servers on
different GPUs by specifying different ports for each server and use <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> to
specify the GPUs for each server, for example:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>vllm.entrypoints.api_server<span class="w"> </span>--model
/path/to/model<span class="w"> </span>--dtype<span class="w"> </span>float16<span class="w"> </span>-tp<span class="w"> </span><span class="m">2</span><span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="p">&amp;</span>

<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">2</span>,3<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>vllm.entrypoints.api_server<span class="w"> </span>--model
/path/to/model<span class="w"> </span>--dtype<span class="w"> </span>float16<span class="w"> </span>-tp<span class="w"> </span><span class="m">2</span><span class="w"> </span>--port<span class="w"> </span><span class="m">8001</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
</section>
<section id="choose-an-attention-backend">
<h6>Choose an attention backend<a class="headerlink" href="#choose-an-attention-backend" title="Link to this heading">#</a></h6>
<p>vLLM on ROCm supports two attention backends, each suitable for different use cases and performance
requirements:</p>
<ul class="simple">
<li><p><strong>Triton Flash Attention</strong> - For benchmarking, run vLLM scripts at
least once as a warm-up step so Triton can perform auto-tuning before
collecting benchmarking numbers. This is the default setting.</p></li>
<li><p><strong>Composable Kernel (CK) Flash Attention</strong> - To use CK Flash Attention, specify
the environment variable as <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">VLLM_USE_TRITON_FLASH_ATTN=0</span></code>.</p></li>
</ul>
<p>Refer to <a class="reference internal" href="#acceleration-flash-attention"><span class="std std-ref">Model acceleration libraries</span></a>
to learn more about Flash Attention with Triton or CK backends.</p>
</section>
<section id="vllm-engine-arguments">
<span id="vllm-engine-args"></span><h6>vLLM engine arguments<a class="headerlink" href="#vllm-engine-arguments" title="Link to this heading">#</a></h6>
<p>The following are configuration suggestions to potentially improve performance with vLLM. See
<a class="reference external" href="https://docs.vllm.ai/en/latest/serving/engine_args.html">vLLM’s engine arguments documentation</a>
for a full list of configurable engine arguments.</p>
<section id="configure-the-max-num-seqs-parameter">
<h6 aria-level="7">Configure the max-num-seqs parameter<a class="headerlink" href="#configure-the-max-num-seqs-parameter" title="Link to this heading">#</a></h6>
<p>Increase the <code class="docutils literal notranslate"><span class="pre">max-num-seqs</span></code> parameter from the default <code class="docutils literal notranslate"><span class="pre">256</span></code> to <code class="docutils literal notranslate"><span class="pre">512</span></code> (<code class="docutils literal notranslate"><span class="pre">--max-num-seqs</span>
<span class="pre">512</span></code>). This increases the maximum number of sequences per iteration and can improve throughput.</p>
</section>
<section id="use-the-float16-dtype">
<h6 aria-level="7">Use the float16 dtype<a class="headerlink" href="#use-the-float16-dtype" title="Link to this heading">#</a></h6>
<p>The default data type (<code class="docutils literal notranslate"><span class="pre">dtype</span></code>) is specified in the model’s configuration file. For instance, some models use <code class="docutils literal notranslate"><span class="pre">torch.bfloat16</span></code> as their default <code class="docutils literal notranslate"><span class="pre">dtype</span></code>.
Use float16 (<code class="docutils literal notranslate"><span class="pre">--dtype</span> <span class="pre">float16</span></code>) for better performance.</p>
</section>
<section id="multi-step-scheduling">
<h6 aria-level="7">Multi-step scheduling<a class="headerlink" href="#multi-step-scheduling" title="Link to this heading">#</a></h6>
<p>Setting <code class="docutils literal notranslate"><span class="pre">num-scheduler-steps</span></code> for multi-step scheduling can increase performance. Set it between 10 to 15 (<code class="docutils literal notranslate"><span class="pre">--num-scheduler-steps</span> <span class="pre">10</span></code>).</p>
</section>
<section id="distributed-executor-backend">
<h6 aria-level="7">Distributed executor backend<a class="headerlink" href="#distributed-executor-backend" title="Link to this heading">#</a></h6>
<p>The vLLM supports two modes of distributed executor backend: <code class="docutils literal notranslate"><span class="pre">ray</span></code> and <code class="docutils literal notranslate"><span class="pre">mp</span></code>. When using the <a class="github reference external" href="https://github.com/ROCm/vllm">ROCm/vllm</a> fork, using the <code class="docutils literal notranslate"><span class="pre">mp</span></code>
backend (<code class="docutils literal notranslate"><span class="pre">--distributed_executor_backend</span> <span class="pre">mp</span></code>) is recommended.</p>
</section>
<section id="graph-mode-max-seq-len-to-capture">
<h6 aria-level="7">Graph mode max-seq-len-to-capture<a class="headerlink" href="#graph-mode-max-seq-len-to-capture" title="Link to this heading">#</a></h6>
<p>Maximum sequence length covered by CUDA graphs. In the default mode (where <code class="docutils literal notranslate"><span class="pre">enforce_eager</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>), when a sequence has context length
larger than this, vLLM engine falls back to eager mode. The default is 8192.</p>
<p>When working with models that support long context lengths, set the parameter <code class="docutils literal notranslate"><span class="pre">--max-seq-len-to-capture</span></code> to 16384.
See this <a class="reference external" href="https://blog.vllm.ai/2024/10/23/vllm-serving-amd.html">vLLM blog</a> for details.</p>
<p>An example of long context length model is Qwen2-7b.</p>
</section>
<section id="whether-to-enable-chunked-prefill">
<h6 aria-level="7">Whether to enable chunked prefill<a class="headerlink" href="#whether-to-enable-chunked-prefill" title="Link to this heading">#</a></h6>
<p>Another vLLM performance tip is to enable chunked prefill to improve
throughput. Chunked prefill allows large prefills to be chunked into
smaller chunks and batched together with decode requests.</p>
<p>You can enable the feature by specifying <code class="docutils literal notranslate"><span class="pre">--enable-chunked-prefill</span></code> in the
command line or setting <code class="docutils literal notranslate"><span class="pre">enable_chunked_prefill=True</span></code> in the LLM
constructor.</p>
<p>As stated in <a class="reference external" href="https://docs.vllm.ai/en/latest/models/performance.html#chunked-prefill">vLLM’s documentation,</a>,
you can tune the performance by changing <code class="docutils literal notranslate"><span class="pre">max_num_batched_tokens</span></code>. By
default, it is set to 512 and optimized for ITL (inter-token latency).
Smaller <code class="docutils literal notranslate"><span class="pre">max_num_batched_tokens</span></code> achieves better ITL because there are
fewer prefills interrupting decodes.
Higher <code class="docutils literal notranslate"><span class="pre">max_num_batched_tokens</span></code> achieves better TTFT (time to the first
token) as you can put more prefill to the batch.</p>
<p>You might experience noticeable throughput improvements when
benchmarking on a single GPU or 8 GPUs using the vLLM throughput
benchmarking script along with the ShareGPT dataset as input.</p>
<p>In the case of fixed <code class="docutils literal notranslate"><span class="pre">input-len</span></code>/<code class="docutils literal notranslate"><span class="pre">output-len</span></code>, for some configurations,
enabling chunked prefill increases the throughput. For some other
configurations, the throughput may be worse and elicit a need to tune
parameter <code class="docutils literal notranslate"><span class="pre">max_num_batched_tokens</span></code> (for example, increasing <code class="docutils literal notranslate"><span class="pre">max_num_batched_tokens</span></code> value to 4096 or larger).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Chunked prefill is no longer recommended. See the vLLM blog: <a class="reference external" href="https://blog.vllm.ai/2024/10/23/vllm-serving-amd.html">Serving LLMs on AMD MI300X: Best Practices</a> (October 2024).</p>
</div>
</section>
</section>
<section id="quantization-support">
<h6>Quantization support<a class="headerlink" href="#quantization-support" title="Link to this heading">#</a></h6>
<p>Quantization reduces the precision of the model’s weights and activations, which significantly decreases the memory footprint.
<code class="docutils literal notranslate"><span class="pre">fp8(w8a8)</span></code> and <code class="docutils literal notranslate"><span class="pre">AWQ</span></code> quantization are supported for ROCm.</p>
<section id="fp8-quantization">
<h6 aria-level="7">FP8 quantization<a class="headerlink" href="#fp8-quantization" title="Link to this heading">#</a></h6>
<p><a class="github reference external" href="https://github.com/ROCm/vllm">ROCm/vllm</a> supports FP8 (8-bit floating point) weight and activation quantization using hardware acceleration on the Instinct MI300X.
Quantization of models with FP8 allows for a 2x reduction in model memory requirements and up to a 1.6x improvement in throughput with minimal impact on accuracy.</p>
<p>AMD publishes Quark Quantized OCP FP8 models on Hugging Face. For example:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/amd/Llama-3.1-8B-Instruct-FP8-KV">Llama-3.1-8B-Instruct-FP8-KV</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/amd/Llama-3.1-70B-Instruct-FP8-KV">Llama-3.1-70B-Instruct-FP8-KV</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/amd/Llama-3.1-405B-Instruct-FP8-KV">Llama-3.1-405B-Instruct-FP8-KV</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/amd/Mixtral-8x7B-Instruct-v0.1-FP8-KV">Mixtral-8x7B-Instruct-v0.1-FP8-KV</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/amd/Mixtral-8x22B-Instruct-v0.1-FP8-KV">Mixtral-8x22B-Instruct-v0.1-FP8-KV</a></p></li>
</ul>
<p>To enable vLLM benchmarking to run on fp8 quantized models, use the <code class="docutils literal notranslate"><span class="pre">--quantization</span></code> parameter with value <code class="docutils literal notranslate"><span class="pre">fp8</span></code> (<code class="docutils literal notranslate"><span class="pre">--quantization</span> <span class="pre">fp8</span></code>).</p>
</section>
<section id="awq-quantization">
<h6 aria-level="7">AWQ quantization<a class="headerlink" href="#awq-quantization" title="Link to this heading">#</a></h6>
<p>You can quantize your own models by installing AutoAWQ or picking one of the 400+ models on Hugging Face. Be aware that
that AWQ support in vLLM is currently underoptimized.</p>
<p>To enable vLLM to run on <code class="docutils literal notranslate"><span class="pre">awq</span></code> quantized models, using <code class="docutils literal notranslate"><span class="pre">--quantization</span></code> parameter with <code class="docutils literal notranslate"><span class="pre">awq</span></code> (<code class="docutils literal notranslate"><span class="pre">--quantization</span> <span class="pre">awq</span></code>).</p>
<p>You can find more specifics in the <a class="reference external" href="https://docs.vllm.ai/en/stable/quantization/auto_awq.html">vLLM AutoAWQ documentation</a>.</p>
</section>
<section id="fp8-kv-cached-dtype">
<h6 aria-level="7">fp8 kv-cached-dtype<a class="headerlink" href="#fp8-kv-cached-dtype" title="Link to this heading">#</a></h6>
<p>Using <code class="docutils literal notranslate"><span class="pre">fp8</span> <span class="pre">kv-cache</span> <span class="pre">dtype</span></code> can improve performance as it reduces the size
of <code class="docutils literal notranslate"><span class="pre">kv-cache</span></code>. As a result, it reduces the cost required for reading and
writing the <code class="docutils literal notranslate"><span class="pre">kv-cache</span></code>.</p>
<p>To use this feature, specify <code class="docutils literal notranslate"><span class="pre">--kv-cache-dtype</span></code> as <code class="docutils literal notranslate"><span class="pre">fp8</span></code>.</p>
<p>To specify the quantization scaling config, use the
<code class="docutils literal notranslate"><span class="pre">--quantization-param-path</span></code> parameter. If the parameter is not specified,
the default scaling factor of <code class="docutils literal notranslate"><span class="pre">1</span></code> is used, which can lead to less accurate
results. To generate <code class="docutils literal notranslate"><span class="pre">kv-cache</span></code> scaling JSON file, see <a class="reference external" href="https://github.com/vllm-project/llm-compressor/blob/main/examples/quantization_kv_cache/README.md">FP8 KV
Cache</a>
in the vLLM GitHub repository.</p>
<p>Two sample Llama scaling configuration files are in vLLM for <code class="docutils literal notranslate"><span class="pre">llama2-70b</span></code> and
<code class="docutils literal notranslate"><span class="pre">llama2-7b</span></code>.</p>
<p>If building the vLLM using
<a class="reference external" href="https://github.com/vllm-project/vllm/blob/main/docker/Dockerfile.rocm">Dockerfile.rocm</a>
for <code class="docutils literal notranslate"><span class="pre">llama2-70b</span></code> scale config, find the file at
<code class="docutils literal notranslate"><span class="pre">/vllm-workspace/tests/fp8_kv/llama2-70b-fp8-kv/kv_cache_scales.json</span></code> at
runtime.</p>
<p>Below is a sample command to run benchmarking with this feature enabled
for the <code class="docutils literal notranslate"><span class="pre">llama2-70b</span></code> model:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>/vllm-workspace/benchmarks/benchmark_throughput.py<span class="w"> </span>--model<span class="w"> </span><span class="se">\</span>
/path/to/llama2-70b-model<span class="w"> </span>--kv-cache-dtype<span class="w"> </span><span class="s2">"fp8"</span><span class="w"> </span><span class="se">\</span>
--quantization-param-path<span class="w"> </span><span class="se">\</span>
<span class="s2">"/vllm-workspace/tests/fp8_kv/llama2-70b-fp8-kv/kv_cache_scales.json"</span><span class="w"> </span><span class="se">\</span>
--input-len<span class="w"> </span><span class="m">512</span><span class="w"> </span>--output-len<span class="w"> </span><span class="m">256</span><span class="w"> </span>--num-prompts<span class="w"> </span><span class="m">500</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="pytorch-tunableop">
<span id="mi300x-tunableop"></span><h5>PyTorch TunableOp<a class="headerlink" href="#pytorch-tunableop" title="Link to this heading">#</a></h5>
<p><a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/cuda/tunable/README.md">TunableOp</a>
is a feature used to obtain the optimal GPU kernel for a key PyTorch operations. At the moment,
TunableOp supports the tuning of dense matrix multiplies (GEMM, batched GEMM, GEMM and bias, and scaled GEMM).
This feature is useful for squeezing out the last bit of performance.
In short, it will try up to thousands of matrix multiply algorithms that are available in rocBLAS and hipBLASLt.
A caveat is that as the math libraries improve over time, there is a less benefit to using TunableOp,
and there is also no guarantee that the workload being tuned will be able to outperform the default GEMM algorithm in hipBLASLt.</p>
<p>Some additional references for PyTorch TunableOp include <a class="reference external" href="https://rocm.blogs.amd.com/artificial-intelligence/pytorch-tunableop/README.html">ROCm blog</a>,
TunableOp <a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/cuda/tunable/README.md">README</a>, and
<a class="reference external" href="https://rocm.docs.amd.com/en/latest/how-to/llm-fine-tuning-optimization/model-acceleration-libraries.html#fine-tuning-llms-pytorch-tunableop">llm tuning</a>.</p>
<p>The three most important environment variables for controlling TunableOp are:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">PYTORCH_TUNABLEOP_ENABLED</span></code></dt><dd><p>The main on/off switch for all TunableOp implementations. Default is <code class="docutils literal notranslate"><span class="pre">0</span></code> (disabled). Set to <code class="docutils literal notranslate"><span class="pre">1</span></code> to enable.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">PYTORCH_TUNABLEOP_TUNING</span></code></dt><dd><p>When enabled, if a tuned entry isn’t found, runs the tuning step and records the entry. Default is <code class="docutils literal notranslate"><span class="pre">1</span></code> (enabled). Set to <code class="docutils literal notranslate"><span class="pre">0</span></code> to disable.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">PYTORCH_TUNABLEOP_VERBOSE</span></code></dt><dd><p>Enables verbose output for debugging purposes – it can be useful to see if TunableOp is being used at all. Default is <code class="docutils literal notranslate"><span class="pre">0</span></code> (disabled). Set to <code class="docutils literal notranslate"><span class="pre">1</span></code> to enable.</p>
</dd>
</dl>
<p>For the complete list of environment variables, see the
TunableOp <a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/cuda/tunable/README.md">README</a>.
There are also Python APIs to set some of these environment variables,
but the preferred way to set the TunableOp tuning parameters is to use the environment variables.</p>
<section id="workflow">
<h6>Workflow<a class="headerlink" href="#workflow" title="Link to this heading">#</a></h6>
<p>Use these environment variables to enable TunableOp for any applications or libraries that use PyTorch (2.3 or later).</p>
<p>The first step is the tuning pass:</p>
<ol class="arabic">
<li><p>Enable TunableOp and tuning. Optionally enable verbose mode:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">PYTORCH_TUNABLEOP_ENABLED</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">PYTORCH_TUNABLEOP_VERBOSE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>your_script.sh
</pre></div>
</div>
<p>This pass can be very slow. The output will be the <code class="docutils literal notranslate"><span class="pre">tunableop_results.csv</span></code> file containing a list of GEMMs encountered
and the optimal GPU kernel that was identified.</p>
<p>Multi-GPU tuning is supported, producing a separate tunableop_results.csv file for each GPU. The
tuning algorithm executes independently on each GPU, with each tuning process sandboxed to its
respective GPU. There is no inter-GPU communication during tuning.</p>
<p>For data-parallel algorithms, where GEMM configurations across GPUs are typically identical, this
approach can result in redundant work. In such cases, running the workload on a single GPU might
suffice. However, for algorithms involving multiple levels of parallelism (as in data parallelism
combined with ML model parallelism), different GPUs might require distinct GEMM parameters. In
these scenarios, a multi-GPU configuration is recommended.</p>
</li>
</ol>
<p>In the second step, we re-run the workload with optimal configuration using the <code class="docutils literal notranslate"><span class="pre">tunableop_results.csv</span></code> file obtained in step 1.</p>
<ol class="arabic" start="2">
<li><p>Enable TunableOp, disable tuning, and measure:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">PYTORCH_TUNABLEOP_ENABLED</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">PYTORCH_TUNABLEOP_TUNING</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>your_script.sh
</pre></div>
</div>
</li>
</ol>
<p>Compare the wall-clock time from this second step to your reference wall-clock time with TunableOp completely disabled (<code class="docutils literal notranslate"><span class="pre">PYTORCH_TUNABLEOP_ENABLED=0</span></code>).</p>
</section>
<section id="offline-tuning">
<h6>Offline tuning<a class="headerlink" href="#offline-tuning" title="Link to this heading">#</a></h6>
<p>A new feature of TunableOp, offline tuning, is available in upstream PyTorch and supported in PyTorch 2.6 or later.</p>
<p>Traditionally, tuning is performed in-place during workload execution. While convenient for one-off
tuning, this approach can become cumbersome if frequent re-tuning is required – such as when a new
version of a math library is released. In these cases, re-running the workload and performing tuning
repeatedly can be inefficient.</p>
<p>Offline tuning addresses this challenge by decoupling the tuning process from workload execution. It
enables the collection of GEMMs from a workload during a collection pass, followed by tuning these
GEMMs in a separate tuning pass, without re-running the original workload. This approach
significantly reduces compute resource requirements, particularly for time-intensive workloads.</p>
<p>For workflow instructions, refer to the <a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/cuda/tunable/README.md#offline-tuning">Offline Tuning documentation</a>.</p>
</section>
</section>
<section id="pytorch-inductor-max-autotune-tuning-knobs">
<span id="mi300x-torchinductor-tuning"></span><h5>PyTorch inductor max-autotune tuning knobs<a class="headerlink" href="#pytorch-inductor-max-autotune-tuning-knobs" title="Link to this heading">#</a></h5>
<p>The following are suggestions for optimizing matrix multiplication (GEMM) and
convolution (<code class="docutils literal notranslate"><span class="pre">conv</span></code>) operations in PyTorch using <code class="docutils literal notranslate"><span class="pre">inductor</span></code>, a part of the
PyTorch compilation framework.</p>
<p>Learn more about TorchInductor environment variables and usage in the
<a class="reference external" href="https://pytorch.org/docs/2.3/torch.compiler_inductor_profiling.html">PyTorch documentation</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Triton is not used if regular <a class="reference external" href="https://rocm.docs.amd.com/projects/MIOpen/en/latest/index.html" title="(in MIOpen Documentation v3.4.0)"><span class="xref std std-doc">MIOpen</span></a> or
<a class="reference external" href="https://rocm.docs.amd.com/projects/rocBLAS/en/latest/index.html" title="(in rocBLAS Documentation v4.4.1)"><span class="xref std std-doc">rocBLAS</span></a> performs faster for a specific operation.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Experimental: TunableOp (see the <a class="reference internal" href="#mi300x-tunableop"><span class="std std-ref">PyTorch TunableOp</span></a> section) can also be used in combination
with <code class="docutils literal notranslate"><span class="pre">TorchInductor</span></code> <code class="docutils literal notranslate"><span class="pre">max-autotune</span></code> mode to boost ATen GEMM performance but will further increase tuning time.
The environment variable <code class="docutils literal notranslate"><span class="pre">TORCHINDUCTOR_AUTOTUNE_MULTI_DEVICE=1</span></code> can be useful in single GPU workloads to distribute Triton GEMM tuning.</p>
</div>
<section id="triton-backend">
<h6>Triton backend<a class="headerlink" href="#triton-backend" title="Link to this heading">#</a></h6>
<p>The goal is to leverage Triton to achieve better performance. To tune Triton kernels with <code class="docutils literal notranslate"><span class="pre">gemm</span></code> and convolution ops (<code class="docutils literal notranslate"><span class="pre">conv</span></code>), use the
<code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> function with the <code class="docutils literal notranslate"><span class="pre">max-autotune</span></code> mode. This benchmarks a
predefined list of Triton configurations and selects the fastest one for each
shape. See the configurations in PyTorch source code:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/pytorch/pytorch/blob/a1d02b423c6b4ccacd25ebe86de43f650463bbc6/torch/_inductor/kernel/conv.py#L51">conv configurations for “max-autotune”</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/pytorch/blob/a1d02b423c6b4ccacd25ebe86de43f650463bbc6/torch/_inductor/kernel/mm_common.py#L118">matmul configurations for “max-autotune”</a></p></li>
</ul>
<p>This tuning will select the best Triton <code class="docutils literal notranslate"><span class="pre">gemm</span></code> configurations according to tile-size
<code class="docutils literal notranslate"><span class="pre">(BLOCK_M,</span> <span class="pre">BLOCK_N,</span> <span class="pre">BLOCK_K),</span> <span class="pre">num_stages,</span> <span class="pre">num_warps</span></code> and <code class="docutils literal notranslate"><span class="pre">mfma</span></code> instruction size ( <code class="docutils literal notranslate"><span class="pre">matrix_instr_nonkdim</span></code> )
(see “Triton kernel optimization” section for more details).</p>
<ul>
<li><p>Set <code class="docutils literal notranslate"><span class="pre">torch._inductor.config.max_autotune</span> <span class="pre">=</span> <span class="pre">True</span></code> or <code class="docutils literal notranslate"><span class="pre">TORCHINDUCTOR_MAX_AUTOTUNE=1</span></code>.</p></li>
<li><p>Or, for more fine-grained control:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch._inductor.config.max_autotune_gemm</span> <span class="pre">=</span> <span class="pre">True</span></code></dt><dd><p>To enable tuning or lowering of <code class="docutils literal notranslate"><span class="pre">mm</span></code>/<code class="docutils literal notranslate"><span class="pre">conv</span></code>s.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">torch._inductor.config.max_autotune.pointwise</span> <span class="pre">=</span> <span class="pre">True</span></code></dt><dd><p>To enable tuning for <code class="docutils literal notranslate"><span class="pre">pointwise</span></code>/<code class="docutils literal notranslate"><span class="pre">reduction</span></code> ops.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">torch._inductor.max_autotune_gemm_backends</span></code> or <code class="docutils literal notranslate"><span class="pre">TORCHINDUCTOR_MAX_AUTOTUNE_GEMM_BACKENDS</span></code></dt><dd><p>Selects the candidate backends for <code class="docutils literal notranslate"><span class="pre">mm</span></code> auto-tuning. Defaults to
<code class="docutils literal notranslate"><span class="pre">TRITON,ATEN</span></code>.
Limiting this to <code class="docutils literal notranslate"><span class="pre">TRITON</span></code> might improve performance by
enabling more fused <code class="docutils literal notranslate"><span class="pre">mm</span></code> kernels instead of going to rocBLAS.</p>
</dd>
</dl>
</li>
<li><p>Inference can see large improvements on AMD GPUs by utilizing
<code class="docutils literal notranslate"><span class="pre">torch._inductor.config.freezing=True</span></code> or the <code class="docutils literal notranslate"><span class="pre">TORCHINDUCTOR_FREEZING=1</span></code> variable, which
in-lines weights as constants and enables constant folding optimizations.</p></li>
<li><p>Enabling <code class="docutils literal notranslate"><span class="pre">inductor</span></code>’s cpp_wrapper might improve overhead. This generates
C++ code which launches Triton binaries directly with
<code class="docutils literal notranslate"><span class="pre">hipModuleLaunchKernel</span></code> and relies on <cite>hipification</cite>.</p>
<p><code class="docutils literal notranslate"><span class="pre">torch._inductor.config.cpp_wrapper=True</span></code> or <code class="docutils literal notranslate"><span class="pre">TORCHINDUCTOR_CPP_WRAPPER=1</span></code></p>
</li>
<li><p>Convolution workloads might see a performance benefit by specifying
<code class="docutils literal notranslate"><span class="pre">torch._inductor.config.layout_optimization=True</span></code> or <code class="docutils literal notranslate"><span class="pre">TORCHINDUCTOR_LAYOUT_OPTIMIZATION=1</span></code>.
This can help performance by enforcing <code class="docutils literal notranslate"><span class="pre">channel_last</span></code> memory format on the
convolution in TorchInductor, avoiding any unnecessary transpose operations.
Note that <code class="docutils literal notranslate"><span class="pre">PYTORCH_MIOPEN_SUGGEST_NHWC=1</span></code> is recommended if using this.</p></li>
<li><p>To extract the Triton kernels generated by <code class="docutils literal notranslate"><span class="pre">inductor</span></code>, set the environment variable
<code class="docutils literal notranslate"><span class="pre">TORCH_COMPILE_DEBUG=1</span></code>, which will create a <code class="docutils literal notranslate"><span class="pre">torch_compile_debug/</span></code> directory
in the current path. The wrapper codes generated by <code class="docutils literal notranslate"><span class="pre">inductor</span></code> are in one or more
<code class="docutils literal notranslate"><span class="pre">output_code.py</span></code> files corresponding to the FX graphs associated with the model.
The Triton kernels are defined in these generated codes.</p></li>
</ul>
</section>
<section id="composable-kernel-backend">
<h6>Composable Kernel backend<a class="headerlink" href="#composable-kernel-backend" title="Link to this heading">#</a></h6>
<p>You can enable the Composable Kernel (<code class="docutils literal notranslate"><span class="pre">CK</span></code>) backend by appending <code class="docutils literal notranslate"><span class="pre">CK</span></code> to the comma-separated list of backends. This allows the
auto-tuning process to use kernels from the Composable Kernel library.</p>
<p><code class="docutils literal notranslate"><span class="pre">torch._inductor.max_autotune_gemm_backends</span></code> or <code class="docutils literal notranslate"><span class="pre">TORCHINDUCTOR_MAX_AUTOTUNE_GEMM_BACKENDS</span></code>.</p>
<p>Install the Composable Kernel library’s Python wrapper via pip using the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>git+https://github.com/rocm/composable_kernel@develop
</pre></div>
</div>
<p>This wrapper library is responsible for constructing a list of kernel instances available in the Composable Kernel library,
as well as storing the kernel instance C++ includes in a known location (so clang can look into these paths when compiling the <code class="docutils literal notranslate"><span class="pre">gemm</span></code> auto-tune candidates).</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">matmul</span></code> (with <code class="docutils literal notranslate"><span class="pre">float16</span></code> and <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> inputs, row-major X, row-major or column-major W)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">addmm</span></code> (with <code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> X, W and Bias; row-major X, row-major or column-major W; Bias can be broadcast either along row-major or column-major dimension)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scaled_mm</span></code> (<code class="docutils literal notranslate"><span class="pre">float8_e4m3fnuz</span></code> inputs, <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> output)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">conv2d</span></code> (with <code class="docutils literal notranslate"><span class="pre">float32</span></code>, <code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> inputs, channels-last weight layout)</p></li>
</ul>
</div></blockquote>
<ul class="simple">
<li><p>For working examples, see <a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/test/inductor/test_ck_backend.py">test/inductor/test_ck_backend.py</a>.</p></li>
<li><p>Compiling or build time can be configured by modifying <code class="docutils literal notranslate"><span class="pre">torch._inductor.config</span></code> to reduce the build time to avoid time-out.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">compile_threads</span></code>: Number of threads used for compilation. Set it to the number of available CPU cores.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rocm.n_max_profiling_configs</span></code>: Limiting the number of kernels to speed up compilation.</p></li>
</ul>
</li>
<li><p>Setting environment variable <code class="docutils literal notranslate"><span class="pre">PYTORCH_MIOPEN_SUGGEST_NHWC=1</span></code> to optimize convolution operations.</p></li>
</ul>
<p>Debugging and troubleshooting performance:</p>
<ul class="simple">
<li><p>Generate a standalone executable runner to debug or assess kernels’ performance by setting environment variable
<code class="docutils literal notranslate"><span class="pre">INDUCTOR_CK_BACKEND_GENERATE_TEST_RUNNER_CODE=1</span></code> to facilitate debugging and profiling. By default,
the CK backend will not build a standalone executable runner.</p></li>
<li><p>Enable debug by passing compilation flags (e.g., <code class="docutils literal notranslate"><span class="pre">is_debug</span></code>) to clang when compiling the kernels in <code class="docutils literal notranslate"><span class="pre">torch._inductor.config.rocm</span></code> class.</p></li>
<li><p>The generated source files and other products of clang compilation are located in the torch inductor root directory (default: <code class="docutils literal notranslate"><span class="pre">/tmp/torchinductor_root</span></code>)</p></li>
</ul>
</section>
</section>
<section id="rocm-library-tuning">
<span id="mi300x-rocm-library-tuning"></span><h5>ROCm library tuning<a class="headerlink" href="#rocm-library-tuning" title="Link to this heading">#</a></h5>
<p>ROCm library tuning involves optimizing the performance of routine computational
operations (such as <code class="docutils literal notranslate"><span class="pre">GEMM</span></code>) provided by ROCm libraries like
<a class="reference internal" href="#mi300x-hipblaslt"><span class="std std-ref">hipBLASLt</span></a>, <a class="reference internal" href="#mi300x-ck"><span class="std std-ref">Composable Kernel</span></a>,
<a class="reference internal" href="#mi300x-miopen"><span class="std std-ref">MIOpen</span></a>, and <a class="reference internal" href="#mi300x-rccl"><span class="std std-ref">RCCL</span></a>. This tuning aims
to maximize efficiency and throughput on Instinct MI300X accelerators to gain
improved application performance.</p>
<section id="gemm-general-matrix-multiplication">
<span id="mi300x-library-gemm"></span><h6>GEMM (general matrix multiplication)<a class="headerlink" href="#gemm-general-matrix-multiplication" title="Link to this heading">#</a></h6>
<p>GEMMs (General Matrix Multiplications) are a fundamental building block for many operations in neural networks.
GEMM is defined as <code class="docutils literal notranslate"><span class="pre">C</span> <span class="pre">=</span> <span class="pre">αAB</span> <span class="pre">+</span> <span class="pre">βC</span></code> where A is an <code class="docutils literal notranslate"><span class="pre">MxK</span></code> matrix input and B is <code class="docutils literal notranslate"><span class="pre">KxN</span></code> matrix input,
and C is <code class="docutils literal notranslate"><span class="pre">MxN</span></code> matrix input and is overwritten by the output. α and β are scalar inputs.
hipBLASLt is a library that provides general matrix-matrix operations with a flexible API
and extends functionalities beyond a traditional BLAS library.</p>
<section id="hipblaslt-benchmarking">
<span id="mi300x-hipblaslt"></span><h6 aria-level="7">hipBLASLt benchmarking<a class="headerlink" href="#hipblaslt-benchmarking" title="Link to this heading">#</a></h6>
<p>The GEMM library
<a class="reference external" href="https://rocm.docs.amd.com/projects/hipBLASLt/en/latest/index.html">hipBLASLt</a>
provides a benchmark tool for its supported operations. Refer to the
<a class="reference external" href="https://github.com/ROCm/hipBLASLt/blob/develop/clients/benchmarks/README.md">documentation</a>
for details.</p>
<ul>
<li><p>Example 1: Benchmark mix fp8 GEMM</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">HIP_FORCE_DEV_KERNARG</span><span class="o">=</span><span class="m">1</span><span class="w">  </span>hipblaslt-bench<span class="w"> </span>--alpha<span class="w"> </span><span class="m">1</span><span class="w"> </span>--beta<span class="w"> </span><span class="m">0</span><span class="w"> </span>-r<span class="w"> </span>f16_r<span class="w"> </span><span class="se">\</span>
--a_type<span class="w"> </span>f16_r<span class="w"> </span>--b_type<span class="w"> </span>f8_r<span class="w"> </span>--compute_type<span class="w"> </span>f32_f16_r<span class="w"> </span><span class="se">\</span>
--initialization<span class="w"> </span>trig_float<span class="w">  </span>--cold_iters<span class="w"> </span><span class="m">100</span><span class="w"> </span>--iters<span class="w"> </span><span class="m">1000</span><span class="w"> </span>--rotating<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
</li>
<li><p>Example 2: Benchmark forward epilogues and backward epilogues</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_RELU: "--activation_type</span> <span class="pre">relu";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_BIAS: "--bias_vector";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_RELU_BIAS: "--activation_type</span> <span class="pre">relu</span> <span class="pre">--bias_vector";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_GELU: "--activation_type</span> <span class="pre">gelu";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_DGELU":</span> <span class="pre">--activation_type</span> <span class="pre">gelu</span> <span class="pre">--gradient";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_GELU_BIAS: "--activation_type</span> <span class="pre">gelu</span> <span class="pre">--bias_vector";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_GELU_AUX: "--activation_type</span> <span class="pre">gelu</span> <span class="pre">--use_e";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_GELU_AUX_BIAS: "--activation_type</span> <span class="pre">gelu</span> <span class="pre">--bias_vector</span> <span class="pre">--use_e";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_DGELU_BGRAD: "--activation_type</span> <span class="pre">gelu</span> <span class="pre">--bias_vector</span> <span class="pre">--gradient";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_BGRADA: "--bias_vector</span> <span class="pre">--gradient</span> <span class="pre">--bias_source</span> <span class="pre">a";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_BGRADB: </span> <span class="pre">"--bias_vector</span> <span class="pre">--gradient</span> <span class="pre">--bias_source</span> <span class="pre">b";</span></code></p></li>
</ul>
</li>
</ul>
</section>
<section id="hipblaslt-auto-tuning-using-hipblaslt-bench">
<h6 aria-level="7">hipBLASLt auto-tuning using hipblaslt-bench<a class="headerlink" href="#hipblaslt-auto-tuning-using-hipblaslt-bench" title="Link to this heading">#</a></h6>
<p>Use the auto-tuning tool in hipBLASLt to get the best solution for a given problem size.</p>
<section id="prerequisite">
<h6 aria-level="8">Prerequisite<a class="headerlink" href="#prerequisite" title="Link to this heading">#</a></h6>
<p>Build hipBLASLt.
See the <a class="reference external" href="https://github.com/ROCm/hipBLASLt">hipBLASLt repository</a> to see detailed build instructions.</p>
</section>
<section id="quick-start">
<h6 aria-level="8">Quick start<a class="headerlink" href="#quick-start" title="Link to this heading">#</a></h6>
<p>Create a working folder for the auto-tuning tool, for example, <code class="docutils literal notranslate"><span class="pre">tuning/</span></code>.</p>
<ol class="arabic simple">
<li><p>Set the <code class="docutils literal notranslate"><span class="pre">ProblemType</span></code>, <code class="docutils literal notranslate"><span class="pre">TestConfig</span></code>, and <code class="docutils literal notranslate"><span class="pre">TuningParameters</span></code> in the YAML file. You can modify the template YAML file in <code class="docutils literal notranslate"><span class="pre">hipblaslt/utilities</span></code>.</p></li>
</ol>
<figure class="align-center">
<img alt="HipBLASLt auto-tuning yaml file template" src="_images/hipblaslt_yaml_template.png"/>
</figure>
<ol class="arabic" start="2">
<li><p>Run the following command to start tuning.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># python3 hipblaslt/utilities/find_exact.py &lt;path-to-config-yaml&gt; &lt;path-to-the-root-of-built-hipblaslt&gt; &lt;working-directory&gt;</span>
<span class="c1"># Assume we're in folder tuning, the default root of the build folder of hipblaslt is hipblaslt/build/release</span>
python3<span class="w"> </span>../hipblaslt/utilities/find_exact.py<span class="w"> </span>tuning.yaml<span class="w"> </span>hipblaslt/build/release<span class="w"> </span>./
</pre></div>
</div>
</li>
</ol>
</section>
<section id="output">
<h6 aria-level="8">Output<a class="headerlink" href="#output" title="Link to this heading">#</a></h6>
<p>The tool will create two output folders. The first one is the benchmark results,
the second one is the generated equality kernels. If <code class="docutils literal notranslate"><span class="pre">SplitK</span></code> is used, the solution’s <code class="docutils literal notranslate"><span class="pre">GlobalSplitU</span></code> will
also change if the winner is using a different <code class="docutils literal notranslate"><span class="pre">SplitK</span></code> from the solution. The YAML files generated inside the
folder <code class="docutils literal notranslate"><span class="pre">1_LogicYaml</span></code> are logic ones. These YAML files are just like those generated from TensileLite.</p>
<figure class="align-center">
<img alt="HipBLASLt auto-tuning output folder" src="_images/hipblaslt_auto_tuning_output_files.png"/>
</figure>
</section>
<section id="a-quick-view-of-the-config-yaml">
<h6 aria-level="8">A quick view of the config YAML<a class="headerlink" href="#a-quick-view-of-the-config-yaml" title="Link to this heading">#</a></h6>
<p>The tuning tool is a two-step tool. It first runs the benchmark, then it creates the equality YAML for the user. Note that this config YAML file is different from the config YAML used in TensileLite.</p>
<ul>
<li><p><strong>Benchmarking</strong></p>
<p>The first step is to run the benchmark, <code class="docutils literal notranslate"><span class="pre">find_exact.py</span></code> will run the benchmark with <code class="docutils literal notranslate"><span class="pre">hipblaslt-bench</span></code>.
For the default configurations, see the Python file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">defaultBenchOptions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"ProblemType"</span><span class="p">:</span> <span class="p">{</span>
    <span class="s2">"TransposeA"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">"TransposeB"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">"ComputeInputDataType"</span><span class="p">:</span> <span class="s2">"s"</span><span class="p">,</span>
    <span class="s2">"ComputeDataType"</span><span class="p">:</span> <span class="s2">"s"</span><span class="p">,</span>
    <span class="s2">"DataTypeC"</span><span class="p">:</span> <span class="s2">"s"</span><span class="p">,</span>
    <span class="s2">"DataTypeD"</span><span class="p">:</span> <span class="s2">"s"</span><span class="p">,</span>
    <span class="s2">"UseBias"</span><span class="p">:</span> <span class="kc">False</span>
<span class="p">},</span> <span class="s2">"TestConfig"</span><span class="p">:</span> <span class="p">{</span>
    <span class="s2">"ColdIter"</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s2">"Iter"</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
    <span class="s2">"AlgoMethod"</span><span class="p">:</span> <span class="s2">"all"</span><span class="p">,</span>
    <span class="s2">"RequestedSolutions"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="c1"># Only works in AlgoMethod heuristic</span>
    <span class="s2">"SolutionIndex"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="c1"># Only works in AlgoMethod index</span>
    <span class="s2">"ApiMethod"</span><span class="p">:</span> <span class="s2">"cpp"</span><span class="p">,</span>
    <span class="s2">"RotatingBuffer"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">},</span> <span class="s2">"TuningParameters"</span><span class="p">:</span> <span class="p">{</span>
    <span class="s2">"SplitK"</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="p">},</span> <span class="s2">"ProblemSizes"</span><span class="p">:</span> <span class="p">[]}</span>
<span class="n">defaultCreateLogicOptions</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># Currently unused</span>
</pre></div>
</div>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">TestConfig</span></code></dt><dd><ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ColdIter</span></code>: This is number the warm-up iterations before starting the kernel benchmark.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Iter</span></code>: This is the number of iterations in kernel benchmarking</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">AlgoMethod</span></code>: We recommended to keep this unchanged because method “all” returns all the available solutions for the problem type.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ApiMethod</span></code>: We have c, mix, and cpp. Doesn’t affect the result much.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RotatingBuffer</span></code>: This is a size in the unit of MB. Recommended to set the value equal to the size of the cache of the card to avoid the kernel fetching data from the cache.</p></li>
</ol>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">TuningParameters</span></code></dt><dd><p><code class="docutils literal notranslate"><span class="pre">SplitK</span></code>: Divide <code class="docutils literal notranslate"><span class="pre">K</span></code> into <code class="docutils literal notranslate"><span class="pre">N</span></code> portions. Not every solution supports <code class="docutils literal notranslate"><span class="pre">SplitK</span></code>.
The solution will be skipped if not supported.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">CreateLogic</span></code></dt><dd><p>Currently no control parameters.</p>
</dd>
</dl>
</li>
</ul>
</section>
</section>
<section id="hipblaslt-backend-assembly-generator-tuning">
<h6 aria-level="7">hipBLASLt backend assembly generator tuning<a class="headerlink" href="#hipblaslt-backend-assembly-generator-tuning" title="Link to this heading">#</a></h6>
<p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipBLASLt/en/latest/index.html" title="(in hipBLASLt Documentation v0.12.1)"><span class="xref std std-doc">hipBLASLt</span></a> has a backend assembly generator in
<a class="reference external" href="https://github.com/ROCm/hipBLASLt/tree/develop/tensilelite">hipBLASLt’s GitHub repository</a>,
named TensileLite. TensileLite enables performance optimization by tuning the backend assembly generator.
The following section explains how to use TensileLite to tune hipBLASLt for better performance.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/hipBLASLt/tensilelite
./Tensile/bin/Tensile<span class="w"> </span>config.yaml<span class="w"> </span>output_path
</pre></div>
</div>
<section id="config-yaml">
<h6 aria-level="8">config.yaml<a class="headerlink" href="#config-yaml" title="Link to this heading">#</a></h6>
<p>This file contains the parameters and settings for the tuning process. Here’s
a breakdown of the important sections:</p>
<dl>
<dt><code class="docutils literal notranslate"><span class="pre">GlobalParameters</span></code></dt><dd><p>The set of parameters which provides context for the entire tuning exercise.</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">0</span></code> for <code class="docutils literal notranslate"><span class="pre">NumElementsToValidate</span></code> is suggested for performance tuning to avoid validation overhead.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">globalParameters</span><span class="p">[</span><span class="s2">"NumElementsToValidate"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">BenchmarkProblems</span></code></dt><dd><p>Defines the set of kernel specifications as well as the size definitions
for the tuning exercise.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ProblemType</span></code> (<code class="docutils literal notranslate"><span class="pre">OperationType</span></code>, <code class="docutils literal notranslate"><span class="pre">DataType</span></code>, <code class="docutils literal notranslate"><span class="pre">TransposeA</span></code>, <code class="docutils literal notranslate"><span class="pre">TransposeB</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">BenchmarkCommonParameters</span></code> (the same parameters for all solutions)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ForkParameters</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">BenchmarkFinalParameters</span></code> (<code class="docutils literal notranslate"><span class="pre">ProblemSizes</span></code>)</p></li>
</ul>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">LibraryLogic</span></code></dt><dd><p>Specifies the target environment and platform.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ScheduleName</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">aldebaran</span></code> is MI200</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">aquavanjaram</span></code> is MI300</p></li>
</ul>
</li>
</ul>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>ls
aldebaran<span class="w">  </span>aquavanjaram<span class="w">  </span>navi31<span class="w">  </span>navi32
</pre></div>
</div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">LibraryLogic</span><span class="p">:</span>
<span class="w">  </span><span class="nt">ScheduleName</span><span class="p">:</span><span class="w"> </span><span class="s">"aldebaran"</span>
<span class="w">  </span><span class="nt">DeviceNames</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">Device 0050</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">Device 0052</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">Device 0054</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">Device 0062</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">Device 7400</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">ArchitectureName</span><span class="p">:</span><span class="w"> </span><span class="s">"gfx90a"</span>
</pre></div>
</div>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">LibraryClient</span></code></dt><dd><p>If defined, this will enable step 4 of the tuning process, which means the final
library will be created.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>ls
aldebaran_Cijk_Ailk_Bjlk_S.yaml
</pre></div>
</div>
</dd>
</dl>
</section>
</section>
</section>
<section id="tensilelite-tuning-flow">
<h6>TensileLite tuning flow<a class="headerlink" href="#tensilelite-tuning-flow" title="Link to this heading">#</a></h6>
<p>The TensileLite tuning flow consists of seven steps. In the first six steps,
the programmable benchmarking protocol generates fast kernel candidates. In the
final step (<a class="reference internal" href="#tensilelite-tuning-step-7"><span class="std std-ref">step 7</span></a>), these candidates are benchmarked against a predefined set
of problem sizes.</p>
<figure class="align-center" id="tensilelite-tuning-flow-fig">
<img alt="TensileLite tuning flow" src="_images/tensilelite-tuning-flow.png"/>
</figure>
<section id="step-1-initial-solution-parameters">
<span id="tensilelite-tuning-step-1"></span><h6 aria-level="7">Step 1: Initial solution parameters<a class="headerlink" href="#step-1-initial-solution-parameters" title="Link to this heading">#</a></h6>
<p>Before Tensile is able to benchmark a kernel parameter in Step 2 of the <a class="reference internal" href="#tensilelite-tuning-flow-fig"><span class="std std-ref">preceding figure</span></a>,
such as <code class="docutils literal notranslate"><span class="pre">PrefetchGlobalRead={False,</span> <span class="pre">True}</span></code>, all other kernel parameters not being measured must be specified.
Therefore, the first step is to initialize a list of default kernel parameters, then subsequent steps of
benchmarking will override a parameter from this default list, with the parameter determined from benchmarking.
Tensile is pre-loaded with default parameters for any unspecified during tuning.</p>
</section>
<section id="step-2-benchmark-common-parameters">
<h6 aria-level="7">Step 2: Benchmark common parameters<a class="headerlink" href="#step-2-benchmark-common-parameters" title="Link to this heading">#</a></h6>
<p>Benchmarking common parameters determines parameters which are universally preferable to their alternatives
regardless of other parameters. To benchmark common parameters:</p>
<ul class="simple">
<li><p>User specifies parameters and values to benchmark.</p></li>
<li><p>Tensile benchmarks all parameter combinations for a user-specified problem size.</p></li>
<li><p>Tensile selects the fastest parameter combination which is now labeled determined and will subsequently be used.</p></li>
</ul>
<p>In practice, these parameters are not used, since globally preferred parameters are set as defaults in Tensile and do not need to be re-measured.</p>
</section>
<section id="step-3-fork-parameters">
<h6 aria-level="7">Step 3: Fork parameters<a class="headerlink" href="#step-3-fork-parameters" title="Link to this heading">#</a></h6>
<p>Rather than continuing to determine globally fastest parameters, which eventually leads
to a single fastest kernel, forking creates many different kernels,
all of which will be considered for use. All forked
parameters are considered determined, i.e., they aren’t measured to determine
which is fastest. The <a class="reference internal" href="#tensilelite-tuning-flow-fig"><span class="std std-ref">preceding figure</span></a> shows 7 kernels being forked in Step 3.</p>
</section>
<section id="step-4-benchmark-fork-parameters">
<h6 aria-level="7">Step 4: Benchmark fork parameters<a class="headerlink" href="#step-4-benchmark-fork-parameters" title="Link to this heading">#</a></h6>
<p>Next, tuning continues its refinement by determining fastest parameters for
each forked permutation, same as in Step 2.</p>
</section>
<section id="step-5-join-parameters">
<h6 aria-level="7">Step 5: Join parameters<a class="headerlink" href="#step-5-join-parameters" title="Link to this heading">#</a></h6>
<p>After tuning the forked kernels, joining reduces the list of kernels so that fewer kernels
will be considered for final use. Each kernel in the resulting list must have different values
for the listed <code class="docutils literal notranslate"><span class="pre">JoinParameters</span></code>, for example, employing <code class="docutils literal notranslate"><span class="pre">JoinParameters</span></code> = <code class="docutils literal notranslate"><span class="pre">MacroTile</span></code> will result in only a
few final kernels, each with a different <code class="docutils literal notranslate"><span class="pre">MacroTile</span></code>. If there are multiple kernels with the same <code class="docutils literal notranslate"><span class="pre">MacroTile</span></code>,
only the fastest is kept. In the above figure the 7 forked kernel have been reduced to 3 joined kernels.</p>
</section>
<section id="step-6-benchmark-join-parameters">
<h6 aria-level="7">Step 6: Benchmark join parameters<a class="headerlink" href="#step-6-benchmark-join-parameters" title="Link to this heading">#</a></h6>
<p>Users can further tune parameters of the joined kernels. This steps is same as Steps 4 except
that it tunes after joining so that there are fewer kernels to be tuned. In practice,
this step is not used; using Step 4 is preferred so that all parameters are measured before joining.</p>
</section>
<section id="step-7-benchmark-final-parameters">
<span id="tensilelite-tuning-step-7"></span><h6 aria-level="7">Step 7: Benchmark final parameters<a class="headerlink" href="#step-7-benchmark-final-parameters" title="Link to this heading">#</a></h6>
<p>At the conclusion of Step 6, all parameters of all kernels have been determined and the
final set of kernels for consideration has been established. Now all final kernels will be
measured against all problem sizes specified by the user. Problem sizes can be specified
as Range sizes and Exact sizes. Range sizes cause benchmarking of a broad range of sizes,
and Tensile will be able to interpolate which kernel is best even between the specifically
measured sizes. Exact sizes cause a single problem size to be measured, and the final
library is guaranteed to choose the fastest kernel for that size. This final benchmarking
generates the data that is subsequently analyzed for creating the mapping of problem size
to optimal kernel.</p>
</section>
</section>
<section id="update-logic-yaml-files">
<h6>Update logic YAML files<a class="headerlink" href="#update-logic-yaml-files" title="Link to this heading">#</a></h6>
<p>The logic YAML files in hipBLASLt are located in
<code class="docutils literal notranslate"><span class="pre">library/src/amd_detail/rocblaslt/src/Tensile/Logic/asm_full/</span></code>.</p>
<p>To merge the YAML files from the tuned results in TensileLite, use the
<code class="docutils literal notranslate"><span class="pre">merge.py</span></code> located in <code class="docutils literal notranslate"><span class="pre">tensilelite/Tensile/Utilities</span></code> with the following
command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>merge.py<span class="w"> </span>original_dir<span class="w"> </span>new_tuned_yaml_dir<span class="w"> </span>output_dir
</pre></div>
</div>
<p>The following table describes the logic YAML files.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Logic YAML</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">Equality</span></code></p></td>
<td><p>Update the equality file when your tuned YAML is
an exact tuning.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">GridBased</span></code></p></td>
<td><p>Update the gridbased file when your tuned YAML is
a grid-based tuning.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">FreeSize</span></code></p></td>
<td><p>Update the freesize file when your tuned YAML
contains confidential sizes, or others. Note that
freesize YAML files do not require any problem size.</p></td>
</tr>
</tbody>
</table>
</div>
<section id="tensile-optimization-and-performance-tuning-tips">
<h6 aria-level="7">Tensile optimization and performance tuning tips<a class="headerlink" href="#tensile-optimization-and-performance-tuning-tips" title="Link to this heading">#</a></h6>
<dl class="simple">
<dt>MI16x16 versus MI32x32</dt><dd><p>MI16x16 outperforms MI32x32 due to its superior power efficiency. The MI16x16
format refers to the <code class="docutils literal notranslate"><span class="pre">v_mfma</span></code> instruction (such as
<code class="docutils literal notranslate"><span class="pre">v_mfma_f32_16x16x16f16</span></code>). See
<a class="reference external" href="https://llvm.org/docs/AMDGPU/AMDGPUAsmGFX940.html#vop3p">https://llvm.org/docs/AMDGPU/AMDGPUAsmGFX940.html#vop3p</a>.</p>
</dd>
<dt>Clock differences among XCDs</dt><dd><p>There can be a clock speed variation of 3% to 10% among different XCDs.
Typically, XCD0 has the highest clock speed, while XCD7 has the lowest on
MI300X. For optimal efficiency calculations on MI300X, use the XCD with the
lowest average clock speed. If the average clock speed of XCD0 is used,
target efficiencies (such as, 95% for DGEMM HPL cases with K=512) may not be
achievable.</p>
</dd>
<dt><cite>WorkGroupMapping</cite></dt><dd><p>To maximize L2 cache efficiency, use multiples of the XCD number. For MI300X,
this means using multiples of 8 (such as, 24, 32, 40).</p>
</dd>
<dt>GEMM stride issues</dt><dd><p>On MI300, if the matrix stride in GEMM is a multiple of 512 bytes, it can lead to
Tagram channel hotspotting issues, causing a significant performance drop, especially for TN
transpose cases. This can increase the latency of VMEM instructions and cause
a notable performance drop. To avoid this, use stride padding to ensure the
stride is not a multiple of 512 bytes (for instance, for TN F16 GEMM, set
<code class="docutils literal notranslate"><span class="pre">lda</span> <span class="pre">=</span> <span class="pre">ldb</span> <span class="pre">=</span> <span class="pre">K</span> <span class="pre">+</span> <span class="pre">128</span></code> when <code class="docutils literal notranslate"><span class="pre">K</span> <span class="pre">%</span> <span class="pre">256</span> <span class="pre">==</span> <span class="pre">0</span></code>).</p>
</dd>
</dl>
</section>
<section id="optimizing-composable-kernel-gemm-kernels">
<span id="mi300x-ck"></span><h6 aria-level="7">Optimizing Composable Kernel GEMM kernels<a class="headerlink" href="#optimizing-composable-kernel-gemm-kernels" title="Link to this heading">#</a></h6>
<p>The performance of a GEMM kernel is significantly influenced by the input
values. The performance hierarchy based on input value types, from highest to
lowest, is as follows:</p>
<ul class="simple">
<li><p>Case 1: [all 0]</p></li>
<li><p>Case 2: [all identical integers]</p></li>
<li><p>Case 3: [random integers]</p></li>
<li><p>Case 4: [random floats]</p></li>
</ul>
<p>There can be more than a 20 percent performance drop between Case 1 and Case 4,
and a 10 percent drop between random integers and random floats.</p>
<p>Additionally, <code class="docutils literal notranslate"><span class="pre">bf16</span></code> matrix core execution is noticeably faster than <code class="docutils literal notranslate"><span class="pre">f16</span></code>.</p>
<p>Distributing workgroups with data sharing on the same XCD can enhance
performance (reduce latency) and improve benchmarking stability.</p>
<p>CK provides a rich set of template parameters for generating flexible accelerated
computing kernels for difference application scenarios.</p>
<p>See <a class="reference internal" href="#document-how-to/rocm-for-ai/inference-optimization/optimizing-with-composable-kernel"><span class="doc">Optimizing with Composable Kernel</span></a>
for an overview of Composable Kernel GEMM kernels, information on tunable
parameters, and examples.</p>
</section>
</section>
<section id="miopen">
<span id="mi300x-miopen"></span><h6>MIOpen<a class="headerlink" href="#miopen" title="Link to this heading">#</a></h6>
<p>MIOpen is AMD’s open-source, deep learning primitives library for GPUs. It
implements fusion to optimize for memory bandwidth and GPU launch overheads,
providing an auto-tuning infrastructure to overcome the large design space of
problem configurations.</p>
<section id="convolution">
<h6 aria-level="7">Convolution<a class="headerlink" href="#convolution" title="Link to this heading">#</a></h6>
<p>Many of MIOpen kernels have parameters which affect
their performance. Setting these kernel parameters to optimal values
for a given convolution problem, allows reaching the best possible
throughput. The optimal values of these kernel parameters are saved
in PerfDb (Performance database). PerfDb is populated through
tuning. To manipulate the tuning level, use the environment variable
<code class="docutils literal notranslate"><span class="pre">MIOPEN_FIND_ENFORCE</span></code> (1-6). Optimal values of kernel parameters are
used to benchmark all applicable convolution kernels for the given
convolution problem. These values reside in the FindDb. To manipulate
how to find the best performing kernel for a given convolution
problem, use the environment variable <code class="docutils literal notranslate"><span class="pre">MIOPEN_FIND_MODE</span></code> (1-5).</p>
</section>
<section id="tuning-in-miopen">
<span id="mi300x-miopen-tuning"></span><h6 aria-level="7">Tuning in MIOpen<a class="headerlink" href="#tuning-in-miopen" title="Link to this heading">#</a></h6>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">MIOPEN_FIND_ENFORCE=DB_UPDATE</span></code>, <code class="docutils literal notranslate"><span class="pre">2</span></code></dt><dd><p>Performs auto-tuning and update to the PerfDb.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">MIOPEN_FIND_ENFORCE=SEARCH</span></code>, <code class="docutils literal notranslate"><span class="pre">3</span></code></dt><dd><p>Only perform auto-tuning if PerfDb does not contain optimized value for a
given convolution problem</p>
</dd>
</dl>
<p>What does <a class="reference external" href="https://rocm.docs.amd.com/projects/MIOpen/en/latest/conceptual/perfdb.html" title="(in MIOpen Documentation v3.4.0)"><span class="xref std std-doc">PerfDb</span></a> look like?</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>
 <span class="mi">2</span><span class="n">x128x56xNHWCxF</span><span class="p">,</span> <span class="p">[</span>
                  <span class="n">ConvAsm1x1U</span>          <span class="p">:</span>  <span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span> <span class="p">;</span>       <span class="o">//</span> <span class="n">optimum</span> <span class="n">kernel</span> <span class="n">params</span> <span class="k">for</span> <span class="n">convolution</span> <span class="n">problem</span> <span class="mi">2</span><span class="n">x128x56xNHWCxF</span>
                  <span class="n">ConvOclDirectFwd1x1</span>  <span class="p">:</span> <span class="mi">1</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">0</span><span class="p">;</span>     <span class="o">//</span> <span class="n">optimum</span> <span class="n">kernel</span> <span class="n">params</span> <span class="k">for</span> <span class="n">convolution</span> <span class="n">problem</span> <span class="mi">2</span><span class="n">x128x56xNHWCxF</span>
                  <span class="p">],</span>
<span class="mi">2</span><span class="n">x992x516xNHWCxF</span><span class="p">,</span> <span class="p">[</span>
                  <span class="n">ConvAsm1x1U</span>          <span class="p">:</span>  <span class="mi">64</span><span class="p">,</span><span class="mi">18</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">41</span><span class="p">,</span><span class="mi">6</span> <span class="p">;</span>    <span class="o">//</span> <span class="n">optimum</span> <span class="n">kernel</span> <span class="n">params</span> <span class="k">for</span> <span class="n">convolution</span> <span class="n">problem</span> <span class="mi">2</span><span class="n">x992x516xNHWCxF</span>
                  <span class="n">ConvOclDirectFwd1x1</span>  <span class="p">:</span> <span class="mi">54</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">21</span><span class="p">,</span><span class="mi">21</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">23</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">0</span>  <span class="o">//</span> <span class="n">optimum</span> <span class="n">kernel</span> <span class="n">params</span> <span class="k">for</span> <span class="n">convolution</span> <span class="n">problem</span> <span class="mi">2</span><span class="n">x992x516xNHWCxF</span>
                  <span class="p">]</span>
 <span class="o">...</span>
<span class="p">]</span>
</pre></div>
</div>
<p>See <a class="reference external" href="https://rocm.docs.amd.com/projects/MIOpen/en/latest/conceptual/perfdb.html" title="(in MIOpen Documentation v3.4.0)"><span>Using the performance database</span></a> for more information.</p>
</section>
<section id="finding-the-fastest-kernel">
<h6 aria-level="7">Finding the fastest kernel<a class="headerlink" href="#finding-the-fastest-kernel" title="Link to this heading">#</a></h6>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">MIOPEN_FIND_MODE=NORMAL</span></code>, <code class="docutils literal notranslate"><span class="pre">1</span></code></dt><dd><p>Benchmark all the solvers and return a list (front element is the fastest kernel).</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">MIOPEN_FIND_MODE=FAST</span></code>, <code class="docutils literal notranslate"><span class="pre">2</span></code></dt><dd><p>Check FindDb (Find database) if convolution problem is found return - else
immediate fallback mode (predict the performing kernel parameters based on
mathematical and AI models).</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">MIOPEN_FIND_MODE=HYBRID</span></code>, <code class="docutils literal notranslate"><span class="pre">3</span></code></dt><dd><p>Check FindDb if convolution problem is found return - else benchmark that
problem.</p>
</dd>
</dl>
<p>What does <a class="reference external" href="https://rocm.docs.amd.com/projects/MIOpen/en/latest/conceptual/finddb.html" title="(in MIOpen Documentation v3.4.0)"><span class="xref std std-doc">FindDb</span></a> look like?</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>

 <span class="mi">2</span><span class="n">x128x56xNHWCxF</span><span class="p">,</span> <span class="p">[</span>
                  <span class="n">ConvAsm1x1U</span>          <span class="p">:</span>  <span class="mf">0.045</span> <span class="p">(</span><span class="n">time</span><span class="p">),</span> <span class="mi">12312</span> <span class="p">(</span><span class="n">workspace</span><span class="p">),</span> <span class="n">algo_type</span><span class="p">;</span>
                  <span class="n">ConvOclDirectFwd1x1</span>  <span class="p">:</span> <span class="mf">1.145</span> <span class="p">(</span><span class="n">time</span><span class="p">),</span> <span class="mi">0</span> <span class="p">(</span><span class="n">workspace</span><span class="p">),</span> <span class="n">algo_type</span><span class="p">;</span>
                  <span class="p">],</span>

<span class="mi">2</span><span class="n">x992x516xNHWCxF</span><span class="p">,</span> <span class="p">[</span>
                  <span class="n">ConvAsm1x1U</span>          <span class="p">:</span>  <span class="mf">2.045</span> <span class="p">(</span><span class="n">time</span><span class="p">),</span> <span class="mi">12312</span> <span class="p">(</span><span class="n">workspace</span><span class="p">),</span> <span class="n">algo_type</span><span class="p">;</span>
                  <span class="n">ConvOclDirectFwd1x1</span>  <span class="p">:</span> <span class="mf">1.145</span> <span class="p">(</span><span class="n">time</span><span class="p">),</span> <span class="mi">0</span> <span class="p">(</span><span class="n">workspace</span><span class="p">),</span> <span class="n">algo_type</span><span class="p">;</span>
                  <span class="p">]</span>
 <span class="o">...</span>
<span class="p">]</span>
</pre></div>
</div>
<p>See <a class="reference external" href="https://rocm.docs.amd.com/projects/MIOpen/en/latest/how-to/find-and-immediate.html" title="(in MIOpen Documentation v3.4.0)"><span>Using the find APIs and immediate mode</span></a> for more information.</p>
<p>For example:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">MIOPEN_FIND_ENFORCE</span><span class="o">=</span><span class="m">3</span><span class="w"> </span><span class="nv">MIOPEN_FIND_MODE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>./bin/MIOpenDriver<span class="w"> </span>convbfp16<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="w"> </span>-c<span class="w"> </span><span class="m">1024</span><span class="w"> </span>-H<span class="w"> </span><span class="m">14</span><span class="w"> </span>-W<span class="w"> </span><span class="m">14</span><span class="w"> </span>-k<span class="w"> </span><span class="m">256</span><span class="w"> </span>-y<span class="w"> </span><span class="m">1</span><span class="w"> </span>-x<span class="w"> </span><span class="m">1</span><span class="w"> </span>-p<span class="w"> </span><span class="m">0</span><span class="w"> </span>-q<span class="w"> </span><span class="m">0</span><span class="w"> </span>-u<span class="w"> </span><span class="m">1</span><span class="w"> </span>-v<span class="w"> </span><span class="m">1</span><span class="w"> </span>-l<span class="w"> </span><span class="m">1</span><span class="w"> </span>-j<span class="w"> </span><span class="m">1</span><span class="w"> </span>-m<span class="w"> </span>conv<span class="w"> </span>-g<span class="w"> </span><span class="m">1</span><span class="w"> </span>-F<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
</section>
</section>
<section id="rccl">
<span id="mi300x-rccl"></span><h6>RCCL<a class="headerlink" href="#rccl" title="Link to this heading">#</a></h6>
<p><a class="reference external" href="https://rocm.docs.amd.com/projects/rccl/en/latest/index.html" title="(in RCCL Documentation v2.22.3)"><span class="xref std std-doc">RCCL</span></a> is a stand-alone library of standard collective
communication routines for GPUs, implementing all-reduce, all-gather, reduce,
broadcast, reduce-scatter, gather, scatter, and all-to-all. RCCL supports an
arbitrary number of GPUs installed in a single node or multiple nodes
and can be used in either single- or multi-process (such as MPI)
applications.</p>
<p>The following subtopics include information on RCCL features and optimization
strategies:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#mi300x-rccl-8-gpu"><span class="std std-ref">Use all eight GPUs</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-rccl-disable-numa"><span class="std std-ref">Disable NUMA auto-balancing</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-rccl-disable-acs"><span class="std std-ref">Disable ACS for multi-node RCCL</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-rccl-unittests"><span class="std std-ref">Run RCCL-Unittests</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-rccl-npkit"><span class="std std-ref">NPKit profiler</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-rccl-tests"><span class="std std-ref">RCCL-tests</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-rccl-one-process-per-gpu"><span class="std std-ref">Use one-process-per-GPU mode</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-rccl-e2e"><span class="std std-ref">RCCL in E2E workloads</span></a></p></li>
</ul>
<section id="use-all-eight-gpus">
<span id="mi300x-rccl-8-gpu"></span><h6 aria-level="7">Use all eight GPUs<a class="headerlink" href="#use-all-eight-gpus" title="Link to this heading">#</a></h6>
<p>In an <a class="reference internal" href="#mi300x-node-level-arch-fig"><span class="std std-ref">MI300X architecture</span></a>, there are
dedicated links between each pair of GPUs in a fully connected topology.
Therefore, for collective operations, the best performance is achieved
when all 8 GPUs and, hence, all the links between them are used. In the
case of 2- or 4-GPU collective operations (generally less than 8 GPUs),
you can only use a fraction of the potential bandwidth on the node.</p>
<p>The following figure shows an
<a class="reference internal" href="#document-conceptual/gpu-arch/mi300"><span class="doc">MI300X node-level architecture</span></a> of a
system with AMD EPYC processors in a dual-socket configuration and eight
AMD Instinct MI300X accelerators. The MI300X OAMs attach to the host system via
PCIe Gen 5 x16 links (yellow lines). The GPUs use seven high-bandwidth,
low-latency AMD Infinity Fabric™ links (red lines) to form a fully connected
8-GPU system.</p>
<figure class="align-default" id="id5">
<span id="mi300x-node-level-arch-fig"></span><img alt="_images/mi300-node-level-arch.png" src="_images/mi300-node-level-arch.png"/>
<figcaption>
<p><span class="caption-text">MI300 series node-level architecture showing 8 fully interconnected MI300X
OAM modules connected to (optional) PCIe switches via re-timers and HGX
connectors.</span><a class="headerlink" href="#id5" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="disable-numa-auto-balancing">
<span id="mi300x-rccl-disable-numa"></span><h6 aria-level="7">Disable NUMA auto-balancing<a class="headerlink" href="#disable-numa-auto-balancing" title="Link to this heading">#</a></h6>
<p>In order to reduce performance variability and also achieve better
performance, you need to make sure that NUMA auto-balancing is disabled
on the node.</p>
<p>Check whether NUMA auto-balancing is disabled, by running the
following command: <code class="docutils literal notranslate"><span class="pre">cat</span> <span class="pre">/proc/sys/kernel/numa_balancing</span></code> and
checking whether the output is <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p>
<p>If the output is <code class="docutils literal notranslate"><span class="pre">1</span></code>, you can disable NUMA auto-balancing by running the
following command: <code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">sysctl</span> <span class="pre">kernel.numa_balancing=0</span></code>. For more details,
see <a class="reference external" href="https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/system-optimization/mi300x.html#disable-numa-auto-balancing">AMD Instinct MI300X system optimization</a>.</p>
</section>
<section id="disable-acs-for-multi-node-rccl">
<span id="mi300x-rccl-disable-acs"></span><h6 aria-level="7">Disable ACS for multi-node RCCL<a class="headerlink" href="#disable-acs-for-multi-node-rccl" title="Link to this heading">#</a></h6>
<p>Check if ACS is disabled with <code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">lspci</span> <span class="pre">-vvv</span> <span class="pre">\|</span> <span class="pre">grep</span> <span class="pre">-i</span> <span class="pre">"acsctl"</span></code>.
This will print many lines. Check if there are any that show <code class="docutils literal notranslate"><span class="pre">SrcValid+</span></code></p>
<p>If there are any <code class="docutils literal notranslate"><span class="pre">SrcValid+</span></code>, then use the following <code class="docutils literal notranslate"><span class="pre">disable_acs.sh</span></code> script
to disable ACS (requires <code class="docutils literal notranslate"><span class="pre">sudo</span></code>).</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="c1">#</span>

<span class="c1"># Disable ACS on every device that supports it</span>

<span class="c1">#</span>

<span class="nv">PLATFORM</span><span class="o">=</span><span class="k">$(</span>dmidecode<span class="w"> </span>--string<span class="w"> </span>system-product-name<span class="k">)</span>

logger<span class="w"> </span><span class="s2">"PLATFORM=</span><span class="si">${</span><span class="nv">PLATFORM</span><span class="si">}</span><span class="s2">"</span>

<span class="c1"># Enforce platform check here.</span>

<span class="c1">#case "${PLATFORM}" in</span>

<span class="c1">#"OAM"*)</span>

<span class="c1">#logger "INFO: Disabling ACS is no longer necessary for ${PLATFORM}"</span>

<span class="c1">#exit 0</span>

<span class="c1">#;;</span>

<span class="c1">#*)</span>

<span class="c1">#;;</span>

<span class="c1">#esac</span>

<span class="c1"># must be root to access extended PCI config space</span>

<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">"</span><span class="nv">$EUID</span><span class="s2">"</span><span class="w"> </span>-ne<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">"ERROR: </span><span class="nv">$0</span><span class="s2"> must be run as root"</span>

<span class="nb">exit</span><span class="w"> </span><span class="m">1</span>

<span class="k">fi</span>

<span class="k">for</span><span class="w"> </span>BDF<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="se">\`</span>lspci<span class="w"> </span>-d<span class="w"> </span><span class="s2">"*:*:*"</span><span class="w"> </span><span class="se">\|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">'{print $1}'</span><span class="sb">`</span><span class="p">;</span><span class="w"> </span><span class="k">do</span>

<span class="c1"># skip if it doesn't support ACS</span>

setpci<span class="w"> </span>-v<span class="w"> </span>-s<span class="w"> </span><span class="si">${</span><span class="nv">BDF</span><span class="si">}</span><span class="w"> </span>ECAP_ACS+0x6.w<span class="w"> </span>&gt;<span class="w"> </span>/dev/null<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span>

<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="nv">$?</span><span class="w"> </span>-ne<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>

<span class="c1">#echo "${BDF} does not support ACS, skipping"</span>

<span class="k">continue</span>

<span class="k">fi</span>

logger<span class="w"> </span><span class="s2">"Disabling ACS on \`lspci -s </span><span class="si">${</span><span class="nv">BDF</span><span class="si">}</span><span class="s2">`"</span>

setpci<span class="w"> </span>-v<span class="w"> </span>-s<span class="w"> </span><span class="si">${</span><span class="nv">BDF</span><span class="si">}</span><span class="w"> </span>ECAP_ACS+0x6.w<span class="o">=</span><span class="m">0000</span>

<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="nv">$?</span><span class="w"> </span>-ne<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>

logger<span class="w"> </span><span class="s2">"Error enabling directTrans ACS on </span><span class="si">${</span><span class="nv">BDF</span><span class="si">}</span><span class="s2">"</span>

<span class="k">continue</span>

<span class="k">fi</span>

<span class="nv">NEW_VAL</span><span class="o">=</span><span class="sb">`</span>setpci<span class="w"> </span>-v<span class="w"> </span>-s<span class="w"> </span><span class="si">${</span><span class="nv">BDF</span><span class="si">}</span><span class="w"> </span>ECAP_ACS+0x6.w<span class="w"> </span><span class="se">\|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">'{print $NF}'</span><span class="se">\`</span>

<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">"</span><span class="si">${</span><span class="nv">NEW_VAL</span><span class="si">}</span><span class="s2">"</span><span class="w"> </span>!<span class="o">=</span><span class="w"> </span><span class="s2">"0000"</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>

logger<span class="w"> </span><span class="s2">"Failed to enabling directTrans ACS on </span><span class="si">${</span><span class="nv">BDF</span><span class="si">}</span><span class="s2">"</span>

<span class="k">continue</span>

<span class="k">fi</span>

<span class="k">done</span>

<span class="nb">exit</span><span class="w"> </span><span class="m">0</span>
</pre></div>
</div>
</section>
<section id="run-rccl-unittests">
<span id="mi300x-rccl-unittests"></span><h6 aria-level="7">Run RCCL-Unittests<a class="headerlink" href="#run-rccl-unittests" title="Link to this heading">#</a></h6>
<p>In order to verify RCCL installation and test whether all parts and
units of RCCL work as expected you can run the RCCL-Unittests which is
explained in <a class="github reference external" href="https://github.com/ROCm/rccl?tab=readme-ov-file#tests">ROCm/rccl</a>.</p>
</section>
<section id="npkit-profiler">
<span id="mi300x-rccl-npkit"></span><h6 aria-level="7">NPKit profiler<a class="headerlink" href="#npkit-profiler" title="Link to this heading">#</a></h6>
<p>To collect fine-grained trace events in RCCL components, especially in
giant collective GPU kernels you can use the NPKit profiler explained
in <a class="github reference external" href="https://github.com/ROCm/rccl?tab=readme-ov-file#npkit">ROCm/rccl</a>.</p>
</section>
<section id="rccl-tests">
<span id="mi300x-rccl-tests"></span><h6 aria-level="7">RCCL-tests<a class="headerlink" href="#rccl-tests" title="Link to this heading">#</a></h6>
<p>RCCL-tests are performance and error-checking tests for RCCL
maintained in <a class="github reference external" href="https://github.com/ROCm/rccl-tests">ROCm/rccl-tests</a>.</p>
<p>These tests are one of the best ways to check the performance of
different collectives provided by RCCL. You can select collectives,
message sizes, datatypes, operations, number of iterations, etc., for
your test, and then rccl-tests deliver performance metrics such as
latency, algorithm bandwidth, and bus bandwidth for each case.</p>
</section>
<section id="use-one-process-per-gpu-mode">
<span id="mi300x-rccl-one-process-per-gpu"></span><h6 aria-level="7">Use one-process-per-GPU mode<a class="headerlink" href="#use-one-process-per-gpu-mode" title="Link to this heading">#</a></h6>
<p>RCCL delivers the best performance for collectives when it is configured
in a one-process-per-GPU mode. This is due to the fact that for a
one-process-per-multiple-GPUs configuration, you can run into kernel launch
latency issues. This is because ROCm serializes kernel launches on multiple GPUs
from one process which hurts performance.</p>
</section>
<section id="rccl-in-e2e-workloads">
<span id="mi300x-rccl-e2e"></span><h6 aria-level="7">RCCL in E2E workloads<a class="headerlink" href="#rccl-in-e2e-workloads" title="Link to this heading">#</a></h6>
<p>Use the following environment variable to increase the number of
channels used by RCCL when using RCCL in end-to-end workloads to potentially
improve the performance:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>export NCCL_MIN_NCHANNELS=112
</pre></div>
</div>
</section>
</section>
</section>
<section id="triton-kernel-performance-optimization">
<span id="mi300x-triton-kernel-performance-optimization"></span><h5>Triton kernel performance optimization<a class="headerlink" href="#triton-kernel-performance-optimization" title="Link to this heading">#</a></h5>
<p>Triton kernel optimization encompasses a variety of strategies aimed at
maximizing the efficiency and performance of GPU computations. These strategies
include
<a class="reference internal" href="#mi300x-triton-gpu-utilization"><span class="std std-ref">optimizing overall GPU resource utilization</span></a>,
<a class="reference internal" href="#mi300x-autotunable-kernel-config"><span class="std std-ref">tuning kernel configurations</span></a>, and
<a class="reference internal" href="#mi300x-assembly-analysis"><span class="std std-ref">leveraging specific hardware features</span></a> to
achieve higher throughput and lower latency.</p>
<section id="auto-tunable-kernel-configurations">
<span id="mi300x-autotunable-kernel-config"></span><h6>Auto-tunable kernel configurations<a class="headerlink" href="#auto-tunable-kernel-configurations" title="Link to this heading">#</a></h6>
<p>Auto-tunable kernel configuration involves adjusting memory access and computational
resources assigned to each compute unit. It encompasses the usage of
<a class="reference internal" href="#mi300x-cu-fig"><span class="std std-ref">LDS</span></a>, register, and task scheduling on a compute unit.</p>
<p>The accelerator or GPU contains global memory, local data share (LDS), and
registers. Global memory has high access latency, but is large. LDS access has
much lower latency, but is smaller. It is a fast on-CU software-managed memory
that can be used to efficiently share data between all work items in a block.
Register access is the fastest yet smallest among the three.</p>
<figure class="align-default" id="id6">
<span id="mi300x-cu-fig"></span><img alt="_images/compute-unit.png" src="_images/compute-unit.png"/>
<figcaption>
<p><span class="caption-text">Schematic representation of a CU in the CDNA2 or CDNA3 architecture.</span><a class="headerlink" href="#id6" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The following is a list of kernel arguments used for tuning performance and
resource allocation on AMD accelerators, which helps in optimizing the
efficiency and throughput of various computational kernels.</p>
<dl>
<dt><code class="docutils literal notranslate"><span class="pre">num_stages=n</span></code></dt><dd><p>Adjusts the number of pipeline stages for different types of kernels. On AMD accelerators, set <code class="docutils literal notranslate"><span class="pre">num_stages</span></code>
according to the following rules:</p>
<ul class="simple">
<li><p>For kernels with a single GEMM, set to <code class="docutils literal notranslate"><span class="pre">2</span></code>.</p></li>
<li><p>For kernels with two GEMMs fused (Flash Attention, or any other kernel
that fuses 2 GEMMs), set to <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p>For kernels that fuse a single GEMM with another non-GEMM operator
(for example ReLU activation), set to <code class="docutils literal notranslate"><span class="pre">2</span></code>.</p></li>
<li><p>For kernels that have no GEMMs, set to <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
</ul>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">waves_per_eu=n</span></code></dt><dd><p>Helps to manage Vector General Purpose Registers (VGPR) usage to achieve
desired occupancy levels. This argument hints to the compiler to reduce VGPR
to achieve <code class="docutils literal notranslate"><span class="pre">n</span></code> occupancy where <code class="docutils literal notranslate"><span class="pre">n</span></code> is a number. The goal is to achieve a
certain occupancy level for each Execution Unit (EU, also called
<a class="reference internal" href="#mi300x-cu-fig"><span class="std std-ref">SIMD Unit</span></a>) to achieve better latency or throughput.
For more information on how to compute occupancy, see
<a class="reference internal" href="#mi300x-compute-kernel-occ"><span class="std std-ref">Compute the occupancy of a kernel</span></a>.</p>
<p>This argument is useful if:</p>
<ul class="simple">
<li><p>The occupancy of the kernel is limited by VGPR usage, and</p></li>
<li><p>The current VGPR usage is only a few above a boundary in
<a class="reference internal" href="#mi300x-occupancy-vgpr-table"><span class="std std-ref">Occupancy related to VGPR usage in an Instinct MI300X accelerator</span></a>.</p></li>
</ul>
</dd>
</dl>
<figure class="align-center" id="id7">
<span id="mi300x-occupancy-vgpr-table"></span><img alt="Occupancy related to VGPR usage in an Instinct MI300X accelerator." src="_images/occupancy-vgpr.png"/>
<figcaption>
<p><span class="caption-text">Occupancy related to VGPRs usage on an Instinct MI300X accelerator</span><a class="headerlink" href="#id7" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>For example, according to the table, each Execution Unit (EU) has 512 available
VGPRs, which are allocated in blocks of 16. If the current VGPR usage is 170,
it will be rounded up to 176 due to the allocation granularity. In this case,
the occupancy is limited to 2 waves per EU because <span class="math notranslate nohighlight">\(176 \times 3 &gt; 512\)</span>.
So, if you set <code class="docutils literal notranslate"><span class="pre">waves_per_eu</span></code> to 3, the LLVM backend will attempt to reduce
VGPR usage so that it might fit 3 waves per EU.</p>
<dl>
<dt><code class="docutils literal notranslate"><span class="pre">BLOCK_M</span></code>, <code class="docutils literal notranslate"><span class="pre">BLOCK_N</span></code>, <code class="docutils literal notranslate"><span class="pre">BLOCK_K</span></code></dt><dd><p>Tile sizes to be tuned to balance the memory-to-computation ratio. The goal
is to minimize the memory transfer from global to shared and reuse memory
across different threads. This needs to be tuned. The tile sizes should be
large enough to maximize the efficiency of the memory-to-computation
ratio but small enough to parallelize the greatest number of workgroups at
the grid level.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">matrix_instr_nonkdim</span></code></dt><dd><p>Experimental feature for Flash Attention-like kernels that determines the size of the Matrix Fused Multiply-Add
(MFMA) instruction used.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">matrix_instr_nonkdim</span> <span class="pre">=</span> <span class="pre">16</span></code>: <code class="docutils literal notranslate"><span class="pre">mfma_16x16</span></code> is used.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">matrix_instr_nonkdim</span> <span class="pre">=</span> <span class="pre">32</span></code>: <code class="docutils literal notranslate"><span class="pre">mfma_32x32</span></code> is used.</p></li>
</ul>
<p>For GEMM kernels on an MI300X accelerator, <code class="docutils literal notranslate"><span class="pre">mfma_16x16</span></code> typically outperforms <code class="docutils literal notranslate"><span class="pre">mfma_32x32</span></code>, even for large
tile/GEMM sizes.</p>
</dd>
</dl>
</section>
<section id="overall-gpu-resource-utilization">
<span id="mi300x-triton-gpu-utilization"></span><h6>Overall GPU resource utilization<a class="headerlink" href="#overall-gpu-resource-utilization" title="Link to this heading">#</a></h6>
<p>As depicted in the following figure, each XCD in
<a class="reference internal" href="#document-conceptual/gpu-arch/mi300"><span class="doc">MI300X</span></a> contains 40 compute units (CUs),
with 38 active. Each MI300X contains eight vertical XCDs, and a total of 304
active compute units capable of parallel computation. The first consideration is
the number of CUs a kernel can distribute its task across.</p>
<figure class="align-default" id="id8">
<img alt="_images/xcd-sys-arch.png" src="_images/xcd-sys-arch.png"/>
<figcaption>
<p><span class="caption-text">XCD-level system architecture showing 40 compute units,
each with 32 KB L1 cache, a unified compute system with 4 ACE compute
accelerators, shared 4MB of L2 cache, and a hardware scheduler (HWS).</span><a class="headerlink" href="#id8" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>You can query hardware resources with the command <code class="docutils literal notranslate"><span class="pre">rocminfo</span></code> in the
<code class="docutils literal notranslate"><span class="pre">/opt/rocm/bin</span></code> directory. For instance, query the number of CUs, number of
SIMD, and wavefront size using the following commands.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>rocminfo<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span><span class="s2">"Compute Unit"</span>

rocminfo<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span><span class="s2">"SIMD"</span>

rocminfo<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span><span class="s2">"Wavefront Size"</span>
</pre></div>
</div>
<p>For the MI300X, the goal is to have a minimum of 1024 thread
blocks or workgroups in the grid (kernel), with a preference for
more.</p>
<p>Identifying additional parallelism within the algorithm is necessary to
enhance GPU utilization. For more information and examples, see
<a class="reference external" href="https://arxiv.org/pdf/2402.00025v1">Accelerating A Triton Fused Kernel For W4a16 Quantized Inference With
SplitK Work Decomposition</a>.</p>
</section>
<section id="mlir-analysis">
<span id="mi300x-mlir-analysis"></span><h6>MLIR analysis<a class="headerlink" href="#mlir-analysis" title="Link to this heading">#</a></h6>
<p>Triton includes the following layouts: <strong>blocked</strong>, <strong>shared</strong>, <strong>sliced</strong>, and <strong>MFMA</strong>.</p>
<p>Use the Triton GPU Intermediate Representation (IR) to identify the memory in
which each computation takes place.</p>
<p>Use the environment variable <code class="docutils literal notranslate"><span class="pre">MLIR_ENABLE_DUMP</span></code> to dump MLIR:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MLIR_ENABLE_DUMP</span><span class="o">=</span><span class="m">1</span>
</pre></div>
</div>
<p>The following is a snippet of IR from the Flash Attention decode <code class="docutils literal notranslate"><span class="pre">int4</span></code> KV program. It is to
de-quantize the <code class="docutils literal notranslate"><span class="pre">int4</span></code> key-value from the <code class="docutils literal notranslate"><span class="pre">int4</span></code> data type to <code class="docutils literal notranslate"><span class="pre">fp16</span></code>.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>%190 = tt.load %189 {cache = 1 : i32, evict = 1 : i32, isVolatile =
false} : tensor&lt;1x64xi32, #blocked6&gt; loc(#loc159)

%266 = arith.andi %190, %cst_28 : tensor&lt;1x64xi32, #blocked6&gt;
loc(#loc250)

%267 = arith.trunci %266 : tensor&lt;1x64xi32, #blocked6&gt; to
tensor&lt;1x64xi16, #blocked6&gt; loc(#loc251)

%268 = tt.bitcast %267 : tensor&lt;1x64xi16, #blocked6&gt; -&gt; tensor&lt;1x64xf16,
#blocked6&gt; loc(#loc252)

%269 = triton_gpu.convert_layout %268 : (tensor&lt;1x64xf16, #blocked6&gt;) -&gt;
tensor&lt;1x64xf16, #shared1&gt; loc(#loc252)

%270 = tt.trans %269 : (tensor&lt;1x64xf16, #shared1&gt;) -&gt; tensor&lt;64x1xf16,
#shared2&gt; loc(#loc194)

%276 = triton_gpu.convert_layout %270 : (tensor&lt;64x1xf16, #shared2&gt;) -&gt;
tensor&lt;64x1xf16, #blocked5&gt; loc(#loc254)

%293 = arith.mulf %276, %cst_30 : tensor&lt;64x1xf16, #blocked5&gt;
loc(#loc254)

%295 = arith.mulf %292, %294 : tensor&lt;64x32xf16, #blocked5&gt; loc(#loc264)

%297 = arith.addf %295, %296 : tensor&lt;64x32xf16, #blocked5&gt; loc(#loc255)

%298 = triton_gpu.convert_layout %297 : (tensor&lt;64x32xf16, #blocked5&gt;)
-&gt; tensor&lt;64x32xf16, #shared1&gt; loc(#loc255)

%299 = tt.trans %298 : (tensor&lt;64x32xf16, #shared1&gt;) -&gt;
tensor&lt;32x64xf16, #shared2&gt; loc(#loc196)

%300 = triton_gpu.convert_layout %299 : (tensor&lt;32x64xf16, #shared2&gt;) -&gt;
tensor&lt;32x64xf16, #triton_gpu.dot_op&lt;{opIdx = 1, parent = #mfma, kWidth
= 4}&gt;&gt; loc(#loc197)
</pre></div>
</div>
<p>From the IR snippet, you can see <code class="docutils literal notranslate"><span class="pre">i32</span></code> data is loaded from global memory to
registers (<code class="docutils literal notranslate"><span class="pre">%190</span></code>). With a few element-wise operations in registers, it is
stored in shared memory (<code class="docutils literal notranslate"><span class="pre">%269</span></code>) for the transpose operation (<code class="docutils literal notranslate"><span class="pre">%270</span></code>), which
needs data movement across different threads. With the transpose done, it is
loaded from LDS to register again (<code class="docutils literal notranslate"><span class="pre">%276</span></code>), and with a few more
element-wise operations, it is stored to LDS again (<code class="docutils literal notranslate"><span class="pre">%298</span></code>). The last step
loads from LDS to registers and converts to the dot-operand layout
(<code class="docutils literal notranslate"><span class="pre">%300</span></code>).</p>
<p>The IR snippet uses the LDS twice. The first is for the transpose, and
the second is to convert a blocked layout to a dot operand layout.
There’s an opportunity to optimize performance by using LDS once.</p>
</section>
<section id="isa-assembly-analysis">
<span id="mi300x-assembly-analysis"></span><h6>ISA assembly analysis<a class="headerlink" href="#isa-assembly-analysis" title="Link to this heading">#</a></h6>
<p>To generate ISA, <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">AMDGCN_ENABLE_DUMP=1</span></code> when running the Triton
program. The generated ISA will be printed as standard output. You can
dump it to a file for analysis.</p>
<ul class="simple">
<li><p>Ensure <code class="docutils literal notranslate"><span class="pre">global_load_dwordx4</span></code> is used in the ISA, especially when the
global memory load happens in the loop.</p></li>
<li><p>In most cases, the LDS load and store should use <code class="docutils literal notranslate"><span class="pre">_b128</span></code> to
minimize the number of LDS access instructions.</p></li>
<li><p>The AMD ISA has <code class="docutils literal notranslate"><span class="pre">s_waitcnt</span></code> instruction to synchronize the dependency
of memory access and computations. The <code class="docutils literal notranslate"><span class="pre">s_waitcnt</span></code> instructions can
typically have two signals in the Triton context:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">lgkmcnt(n)</span></code>: <code class="docutils literal notranslate"><span class="pre">lgkm</span></code> stands for LDS, GDS
(Global Data Share), Constant, and Message. It is often related to
LDS access. The <code class="docutils literal notranslate"><span class="pre">n</span></code> indicates the number of data accesses can still
be ongoing before moving on to the next step. For example, if <code class="docutils literal notranslate"><span class="pre">n</span></code> is
<code class="docutils literal notranslate"><span class="pre">0</span></code>, wait for all <code class="docutils literal notranslate"><span class="pre">lgkm</span></code> access to finish before continuing. If <code class="docutils literal notranslate"><span class="pre">n</span></code>
is <code class="docutils literal notranslate"><span class="pre">1</span></code>, move on even if <code class="docutils literal notranslate"><span class="pre">1</span></code> <code class="docutils literal notranslate"><span class="pre">lgkm</span></code> access is still running
asynchronously.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">vmcnt(n)</span></code>: <code class="docutils literal notranslate"><span class="pre">vm</span></code> represents vector memory. This happens when
vector memory is accessed, for example, when global load moves
from global memory to vector memory. The variable <code class="docutils literal notranslate"><span class="pre">n</span></code> is the same as
the previous setting.</p></li>
</ul>
</li>
</ul>
<p>Generally recommended guidelines are as follows.</p>
<ul class="simple">
<li><p>Vectorize memory access as much as possible.</p></li>
<li><p>Ensure synchronization is done efficiently.</p></li>
<li><p>Overlap of instructions to hide latency, but it requires thoughtful
analysis of the algorithms.</p></li>
<li><p>If you find inefficiencies, you can trace it back to LLVM IR, TTGIR
and even TTIR to see where the problem comes from. If you find it
during compiler optimization, activate the MLIR dump
(<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MLIR_ENABLE_DUMP=1</span></code>) and check which optimization pass caused the
problem.</p></li>
</ul>
</section>
</section>
<section id="hip-performance-optimization">
<span id="mi300x-hip-optimization"></span><h5>HIP performance optimization<a class="headerlink" href="#hip-performance-optimization" title="Link to this heading">#</a></h5>
<p>This section summarizes the best practices described in the
<a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/how-to/performance_guidelines.html" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-doc">Performance guidelines</span></a> section of the
HIP documentation.</p>
<p>Optimization areas of concern include:</p>
<ul class="simple">
<li><p>Parallel execution</p></li>
<li><p>Memory usage optimization</p></li>
<li><p>Optimization for maximum throughput</p></li>
<li><p>Minimizing memory thrashing</p></li>
</ul>
<section id="parallel-execution-and-gpu-hardware-utilization">
<h6>Parallel execution and GPU hardware utilization<a class="headerlink" href="#parallel-execution-and-gpu-hardware-utilization" title="Link to this heading">#</a></h6>
<p>The application should reveal and efficiently imply as much parallelism as
possible for optimal use to keep all system components active.</p>
</section>
<section id="memory-usage-optimization">
<h6>Memory usage optimization<a class="headerlink" href="#memory-usage-optimization" title="Link to this heading">#</a></h6>
<p>To optimize memory throughput, minimize low-bandwidth data transfers,
particularly between the host and device. Maximize on-chip memory, including
shared memory and caches, to reduce data transfers between global memory and the
device.</p>
<p>In a GPU, global memory has high latency but a large size, while local data
share (LDS) has lower latency but a smaller size, and registers have the fastest
but smallest access. Aim to limit load/store operations in global memory. If
multiple threads in a block need the same data, transfer it from global memory
to LDS for efficient access.</p>
<p>See <a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/how-to/performance_guidelines.html" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-doc">HIP’s performance guidelines</span></a> for
greater detail.</p>
</section>
</section>
<section id="diagnostic-and-performance-analysis">
<h5>Diagnostic and performance analysis<a class="headerlink" href="#diagnostic-and-performance-analysis" title="Link to this heading">#</a></h5>
<section id="debug-memory-access-faults">
<span id="mi300x-rocr-debug-agent"></span><h6>Debug memory access faults<a class="headerlink" href="#debug-memory-access-faults" title="Link to this heading">#</a></h6>
<p>Identifying a faulting kernel is often enough to triage a memory access
fault. The ROCr Debug Agent can trap a memory access fault and provide a
dump of all active wavefronts that caused the error, as well as the name
of the kernel. For more information, see
<a class="reference external" href="https://rocm.docs.amd.com/projects/rocr_debug_agent/en/latest/index.html" title="(in rocr_debug_agent v2.0.4)"><span class="xref std std-doc">ROCr Debug Agent documentation</span></a>.</p>
<p>To summarize, the key points include:</p>
<ol class="arabic simple">
<li><p>Compiling with <code class="docutils literal notranslate"><span class="pre">-ggdb</span> <span class="pre">-O0</span></code> is recommended but not required.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HSA_TOOLS_LIB=/opt/rocm/lib/librocm-debug-agent.so.2</span> <span class="pre">HSA_ENABLE_DEBUG=1</span> <span class="pre">./my_program</span></code></p></li>
</ol>
<p>When the debug agent traps the fault, it produces verbose output of all
wavefront registers and memory content. Importantly, it also prints
something similar to the following:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Disassembly for function vector_add_assert_trap(int*, int*, int*):

code object:
file:////rocm-debug-agent/build/test/rocm-debug-agent-test#offset=14309&amp;size=31336

loaded at: [0x7fd4f100c000-0x7fd4f100e070]
</pre></div>
</div>
<p>The kernel name and the code object file should be listed. In the
example above, the kernel name is vector_add_assert_trap, but this might
also look like:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Disassembly for function memory:///path/to/codeobject#offset=1234&amp;size=567:
</pre></div>
</div>
<p>In this case, it’s an in-memory kernel that was generated at runtime.
Using the environment variable <code class="docutils literal notranslate"><span class="pre">ROCM_DEBUG_AGENT_OPTIONS="--all</span> <span class="pre">--save-code-objects"</span></code>
will have the debug agent save all code objects to the current directory. Use
<code class="docutils literal notranslate"><span class="pre">--save-code-objects=[DIR]</span></code> to save them in another location.</p>
<p>The code objects will be renamed from the URI format with special
characters replaced by ‘_’. Use <code class="docutils literal notranslate"><span class="pre">llvm-objdump</span></code> to disassemble the
indicated in-memory code object that has been saved to disk. The name of
the kernel is often found in the disassembled code object.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>llvm-objdump<span class="w"> </span>--disassemble-all<span class="w"> </span>path/to/code-object.co
</pre></div>
</div>
<p>Disabling memory caching strategies within the ROCm stack and PyTorch is
recommended, where possible. This gives the debug agent the best chance
of finding the memory fault where it originates. Otherwise, it could be
masked by writing past the end of a cached block within a larger
allocation.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>PYTORCH_NO_HIP_MEMORY_CACHING=1

HSA_DISABLE_FRAGMENT_ALLOCATOR=1
</pre></div>
</div>
</section>
<section id="compute-the-occupancy-of-a-kernel">
<span id="mi300x-compute-kernel-occ"></span><h6>Compute the occupancy of a kernel<a class="headerlink" href="#compute-the-occupancy-of-a-kernel" title="Link to this heading">#</a></h6>
<ol class="arabic simple">
<li><p>Get the VGPR count, search for <code class="docutils literal notranslate"><span class="pre">.vgpr_count</span></code> in the ISA (for example,
<code class="docutils literal notranslate"><span class="pre">N</span></code>).</p></li>
<li><p>Get the allocated LDS following the steps (for example, L for the kernel).</p>
<ol class="loweralpha simple">
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MLIR_ENABLE_DUMP=1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rm</span> <span class="pre">-rf</span> <span class="pre">~/.triton/cache</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">kernel.py</span> <span class="pre">|</span> <span class="pre">|</span> <span class="pre">grep</span> <span class="pre">"triton_gpu.shared</span> <span class="pre">=</span> <span class="pre">"</span> <span class="pre">|</span> <span class="pre">tail</span> <span class="pre">-n</span> <span class="pre">1</span></code></p></li>
<li><p>You should see something like <code class="docutils literal notranslate"><span class="pre">triton_gpu.shared</span> <span class="pre">=</span> <span class="pre">65536</span></code>, indicating
65536 bytes of LDS are allocated for the kernel.</p></li>
</ol>
</li>
<li><p>Get number of waves per workgroup using the following steps (for example, <code class="docutils literal notranslate"><span class="pre">nW</span></code>).</p>
<ol class="loweralpha simple">
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MLIR_ENABLE_DUMP=1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rm</span> <span class="pre">-rf</span> <span class="pre">~/.triton/cache</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">kernel.py</span> <span class="pre">|</span> <span class="pre">|</span> <span class="pre">grep</span> <span class="pre">"triton_gpu.num-warps</span> <span class="pre">"</span> <span class="pre">|</span> <span class="pre">tail</span> <span class="pre">-n</span> <span class="pre">1</span></code></p></li>
<li><p>You should see something like <code class="docutils literal notranslate"><span class="pre">“triton_gpu.num-warps"</span> <span class="pre">=</span> <span class="pre">8</span></code>, indicating 8
waves per workgroup.</p></li>
</ol>
</li>
<li><p>Compute occupancy limited by VGPR based on N according to the
<a class="reference internal" href="#mi300x-occupancy-vgpr-table"><span class="std std-ref">preceding table</span></a>. For example, waves per
EU as <code class="docutils literal notranslate"><span class="pre">occ_vgpr</span></code>.</p></li>
<li><p>Compute occupancy limited by LDS based on L by: <code class="docutils literal notranslate"><span class="pre">occ_lds</span> <span class="pre">=</span> <span class="pre">floor(65536</span> <span class="pre">/</span> <span class="pre">L)</span></code>.</p></li>
<li><p>Then the occupancy is <code class="docutils literal notranslate"><span class="pre">occ</span> <span class="pre">=</span> <span class="pre">min(floor(occ_vgpr</span> <span class="pre">*</span> <span class="pre">4</span> <span class="pre">/</span> <span class="pre">nW),</span> <span class="pre">occ_lds)</span> <span class="pre">*</span> <span class="pre">nW</span> <span class="pre">/</span> <span class="pre">4</span></code></p>
<ol class="loweralpha simple">
<li><p><code class="docutils literal notranslate"><span class="pre">occ_vgpr</span> <span class="pre">\*</span> <span class="pre">4</span></code> gives the total number of waves on all 4 execution units (SIMDs)
per CU.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">floor(occ_vgpr</span> <span class="pre">*</span> <span class="pre">4</span> <span class="pre">/</span> <span class="pre">nW)</span></code> gives the occupancy of workgroups per CU
regrading VGPR usage.</p></li>
<li><p>The true <code class="docutils literal notranslate"><span class="pre">occ</span></code> is the minimum of the two.</p></li>
</ol>
</li>
</ol>
<p>Find the full <code class="docutils literal notranslate"><span class="pre">occ.sh</span></code> at
<a class="github reference external" href="https://github.com/ROCm/triton/blob/triton-mlir/scripts/amd/occ.sh">ROCm/triton</a>.</p>
</section>
</section>
<section id="special-considerations">
<h5>Special considerations<a class="headerlink" href="#special-considerations" title="Link to this heading">#</a></h5>
<section id="multi-gpu-communications">
<h6>Multi-GPU communications<a class="headerlink" href="#multi-gpu-communications" title="Link to this heading">#</a></h6>
<p>Because of the characteristics of MI300X inter-GPU communication and
limitation of bandwidth between and among 2 GPUs and 4 GPUs, avoid running
workloads that use 2 or 4 GPU collectives. It’s optimal to either use a
single GPU (where no collective is required) or employ 8 GPU
collectives.</p>
</section>
<section id="multi-node-fsdp-and-rccl-settings">
<h6>Multi-node FSDP and RCCL settings<a class="headerlink" href="#multi-node-fsdp-and-rccl-settings" title="Link to this heading">#</a></h6>
<p>When using PyTorch’s FSDP (Full Sharded Data Parallel) feature, the HIP
streams used by RCCL and HIP streams used for compute kernels do not
always overlap well. As a workaround, it’s recommended to use
high-priority HIP streams with RCCL.</p>
<p>To configure high-priority streams:</p>
<ul class="simple">
<li><p>Set environment variable <code class="docutils literal notranslate"><span class="pre">TORCH_NCCL_HIGH_PRIORITY=1</span></code> to force all RCCL
streams to be high-priority.</p></li>
<li><p>Set environment variable <code class="docutils literal notranslate"><span class="pre">GPU_MAX_HW_QUEUES=2</span></code> via the HIP runtime
library.</p></li>
</ul>
<p>Hardware efficiency is maximized with 4 or fewer HIP streams. These environment variables limit the
configuration to two compute streams and two RCCL streams, aligning with this best practice.
Additionally, RCCL is often pre-optimized for MI300 systems in production by querying the node
topology during startup, reducing the need for extensive manual tuning.</p>
</section>
</section>
</section>
</div>
</section>
</div>
</section>
<span id="document-how-to/rocm-for-hpc/index"></span><section id="using-rocm-for-hpc">
<h2>Using ROCm for HPC<a class="headerlink" href="#using-rocm-for-hpc" title="Link to this heading">#</a></h2>
<p>The ROCm open-source software stack is optimized to extract high-performance
computing (HPC) workload performance from AMD Instinct™ accelerators
while maintaining compatibility with industry software frameworks.</p>
<p>ROCm enhances support and access for developers by providing streamlined and
improved tools that significantly increase productivity. Being open-source, ROCm
fosters innovation, differentiation, and collaboration within the developer
community, making it a powerful and accessible solution for leveraging the full
potential of AMD accelerators’ capabilities in diverse computational
applications.</p>
<ul class="simple">
<li><p>For more information, see <a class="reference internal" href="#document-what-is-rocm"><span class="doc">What is ROCm?</span></a>.</p></li>
<li><p>For guidance on installing ROCm, see <a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/index.html" title="(in ROCm installation on Linux v6.4.2)"><span>ROCm installation for Linux</span></a>. See
the <a class="reference internal" href="#document-compatibility/compatibility-matrix"><span class="doc">Compatibility matrix</span></a> for details on hardware
and operating system support.</p></li>
</ul>
<p>Some of the most popular HPC frameworks are part of the ROCm platform, including
those to help parallelize operations across multiple accelerators and servers,
handle memory hierarchies, and solve linear systems.</p>
<img alt="Software and hardware ecosystem surrounding ROCm and AMD Instinct for HPC" class="align-center" src="_images/hpc-stack-2024_6_20.png"/>
<p>The following catalog of GPU-accelerated solutions includes a vast set of
platform-compatible HPC applications, including those for astrophysics, climate
and weather, computational chemistry, computational fluid dynamics, earth
science, genomics, geophysics, molecular dynamics, and physics computing.</p>
<p>Refer to the resources in the following table for instructions on building,
running, and deploying these applications on ROCm-capable systems with AMD
Instinct accelerators. Each build container provides parameters to specify
different source code branches, release versions of ROCm, OpenMPI, UCX, and
Ubuntu versions.</p>
<span class="target" id="hpc-apps"></span><style>
  #hpc-apps-table tr td:last-child {
    font-size: 0.9rem;
  }
</style><div class="docutils container" id="hpc-apps-table">
<div class="pst-scrollable-table-container"><table class="table">
<colgroup>
<col style="width: 22.2%"/>
<col style="width: 22.2%"/>
<col style="width: 55.6%"/>
</colgroup>
<thead>
<tr class="row-odd"><th class="head stub"><p>Application domain</p></th>
<th class="head"><p>HPC application</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><th class="stub"><p>Physics</p></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/chroma/">Chroma</a></p></td>
<td><p>The Chroma package supports data-parallel programming constructs for lattice
field theory and in particular lattice QCD. It uses the SciDAC QDP++ data-parallel
programming (in C++) that presents a single high-level code image to the user,
but can generate highly optimized code for many architectural systems including
single node workstations, multi and many-core nodes, clusters of nodes via
QMP, and classic vector computers.</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/milc/">MILC</a></p></td>
<td><p>The MILC Code is a set of research codes developed by MIMD Lattice Computation
(MILC) collaboration for doing simulations of four dimensional SU(3) lattice gauge
theory on MIMD parallel machines scaling from single-processor workstations
to HPC systems. The MILC Code is publicly available for research purposes.
Publications of work done using this code or derivatives of this code should
acknowledge this use.</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/quda">QUDA</a></p></td>
<td><p>Library designed for efficient lattice QCD computations on
accelerators. It includes optimized Dirac operators and a variety of
fermion solvers and conjugate gradient (CG) implementations, enhancing
performance and accuracy in lattice QCD simulations.</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/picongpu">PIConGPU</a></p></td>
<td><p>PIConGPU (Particle-in-cell on Graphics Processing Units) is an Open Source
simulations framework for plasma and laser-plasma physics used to develop
advanced particle accelerators for radiation therapy of cancer, high energy
physics and photon science.</p></td>
</tr>
<tr class="row-even"><th class="stub"><p>Astrophysics</p></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/cholla/">Cholla</a></p></td>
<td><p>An astrophysical simulation code developed for the extreme environments
encountered in astrophysical systems.</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>Geophysics</p></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/specfem3d">SPECFEM3D Cartesian</a></p></td>
<td><p>SPECFEM3D Cartesian simulates acoustic (fluid), elastic (solid), coupled
acoustic/elastic, poroelastic or seismic wave propagation in any type of
conforming mesh of hexahedra (structured or not.) It can, for instance,
model seismic waves propagating in sedimentary basins or any other
regional geological model following earthquakes. It can also be used
for non-destructive testing or for ocean acoustics.</p></td>
</tr>
<tr class="row-even"><th class="stub"><p>Molecular dynamics</p></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/amber">Amber</a></p></td>
<td><p>Amber is a suite of biomolecular simulation programs. It is a set of molecular mechanical force fields for
simulating biomolecules. Amber is also a package of molecular simulation
programs which includes source code and demos.</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/gromacs">GROMACS with HIP (AMD implementation)</a></p></td>
<td><p>GROMACS is a versatile package to perform molecular dynamics, i.e.
simulate the Newtonian equations of motion for systems with hundreds
to millions of particles. This AMD container is based on a released
version of GROMACS modified by AMD. This container only supports up
to a 8 GPU configuration</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/lammps">LAMMPS</a></p></td>
<td><p>LAMMPS is a classical molecular dynamics code with a focus on materials
modeling. It’s an acronym for Large-scale Atomic/Molecular Massively
Parallel Simulator.</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>Computational fluid dynamics</p></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/ansys-fluent">Ansys Fluent</a></p></td>
<td><p>Ansys Fluent is an advanced computational fluid dynamics (CFD) tool for
simulating and analyzing fluid flow, heat transfer, and related phenomena in complex systems.
It offers a range of powerful features for detailed and accurate modeling of various physical
processes, including turbulence, chemical reactions, and multiphase flows.</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/neko">NEKO</a></p></td>
<td><p>Neko is a portable framework for high-order spectral element flow simulations.
Written in modern Fortran, Neko adopts an object-oriented approach, allowing
multi-tier abstractions of the solver stack and facilitating various hardware
backends ranging from general-purpose processors, CUDA and HIP enabled
accelerators to SX-Aurora vector processors.</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/nekrs">nekRS</a></p></td>
<td><p>nekRS is an open-source Navier Stokes solver based on the spectral element
method targeting classical processors and accelerators like GPUs.</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/openfoam">OpenFOAM</a></p></td>
<td><p>OpenFOAM is a free, open-source computational fluid dynamics (CFD)
tool developed primarily by OpenCFD Ltd. It has a large user
base across most areas of engineering and science, from both commercial and
academic organizations. OpenFOAM has extensive features to solve
anything from complex fluid flows involving chemical reactions, turbulence, and
heat transfer, to acoustics, solid mechanics, and electromagnetics.</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/pelec">PeleC</a></p></td>
<td><p>PeleC is an adaptive mesh refinement(AMR) solver for compressible reacting flows.</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/siemens-star-ccm">Simcenter Star-CCM+</a></p></td>
<td><p>Simcenter Star-CCM+ is a comprehensive computational fluid dynamics (CFD) and multiphysics
simulation tool developed by Siemens Digital Industries Software. It is designed to
help engineers and researchers analyze and optimize the performance of products and
systems across various industries.</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>Quantum Monte Carlo Simulation</p></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/qmcpack">QMCPACK</a></p></td>
<td><p>QMCPACK is an open-source production-level many-body ab initio Quantum
Monte Carlo code for computing the electronic structure of atoms, molecules, 2D
nanomaterials and solids. The solid-state capabilities include metallic systems
as well as insulators. QMCPACK is expected to run well on workstations through
to the latest generation supercomputers. Besides high performance, particular
emphasis is placed on code quality and reproducibility.</p></td>
</tr>
<tr class="row-even"><th class="stub"><p>Climate and weather</p></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/mpas">MPAS</a></p></td>
<td><p>The Model for Prediction Across Scales (MPAS) is a collaborative project for
developing atmosphere, ocean, and other earth-system simulation components
for use in climate, regional climate, and weather studies.</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>Energy, Oil, and Gas</p></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/devitopro">DevitoPRO</a></p></td>
<td><p>DevitoPRO is an advanced extension of the open-source Devito platform with added
features tailored for high-demand production workflows. It supports
high-performance computing (HPC) needs, especially in seismic imaging and inversion.
It is used to perform optimized finite difference (FD) computations
from high-level symbolic problem definitions. DevitoPro performs automated
code generation and Just-In-time (JIT) compilation based on symbolic equations
defined in SymPy to create and execute highly optimized Finite Difference stencil
kernels on multiple computer platforms.</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/srt-echelon">ECHELON</a></p></td>
<td><p>ECHELON by Stone Ridge Technology is a reservoir simulation tool. With
fast processing, it retains precise accuracy and preserves legacy simulator results.
Faster reservoir simulation enables reservoir engineers to produce many realizations,
address larger models, and use advanced physics. It opens new workflows based on
ensemble methodologies for history matching and forecasting that yield
increased accuracy and more predictive results.</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>Benchmark</p></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/rochpl">rocHPL</a></p></td>
<td><p>HPL, or High-Performance Linpack, is a benchmark which solves a uniformly
random system of linear equations and reports floating-point execution rate.</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/hpl-mxp">rocHPL-MxP</a></p></td>
<td><p>Benchmark that highlights the convergence of HPC and AI workloads by
solving a system of linear equations using novel, mixed-precision
algorithms.</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/hpcg">HPCG</a></p></td>
<td><p>HPCG, or the High Performance Conjugate Gradient Benchmark complements
the High Performance LINPACK (HPL) benchmark. The computational and data
access patterns of HPCG are designed to closely match a broad set of important
applications not represented by HPL, and to incentivize computer system
designers to invest in capabilities that will benefit the collective performance
of these applications.</p></td>
</tr>
<tr class="row-even"><th class="stub"><p>Tools and libraries</p></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/base-gpu-mpi-rocm-docker">AMD ROCm with OpenMPI container</a></p></td>
<td><p>Base container for GPU-aware MPI with ROCm for HPC applications. This
project provides a boilerplate for building and running a Docker
container with ROCm supporting GPU-aware MPI implementations using
OpenMPI or UCX.</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/base-mpich-rocm-docker">AMD ROCm with MPICH container</a></p></td>
<td><p>Base container for GPU-aware MPI with ROCm for HPC applications. This
project provides a boilerplate for building and running a Docker
container with ROCm supporting GPU-aware MPI implementations using MPICH.</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/kokkos">Kokkos</a></p></td>
<td><p>Kokkos is a programming model in C++ for writing performance portable
applications for use across HPC platforms. It provides abstractions for both
parallel execution of code and data management. Kokkos is designed to target
complex node architectures with N-level memory hierarchies and multiple types
of execution resources.</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/pyfr">PyFR</a></p></td>
<td><p>PyFR is an open-source Python based framework for solving advection-diffusion
type problems on streaming architectures using the Flux Reconstruction approach of
Huynh. The framework is designed to solve a range of governing systems on mixed
unstructured grids containing various element types. It is also designed to target a
range of hardware platforms via use of an in-built domain specific language derived
from the Mako templating engine.</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/petsc">PETSc</a></p></td>
<td><p>Portable, Extensible Toolkit for Scientific Computation (PETSc) is a suite of data structures
and routines for the scalable (parallel) solution of scientific applications modeled by partial
differential equations. It supports MPI, GPUs through CUDA, HIP, and OpenCL,
as well as hybrid MPI-GPU parallelism. It also supports the NEC-SX Tsubasa Vector Engine.
PETSc also includes the Toolkit for Advanced Optimization (TAO) library.</p></td>
</tr>
<tr class="row-odd"><th class="stub"></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/raja">RAJA</a></p></td>
<td><p>RAJA is a library of C++ software abstractions, primarily developed at Lawrence
Livermore National Laboratory (LLNL), that enables architecture and programming
model portability for HPC applications.</p></td>
</tr>
<tr class="row-even"><th class="stub"></th>
<td><p><a class="reference external" href="https://github.com/amd/InfinityHub-CI/tree/main/trilinos">Trilinos</a></p></td>
<td><p>The Trilinos Project is an effort to develop algorithms and enabling technologies
within an object-oriented software framework for the solution of large-scale,
complex multi-physics engineering and scientific problems.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<p>To learn about ROCm for AI applications, see <a class="reference internal" href="#document-how-to/rocm-for-ai/index"><span class="doc">Use ROCm for AI</span></a>.</p>
</section>
<span id="document-how-to/system-optimization/index"></span><section id="system-optimization">
<h2>System optimization<a class="headerlink" href="#system-optimization" title="Link to this heading">#</a></h2>
<p>This guide outlines system setup and tuning suggestions for AMD hardware to
optimize performance for specific types of workloads or use-cases. The contents are structured according to the hardware:</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-2 sd-row-cols-xs-2 sd-row-cols-sm-2 sd-row-cols-md-2 sd-row-cols-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
AMD RDNA</div>
<ul class="simple">
<li><p class="sd-card-text"><a class="reference internal" href="#document-how-to/system-optimization/w6000-v620"><span class="doc">AMD RDNA2 system optimization</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
AMD Instinct</div>
<ul class="simple">
<li><p class="sd-card-text"><a class="reference external" href="https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/system-optimization/mi300x.html">AMD Instinct MI300X</a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/system-optimization/mi300a.html">AMD Instinct MI300A</a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/system-optimization/mi200.html">AMD Instinct MI200</a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/system-optimization/mi100.html">AMD Instinct MI100</a></p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<span id="document-how-to/gpu-performance/mi300x"></span><section id="amd-instinct-mi300x-performance-guides">
<h2>AMD Instinct MI300X performance guides<a class="headerlink" href="#amd-instinct-mi300x-performance-guides" title="Link to this heading">#</a></h2>
<p>The following performance guides provide essential guidance on the necessary
steps to properly <a class="reference external" href="https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/system-optimization/mi300x.html">configure your system for AMD Instinct™ MI300X accelerators</a>.
They include detailed instructions on system settings and application
<a class="reference internal" href="#document-how-to/rocm-for-ai/inference-optimization/workload"><span class="doc">workload tuning</span></a> to
help you leverage the maximum capabilities of these accelerators and achieve
superior performance.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/system-optimization/mi300x.html">AMD Instinct MI300X system optimization</a>
covers essential system settings and system management practices to configure
your AMD Instinct MI300X system for performance.</p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/inference-optimization/workload"><span class="doc">AMD Instinct MI300X workload optimization</span></a> covers steps to
optimize the performance of AMD Instinct MI300X series accelerators for HPC
and deep learning operations.</p></li>
<li><p><a class="reference internal" href="#document-how-to/rocm-for-ai/inference/benchmark-docker/vllm"><span class="doc">vLLM inference performance testing</span></a> introduces a preconfigured
environment for LLM inference, designed to help you test performance with
popular models on AMD Instinct MI300X series accelerators.</p></li>
</ul>
</section>
<span id="document-how-to/system-debugging"></span><section class="tex2jax_ignore mathjax_ignore" id="system-debugging">
<h2>System debugging<a class="headerlink" href="#system-debugging" title="Link to this heading">#</a></h2>
<section id="rocm-language-and-system-level-debug-flags-and-environment-variables">
<h3>ROCm language and system-level debug, flags, and environment variables<a class="headerlink" href="#rocm-language-and-system-level-debug-flags-and-environment-variables" title="Link to this heading">#</a></h3>
<p>Kernel options to avoid: the Ethernet port getting renamed every time you change graphics cards, <code class="docutils literal notranslate"><span class="pre">net.ifnames=0</span> <span class="pre">biosdevname=0</span></code></p>
</section>
<section id="rocr-error-code">
<h3>ROCr error code<a class="headerlink" href="#rocr-error-code" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>2 Invalid Dimension</p></li>
<li><p>4 Invalid Group Memory</p></li>
<li><p>8 Invalid (or Null) Code</p></li>
<li><p>32 Invalid Format</p></li>
<li><p>64 Group is too large</p></li>
<li><p>128 Out of VGPRs</p></li>
<li><p>0x80000000 Debug Options</p></li>
</ul>
</section>
<section id="command-to-dump-firmware-version-and-get-linux-kernel-version">
<h3>Command to dump firmware version and get Linux kernel version<a class="headerlink" href="#command-to-dump-firmware-version-and-get-linux-kernel-version" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">cat</span> <span class="pre">/sys/kernel/debug/dri/1/amdgpu_firmware_info</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">uname</span> <span class="pre">-a</span></code></p>
</section>
<section id="debug-flags">
<h3>Debug flags<a class="headerlink" href="#debug-flags" title="Link to this heading">#</a></h3>
<p>Debug messages when developing/debugging base ROCm driver. You could enable the printing from <code class="docutils literal notranslate"><span class="pre">libhsakmt.so</span></code> by setting an environment variable, <code class="docutils literal notranslate"><span class="pre">HSAKMT_DEBUG_LEVEL</span></code>. Available debug levels are 3-7. The higher level you set, the more messages will print.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">HSAKMT_DEBUG_LEVEL=3</span></code> : Only pr_err() prints.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">HSAKMT_DEBUG_LEVEL=4</span></code> : pr_err() and pr_warn() print.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">HSAKMT_DEBUG_LEVEL=5</span></code> : We currently do not implement “notice”. Setting to 5 is same as setting to 4.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">HSAKMT_DEBUG_LEVEL=6</span></code> : pr_err(), pr_warn(), and pr_info print.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">HSAKMT_DEBUG_LEVEL=7</span></code> : Everything including pr_debug prints.</p></li>
</ul>
</section>
<section id="rocr-level-environment-variables-for-debug">
<h3>ROCr level environment variables for debug<a class="headerlink" href="#rocr-level-environment-variables-for-debug" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">HSA_ENABLE_SDMA=0</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">HSA_ENABLE_INTERRUPT=0</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">HSA_SVM_GUARD_PAGES=0</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">HSA_DISABLE_CACHE=1</span></code></p>
</section>
<section id="turn-off-page-retry-on-gfx9-vega-devices">
<h3>Turn off page retry on GFX9/Vega devices<a class="headerlink" href="#turn-off-page-retry-on-gfx9-vega-devices" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">-s</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">echo</span> <span class="pre">1</span> <span class="pre">&gt;</span> <span class="pre">/sys/module/amdkfd/parameters/noretry</span></code></p>
</section>
<section id="hip-environment-variables-3-x">
<h3>HIP environment variables 3.x<a class="headerlink" href="#hip-environment-variables-3-x" title="Link to this heading">#</a></h3>
<section id="opencl-debug-flags">
<h4>OpenCL debug flags<a class="headerlink" href="#opencl-debug-flags" title="Link to this heading">#</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">AMD_OCL_WAIT_COMMAND=1</span> <span class="pre">(0</span> <span class="pre">=</span> <span class="pre">OFF,</span> <span class="pre">1</span> <span class="pre">=</span> <span class="pre">On)</span></code></p>
</section>
</section>
<section id="pcie-debug">
<h3>PCIe-debug<a class="headerlink" href="#pcie-debug" title="Link to this heading">#</a></h3>
<p>For information on how to debug and profile HIP applications, see <a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/how-to/debugging.html" title="(in HIP Documentation v6.4.43484)"><span>Debugging with HIP</span></a></p>
</section>
</section>
<span id="document-conceptual/compiler-topics"></span><head>
<meta charset="utf-8"/>
<meta content="AMD ROCm documentation" name="description"/>
<meta content="documentation, guides, installation, compatibility, support,
  reference, ROCm, AMD" name="keywords"/>
</head>
<section class="tex2jax_ignore mathjax_ignore" id="using-compiler-features">
<h2>Using compiler features<a class="headerlink" href="#using-compiler-features" title="Link to this heading">#</a></h2>
<p>The following topics describe using specific features of the compilation tools:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/index.html">ROCm compiler infrastructure</a></p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/conceptual/using-gpu-sanitizer.html">Using AddressSanitizer</a></p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/conceptual/openmp.html">OpenMP support</a></p></li>
</ul>
<div class="toctree-wrapper compound">
</div>
</section>
<span id="document-how-to/setting-cus"></span><section id="setting-the-number-of-compute-units">
<span id="settings-cus-reference"></span><h2>Setting the number of compute units<a class="headerlink" href="#setting-the-number-of-compute-units" title="Link to this heading">#</a></h2>
<p>The GPU driver provides two environment variables to set the number of CUs used:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">HSA_CU_MASK</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ROC_GLOBAL_CU_MASK</span></code></p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">ROC_GLOBAL_CU_MASK</span></code> variable sets the CU mask on queues created by HIP or OpenCL runtimes. The <code class="docutils literal notranslate"><span class="pre">HSA_CU_MASK</span></code> variable sets the mask on a lower level of queue creation in the driver. It also sets the mask on the queues being profiled.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When using GPUs to accelerate compute workloads, it sometimes becomes necessary to configure the hardware’s usage of compute units (CU). This is a more advanced option, so please read this page before experimentation.</p>
</div>
<p>The environment variables have the following syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ID</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="o">-</span><span class="mi">9</span><span class="p">][</span><span class="mi">0</span><span class="o">-</span><span class="mi">9</span><span class="p">]</span><span class="o">*</span>                         <span class="n">ex</span><span class="o">.</span> <span class="n">base</span> <span class="mi">10</span> <span class="n">numbers</span>
<span class="n">ID_list</span> <span class="o">=</span> <span class="p">(</span><span class="n">ID</span> <span class="o">|</span> <span class="n">ID</span><span class="o">-</span><span class="n">ID</span><span class="p">)[,</span> <span class="p">(</span><span class="n">ID</span> <span class="o">|</span> <span class="n">ID</span><span class="o">-</span><span class="n">ID</span><span class="p">)]</span><span class="o">*</span>  <span class="n">ex</span><span class="o">.</span> <span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">7</span>
<span class="n">GPU_list</span> <span class="o">=</span> <span class="n">ID_list</span>                       <span class="n">ex</span><span class="o">.</span> <span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">7</span>
<span class="n">CU_list</span> <span class="o">=</span> <span class="mi">0</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="o">-</span><span class="n">F</span><span class="p">]</span><span class="o">*</span> <span class="o">|</span> <span class="n">ID_list</span>             <span class="n">ex</span><span class="o">.</span> <span class="mh">0x337F</span> <span class="n">OR</span> <span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">7</span>
<span class="n">CU_Set</span> <span class="o">=</span> <span class="n">GPU_list</span> <span class="p">:</span> <span class="n">CU_list</span>              <span class="n">ex</span><span class="o">.</span> <span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">7</span><span class="p">:</span><span class="mi">0</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span><span class="mi">32</span><span class="o">-</span><span class="mi">47</span> <span class="n">OR</span> <span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">7</span><span class="p">:</span><span class="mh">0x337F</span>
<span class="n">HSA_CU_MASK</span> <span class="o">=</span> <span class="n">CU_Set</span> <span class="p">[;</span> <span class="n">CU_Set</span><span class="p">]</span><span class="o">*</span>         <span class="n">ex</span><span class="o">.</span> <span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">7</span><span class="p">:</span><span class="mi">0</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span><span class="mi">32</span><span class="o">-</span><span class="mi">47</span><span class="p">;</span> <span class="mi">3</span><span class="o">-</span><span class="mi">9</span><span class="p">:</span><span class="mh">0x337F</span>
</pre></div>
</div>
<p>The GPU indices are taken post <code class="docutils literal notranslate"><span class="pre">ROCR_VISIBLE_DEVICES</span></code> reordering. The listed or masked CUs are enabled for listed GPUs, and the others are disabled. Unlisted GPUs are not be affected, and their CUs are enabled.</p>
<p>The variable parsing stops when a syntax error occurs. The erroneous set and the following are ignored. Repeating GPU or CU IDs results in a syntax error. Specifying a mask with no usable CUs (CU_list is 0x0) results in a syntax error. To exclude GPU devices, use <code class="docutils literal notranslate"><span class="pre">ROCR_VISIBLE_DEVICES</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These environment variables only affect ROCm software, not graphics applications.</p>
</div>
<p>Not all CU configurations are valid on all devices. For example, on devices where two CUs can be combined into a WGP (for kernels running in WGP mode), it’s not valid to disable only a single CU in a WGP.</p>
</section>
<span id="document-how-to/Bar-Memory"></span><section id="troubleshoot-bar-access-limitation">
<h2>Troubleshoot BAR access limitation<a class="headerlink" href="#troubleshoot-bar-access-limitation" title="Link to this heading">#</a></h2>
<p>Direct Memory Access (DMA) to PCIe devices using Base Address Registers (BARs) can be restricted due to physical addressing limits. These restrictions can result in data access failures between the system components. Peer-to-peer (P2P) DMA is used to access resources such as registers and memory between devices. PCIe devices need memory-mapped input/output (MMIO) space for DMA, and these MMIO spaces are defined in the PCIe BARs.</p>
<p>These BARs are a set of 32-bit or 64-bit registers that are used to define the resources that PCIe devices provide. The CPU and other system devices also use these to access the resources of the PCIe devices. P2P DMA only works when one device can directly access the local BAR memory of another. If the memory address of a BAR memory exceeds the physical addressing limit of a device, the device will not be able to access that BAR. This could be the device’s own BAR or the BAR of another device in the system.</p>
<p>If the BAR memory exceeds than the physical addressing limit of the device, the device will not be able to access the remote BAR.</p>
<p>To handle any BAR access issues that might occur, you need to be aware of the physical address limitations of the devices and understand the <a class="reference internal" href="#bar-configuration"><span class="std std-ref">BAR configuration of AMD GPUs</span></a>. This information is important when setting up additional MMIO apertures for PCIe devices in the system’s physical address space.</p>
<section id="handling-physical-address-limitation">
<h3>Handling physical address limitation<a class="headerlink" href="#handling-physical-address-limitation" title="Link to this heading">#</a></h3>
<p>When a system boots, the system BIOS allocates the physical address space for the components in the system, including system memory and MMIO apertures. On modern 64-bit platforms, there are generally two or more MMIO apertures: one located below 4 GB of physical address space for 32-bit compatibility, and one or more above 4 GB for devices needing more space.</p>
<p>You can control the memory address of the high MMIO aperture from the system BIOS configuration options. This lets you configure the additional MMIO space to align with the physical addressing limit and allows P2P DMA between the devices. For example, if a PCIe device is limited to 44-bit of physical addressing, you should ensure that the MMIO aperture is set below 44-bit in the system physical address space.</p>
<p>There are two ways to handle this:</p>
<ul class="simple">
<li><p>Ensure that the high MMIO aperture is within the physical addressing limits of the devices in the system. For example, if the devices have a 44-bit physical addressing limit, set the <code class="docutils literal notranslate"><span class="pre">MMIO</span> <span class="pre">High</span> <span class="pre">Base</span></code> and <code class="docutils literal notranslate"><span class="pre">MMIO</span> <span class="pre">High</span> <span class="pre">size</span></code> options in the BIOS such that the aperture is within the 44-bit address range, and ensure that the <code class="docutils literal notranslate"><span class="pre">Above</span> <span class="pre">4G</span> <span class="pre">Decoding</span></code> option is Enabled.</p></li>
<li><p>Enable the Input-Output Memory Management Unit (IOMMU). When the IOMMU is enabled in non-passthrough mode, it will create a virtual I/O address space for each device on the system. It also ensures that all virtual addresses created in that space are within the physical addressing limits of the device. For more information on IOMMU, see <a class="reference external" href="https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/conceptual/iommu.html">Input-Output Memory Management Unit (IOMMU)</a>.</p></li>
</ul>
</section>
<section id="bar-configuration-for-amd-gpus">
<span id="bar-configuration"></span><h3>BAR configuration for AMD GPUs<a class="headerlink" href="#bar-configuration-for-amd-gpus" title="Link to this heading">#</a></h3>
<p>The following table shows how the BARs are configured for AMD GPUs.</p>
<div class="pst-scrollable-table-container"><table class="table">
<colgroup>
<col style="width: 25.0%"/>
<col style="width: 25.0%"/>
<col style="width: 50.0%"/>
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>BAR Type</p></th>
<th class="head"><p>Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BAR0-1 registers</p></td>
<td><p>64-bit, Prefetchable, GPU memory</p></td>
<td><p>8 GB or 16 GB depending on GPU. Set to less than 2^44 to support P2P access from other GPUs with a 44-bit physical address limit. Prefetchable memory enables faster read operation for high-performance computing (HPC) by fetching the contiguous data from the same data source even before requested as an anticipation of a future request.</p></td>
</tr>
<tr class="row-odd"><td><p>BAR2-3 registers</p></td>
<td><p>64-bit, Prefetchable, Doorbell</p></td>
<td><p>Set to less than 2^44 to support P2P access from other GPUs with a 44-bit physical address limit. As a Doorbell BAR, it indicates to the GPU that a new operation is in its queue to be processed.</p></td>
</tr>
<tr class="row-even"><td><p>BAR4 register</p></td>
<td><p>Optional</p></td>
<td><p>Not a boot device</p></td>
</tr>
<tr class="row-odd"><td><p>BAR5 register</p></td>
<td><p>32-bit, Non-prefetchable, MMIO</p></td>
<td><p>Is set to less than 4 GB.</p></td>
</tr>
</tbody>
</table>
</div>
<section id="example-of-bar-usage-on-amd-gpus">
<h4>Example of BAR usage on AMD GPUs<a class="headerlink" href="#example-of-bar-usage-on-amd-gpus" title="Link to this heading">#</a></h4>
<p>Following is an example configuration of BARs set by the system BIOS on GFX8 GPUs with the 40-bit physical addressing limit:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="m">11</span>:00.0<span class="w"> </span>Display<span class="w"> </span>controller:<span class="w"> </span>Advanced<span class="w"> </span>Micro<span class="w"> </span>Devices,<span class="w"> </span>Inc.<span class="w"> </span><span class="o">[</span>AMD/ATI<span class="o">]</span><span class="w"> </span>Fiji<span class="w"> </span><span class="o">[</span>Radeon<span class="w"> </span>R9<span class="w"> </span>FURY<span class="w"> </span>/<span class="w"> </span>NANO
Series<span class="o">]</span><span class="w"> </span><span class="o">(</span>rev<span class="w"> </span>c1<span class="o">)</span>

Subsystem:<span class="w"> </span>Advanced<span class="w"> </span>Micro<span class="w"> </span>Devices,<span class="w"> </span>Inc.<span class="w"> </span><span class="o">[</span>AMD/ATI<span class="o">]</span><span class="w"> </span>Device<span class="w"> </span>0b35

Flags:<span class="w"> </span>bus<span class="w"> </span>master,<span class="w"> </span>fast<span class="w"> </span>devsel,<span class="w"> </span>latency<span class="w"> </span><span class="m">0</span>,<span class="w"> </span>IRQ<span class="w"> </span><span class="m">119</span>

Memory<span class="w"> </span>at<span class="w"> </span>bf40000000<span class="w"> </span><span class="o">(</span><span class="m">64</span>-bit,<span class="w"> </span>prefetchable<span class="o">)</span><span class="w"> </span><span class="o">[</span><span class="nv">size</span><span class="o">=</span>256M<span class="o">]</span>

Memory<span class="w"> </span>at<span class="w"> </span>bf50000000<span class="w"> </span><span class="o">(</span><span class="m">64</span>-bit,<span class="w"> </span>prefetchable<span class="o">)</span><span class="w"> </span><span class="o">[</span><span class="nv">size</span><span class="o">=</span>2M<span class="o">]</span>

I/O<span class="w"> </span>ports<span class="w"> </span>at<span class="w"> </span><span class="m">3000</span><span class="w"> </span><span class="o">[</span><span class="nv">size</span><span class="o">=</span><span class="m">256</span><span class="o">]</span>

Memory<span class="w"> </span>at<span class="w"> </span>c7400000<span class="w"> </span><span class="o">(</span><span class="m">32</span>-bit,<span class="w"> </span>non-prefetchable<span class="o">)</span><span class="w"> </span><span class="o">[</span><span class="nv">size</span><span class="o">=</span>256K<span class="o">]</span>

Expansion<span class="w"> </span>ROM<span class="w"> </span>at<span class="w"> </span>c7440000<span class="w"> </span><span class="o">[</span>disabled<span class="o">]</span><span class="w"> </span><span class="o">[</span><span class="nv">size</span><span class="o">=</span>128K<span class="o">]</span>
</pre></div>
</div>
<p>Details of the BARs configured in the example are:</p>
<p><strong>GPU Frame Buffer BAR:</strong> <code class="docutils literal notranslate"><span class="pre">Memory</span> <span class="pre">at</span> <span class="pre">bf40000000</span> <span class="pre">(64-bit,</span> <span class="pre">prefetchable)</span> <span class="pre">[size=256M]</span></code></p>
<p>The size of the BAR in the example is 256 MB. Generally, it will be the size of the GPU memory (typically 4 GB+). Depending upon the physical address limit and generation of AMD GPUs, the BAR can be set below 2^40, 2^44, or 2^48.</p>
<p><strong>Doorbell BAR:</strong> <code class="docutils literal notranslate"><span class="pre">Memory</span> <span class="pre">at</span> <span class="pre">bf50000000</span> <span class="pre">(64-bit,</span> <span class="pre">prefetchable)</span> <span class="pre">[size=2M]</span></code></p>
<p>The size of the BAR should typically be less than 10 MB for this generation of GPUs and has been set to 2 MB in the example. This BAR is placed less than 2^40 to allow peer-to-peer access from other generations of AMD GPUs.</p>
<p><strong>I/O BAR:</strong> <code class="docutils literal notranslate"><span class="pre">I/O</span> <span class="pre">ports</span> <span class="pre">at</span> <span class="pre">3000</span> <span class="pre">[size=256]</span></code></p>
<p>This is for legacy VGA and boot device support. Because the GPUs used are not connected to a display (VGA devices), this is not a concern, even if it isn’t set up in the system BIOS.</p>
<p><strong>MMIO BAR:</strong> <code class="docutils literal notranslate"><span class="pre">Memory</span> <span class="pre">at</span> <span class="pre">c7400000</span> <span class="pre">(32-bit,</span> <span class="pre">non-prefetchable)</span> <span class="pre">[size=256K]</span></code></p>
<p>The AMD Driver requires this to access the configuration registers. Since the reminder of the BAR available is only 1 DWORD (32-bit), this is set less than 4 GB. In the example, it is fixed at 256 KB.</p>
<p><strong>Expansion ROM:</strong> <code class="docutils literal notranslate"><span class="pre">Expansion</span> <span class="pre">ROM</span> <span class="pre">at</span> <span class="pre">c7440000</span> <span class="pre">[disabled]</span> <span class="pre">[size=128K]</span></code></p>
<p>This is required by the AMD Driver to access the GPU video-BIOS. In the example, it is fixed at 128 KB.</p>
</section>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-conceptual/gpu-arch"></span><head>
<meta charset="utf-8"/>
<meta content="GPU architecture" name="description"/>
<meta content="GPU architecture, architecture support, MI200, MI250, RDNA,
  MI100, AMD Instinct" name="keywords"/>
</head>
<section class="tex2jax_ignore mathjax_ignore" id="gpu-architecture-documentation">
<span id="gpu-arch-documentation"></span><h2>GPU architecture documentation<a class="headerlink" href="#gpu-architecture-documentation" title="Link to this heading">#</a></h2>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-1 sd-row-cols-xs-1 sd-row-cols-sm-1 sd-row-cols-md-2 sd-row-cols-lg-2 sd-g-1 sd-g-xs-1 sd-g-sm-1 sd-g-md-1 sd-g-lg-1 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<p class="sd-card-text"><strong>AMD Instinct MI300 series</strong></p>
<p class="sd-card-text">Review hardware aspects of the AMD Instinct™ MI300 series of GPU accelerators and the CDNA™ 3
architecture.</p>
<ul class="simple">
<li><p class="sd-card-text"><a class="reference internal" href="#document-conceptual/gpu-arch/mi300"><span class="std std-doc">AMD Instinct™ MI300 microarchitecture</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-mi300-cdna3-instruction-set-architecture.pdf">AMD Instinct MI300/CDNA3 ISA</a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-3-white-paper.pdf">White paper</a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#document-conceptual/gpu-arch/mi300-mi200-performance-counters"><span class="std std-doc">Performance counters</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<p class="sd-card-text"><strong>AMD Instinct MI200 series</strong></p>
<p class="sd-card-text">Review hardware aspects of the AMD Instinct™ MI200 series of GPU accelerators and the CDNA™ 2
architecture.</p>
<ul class="simple">
<li><p class="sd-card-text"><a class="reference internal" href="#document-conceptual/gpu-arch/mi250"><span class="std std-doc">AMD Instinct™ MI250 microarchitecture</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://www.amd.com/system/files/TechDocs/instinct-mi200-cdna2-instruction-set-architecture.pdf">AMD Instinct MI200/CDNA2 ISA</a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna2-white-paper.pdf">White paper</a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#document-conceptual/gpu-arch/mi300-mi200-performance-counters"><span class="std std-doc">Performance counters</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<p class="sd-card-text"><strong>AMD Instinct MI100</strong></p>
<p class="sd-card-text">Review hardware aspects of the AMD Instinct™ MI100 series of GPU accelerators and the CDNA™ 1
architecture.</p>
<ul class="simple">
<li><p class="sd-card-text"><a class="reference internal" href="#document-conceptual/gpu-arch/mi100"><span class="std std-doc">AMD Instinct™ MI100 microarchitecture</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://www.amd.com/system/files/TechDocs/instinct-mi100-cdna1-shader-instruction-set-architecture%C2%A0.pdf">AMD Instinct MI100/CDNA1 ISA</a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna-white-paper.pdf">White paper</a></p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<p class="sd-card-text"><strong>RDNA</strong></p>
<ul class="simple">
<li><p class="sd-card-text"><a class="reference external" href="https://www.amd.com/content/dam/amd/en/documents/radeon-tech-docs/instruction-set-architectures/rdna4-instruction-set-architecture.pdf">AMD RDNA4 ISA</a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://www.amd.com/system/files/TechDocs/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf">AMD RDNA3 ISA</a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://www.amd.com/system/files/TechDocs/rdna2-shader-instruction-set-architecture.pdf">AMD RDNA2 ISA</a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://www.amd.com/system/files/TechDocs/rdna-shader-instruction-set-architecture.pdf">AMD RDNA ISA</a></p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<p class="sd-card-text"><strong>Older architectures</strong></p>
<ul class="simple">
<li><p class="sd-card-text"><a class="reference external" href="https://www.amd.com/system/files/TechDocs/vega-7nm-shader-instruction-set-architecture.pdf">AMD Instinct MI50/Vega 7nm ISA</a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://www.amd.com/system/files/TechDocs/vega-shader-instruction-set-architecture.pdf">AMD Instinct MI25/Vega ISA</a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://www.amd.com/system/files/TechDocs/gcn3-instruction-set-architecture.pdf">AMD GCN3 ISA</a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://en.wikichip.org/w/images/a/a1/vega-whitepaper.pdf">AMD Vega Architecture White Paper</a></p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper compound">
<span id="document-conceptual/gpu-arch/mi300"></span><section class="tex2jax_ignore mathjax_ignore" id="amd-instinct-mi300-series-microarchitecture">
<h3>AMD Instinct™ MI300 series microarchitecture<a class="headerlink" href="#amd-instinct-mi300-series-microarchitecture" title="Link to this heading">#</a></h3>
<p>The AMD Instinct MI300 series accelerators are based on the AMD CDNA 3
architecture which was designed to deliver leadership performance for HPC, artificial intelligence (AI), and machine
learning (ML) workloads. The AMD Instinct MI300 series accelerators are well-suited for extreme scalability and compute performance, running
on everything from individual servers to the world’s largest exascale supercomputers.</p>
<p>With the MI300 series, AMD is introducing the Accelerator Complex Die (XCD), which contains the
GPU computational elements of the processor along with the lower levels of the cache hierarchy.</p>
<p>The following image depicts the structure of a single XCD in the AMD Instinct MI300 accelerator series.</p>
<figure class="align-center" id="mi300-xcd">
<img alt="_images/xcd-sys-arch.png" src="_images/xcd-sys-arch.png"/>
<figcaption>
<p><span class="caption-text">XCD-level system architecture showing 40 Compute Units, each with 32 KB L1 cache, a Unified Compute System with 4 ACE Compute Accelerators, shared 4MB of L2 cache and an HWS Hardware Scheduler.</span><a class="headerlink" href="#mi300-xcd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>On the XCD, four Asynchronous Compute Engines (ACEs) send compute shader workgroups to the
Compute Units (CUs). The XCD has 40 CUs: 38 active CUs at the aggregate level and 2 disabled CUs for
yield management. The CUs all share a 4 MB L2 cache that serves to coalesce all memory traffic for the
die. With less than half of the CUs of the AMD Instinct MI200 Series compute die, the AMD CDNA™ 3
XCD die is a smaller building block. However, it uses more advanced packaging and the processor
can include 6 or 8 XCDs for up to 304 CUs, roughly 40% more than MI250X.</p>
<p>The MI300 Series integrate up to 8 vertically stacked XCDs, 8 stacks of
High-Bandwidth Memory 3 (HBM3) and 4 I/O dies (containing system
infrastructure) using the AMD Infinity Fabric™ technology as interconnect.</p>
<p>The Matrix Cores inside the CDNA 3 CUs have significant improvements, emphasizing AI and machine
learning, enhancing throughput of existing data types while adding support for new data types.
CDNA 2 Matrix Cores support FP16 and BF16, while offering INT8 for inference. Compared to MI250X
accelerators, CDNA 3 Matrix Cores triple the performance for FP16 and BF16, while providing a
performance gain of 6.8 times for INT8. FP8 has a performance gain of 16 times compared to FP32,
while TF32 has a gain of 4 times compared to FP32.</p>
<div class="pst-scrollable-table-container"><table class="table" id="mi300x-perf-table">
<caption><span class="caption-text">Peak-performance capabilities of the MI300X for different data types.</span><a class="headerlink" href="#mi300x-perf-table" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Computation and Data Type</p></th>
<th class="head"><p>FLOPS/CLOCK/CU</p></th>
<th class="head"><p>Peak TFLOPS</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Matrix FP64</p></td>
<td><p>256</p></td>
<td><p>163.4</p></td>
</tr>
<tr class="row-odd"><td><p>Vector FP64</p></td>
<td><p>128</p></td>
<td><p>81.7</p></td>
</tr>
<tr class="row-even"><td><p>Matrix FP32</p></td>
<td><p>256</p></td>
<td><p>163.4</p></td>
</tr>
<tr class="row-odd"><td><p>Vector FP32</p></td>
<td><p>256</p></td>
<td><p>163.4</p></td>
</tr>
<tr class="row-even"><td><p>Vector TF32</p></td>
<td><p>1024</p></td>
<td><p>653.7</p></td>
</tr>
<tr class="row-odd"><td><p>Matrix FP16</p></td>
<td><p>2048</p></td>
<td><p>1307.4</p></td>
</tr>
<tr class="row-even"><td><p>Matrix BF16</p></td>
<td><p>2048</p></td>
<td><p>1307.4</p></td>
</tr>
<tr class="row-odd"><td><p>Matrix FP8</p></td>
<td><p>4096</p></td>
<td><p>2614.9</p></td>
</tr>
<tr class="row-even"><td><p>Matrix INT8</p></td>
<td><p>4096</p></td>
<td><p>2614.9</p></td>
</tr>
</tbody>
</table>
</div>
<p>The above table summarizes the aggregated peak performance of the AMD Instinct MI300X Open
Compute Platform (OCP) Open Accelerator Modules (OAMs) for different data types and command
processors. The middle column lists the peak performance (number of data elements processed in a
single instruction) of a single compute unit if a SIMD (or matrix) instruction is submitted in each clock
cycle. The third column lists the theoretical peak performance of the OAM. The theoretical aggregated
peak memory bandwidth of the GPU is 5.3 TB per second.</p>
<p>The following image shows the block diagram of the APU (left) and the OAM package (right) both
connected via AMD Infinity Fabric™ network on-chip.</p>
<figure class="align-center" id="mi300-arch">
<img alt="" src="_images/image008.png"/>
<figcaption>
<p><span class="caption-text">MI300 series system architecture showing MI300A (left) with 6 XCDs and 3 CCDs, while the MI300X (right) has 8 XCDs.</span><a class="headerlink" href="#mi300-arch" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="node-level-architecture">
<h4>Node-level architecture<a class="headerlink" href="#node-level-architecture" title="Link to this heading">#</a></h4>
<figure class="align-center" id="mi300-node">
<img alt="_images/mi300-node-level-arch.png" src="_images/mi300-node-level-arch.png"/>
<figcaption>
<p><span class="caption-text">MI300 series node-level architecture showing 8 fully interconnected MI300X OAM modules connected to (optional) PCIEe switches via retimers and HGX connectors.</span><a class="headerlink" href="#mi300-node" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The image above shows the node-level architecture of a system with AMD EPYC processors in a
dual-socket configuration and eight AMD Instinct MI300X accelerators. The MI300X OAMs attach to the
host system via PCIe Gen 5 x16 links (yellow lines). The GPUs are using seven high-bandwidth,
low-latency AMD Infinity Fabric™ links (red lines) to form a fully connected 8-GPU system.</p>
<!---
We need performance data about the P2P communication here.
-->
</section>
<div class="toctree-wrapper compound">
<span id="document-conceptual/gpu-arch/mi300-mi200-performance-counters"></span><section id="mi300-and-mi200-series-performance-counters-and-metrics">
<h4>MI300 and MI200 series performance counters and metrics<a class="headerlink" href="#mi300-and-mi200-series-performance-counters-and-metrics" title="Link to this heading">#</a></h4>
<p>This document lists and describes the hardware performance counters and derived metrics available
for the AMD Instinct™ MI300 and MI200 GPU. You can also access this information using the
<a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler-sdk/en/latest/how-to/using-rocprofv3.html" title="(in Rocprofiler SDK v0.6.0)"><span class="xref std std-doc">ROCprofiler-SDK</span></a>.</p>
<section id="mi300-and-mi200-series-performance-counters">
<h5>MI300 and MI200 series performance counters<a class="headerlink" href="#mi300-and-mi200-series-performance-counters" title="Link to this heading">#</a></h5>
<p>Series performance counters include the following categories:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#command-processor-counters"><span class="std std-ref">Command processor counters</span></a></p></li>
<li><p><a class="reference internal" href="#graphics-register-bus-manager-counters"><span class="std std-ref">Graphics register bus manager counters</span></a></p></li>
<li><p><a class="reference internal" href="#spi-counters"><span class="std std-ref">Shader processor input counters</span></a></p></li>
<li><p><a class="reference internal" href="#compute-unit-counters"><span class="std std-ref">Compute unit counters</span></a></p></li>
<li><p><a class="reference internal" href="#l1i-and-sl1d-cache-counters"><span class="std std-ref">L1 instruction cache (L1i) and scalar L1 data cache (L1d) counters</span></a></p></li>
<li><p><a class="reference internal" href="#vector-l1-cache-subsystem-counters"><span class="std std-ref">Vector L1 cache subsystem counters</span></a></p></li>
<li><p><a class="reference internal" href="#l2-cache-access-counters"><span class="std std-ref">L2 cache access counters</span></a></p></li>
</ul>
<p>The following sections provide additional details for each category.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Preliminary validation of all MI300 and MI200 series performance counters is in progress. Those with
an asterisk (*) require further evaluation.</p>
</div>
<section id="command-processor-counters">
<span id="id1"></span><h6>Command processor counters<a class="headerlink" href="#command-processor-counters" title="Link to this heading">#</a></h6>
<p>Command processor counters are further classified into command processor-fetcher and command
processor-compute.</p>
<section id="command-processor-fetcher-counters">
<h6 aria-level="7">Command processor-fetcher counters<a class="headerlink" href="#command-processor-fetcher-counters" title="Link to this heading">#</a></h6>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Unit</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">CPF_CMP_UTCL1_STALL_ON_TRANSLATION</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles one of the compute unified translation caches (L1) is stalled waiting on translation</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">CPF_CPF_STAT_BUSY</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles command processor-fetcher is busy</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">CPF_CPF_STAT_IDLE</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles command processor-fetcher is idle</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">CPF_CPF_STAT_STALL</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles command processor-fetcher is stalled</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">CPF_CPF_TCIU_BUSY</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles command processor-fetcher texture cache interface unit interface is busy</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">CPF_CPF_TCIU_IDLE</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles command processor-fetcher texture cache interface unit interface is idle</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">CPF_CPF_TCIU_STALL</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles command processor-fetcher texture cache interface unit interface is stalled waiting on free tags</p></td>
</tr>
</tbody>
</table>
</div>
<p>The texture cache interface unit is the interface between the command processor and the memory
system.</p>
</section>
<section id="command-processor-compute-counters">
<h6 aria-level="7">Command processor-compute counters<a class="headerlink" href="#command-processor-compute-counters" title="Link to this heading">#</a></h6>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Unit</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">CPC_ME1_BUSY_FOR_PACKET_DECODE</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles command processor-compute micro engine is busy decoding packets</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">CPC_UTCL1_STALL_ON_TRANSLATION</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles one of the unified translation caches (L1) is stalled waiting on translation</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">CPC_CPC_STAT_BUSY</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles command processor-compute is busy</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">CPC_CPC_STAT_IDLE</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles command processor-compute is idle</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">CPC_CPC_STAT_STALL</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles command processor-compute is stalled</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">CPC_CPC_TCIU_BUSY</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles command processor-compute texture cache interface unit interface is busy</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">CPC_CPC_TCIU_IDLE</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles command processor-compute texture cache interface unit interface is idle</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">CPC_CPC_UTCL2IU_BUSY</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles command processor-compute unified translation cache (L2) interface is busy</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">CPC_CPC_UTCL2IU_IDLE</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles command processor-compute unified translation cache (L2) interface is idle</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">CPC_CPC_UTCL2IU_STALL</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles command processor-compute unified translation cache (L2) interface is stalled</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">CPC_ME1_DC0_SPI_BUSY</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles command processor-compute micro engine processor is busy</p></td>
</tr>
</tbody>
</table>
</div>
<p>The micro engine runs packet-processing firmware on the command processor-compute counter.</p>
</section>
</section>
<section id="graphics-register-bus-manager-counters">
<span id="id2"></span><h6>Graphics register bus manager counters<a class="headerlink" href="#graphics-register-bus-manager-counters" title="Link to this heading">#</a></h6>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Unit</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">GRBM_COUNT</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of free-running GPU cycles</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">GRBM_GUI_ACTIVE</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of GPU active cycles</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">GRBM_CP_BUSY</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles any of the command processor blocks are busy</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">GRBM_SPI_BUSY</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles any of the shader processor input is busy in the shader engines</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">GRBM_TA_BUSY</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles any of the texture addressing unit is busy in the shader engines</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">GRBM_TC_BUSY</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles any of the texture cache blocks are busy</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">GRBM_CPC_BUSY</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles the command processor-compute is busy</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">GRBM_CPF_BUSY</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles the command processor-fetcher is busy</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">GRBM_UTCL2_BUSY</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles the unified translation cache (Level 2 [L2]) block is busy</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">GRBM_EA_BUSY</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles the efficiency arbiter block is busy</p></td>
</tr>
</tbody>
</table>
</div>
<p>Texture cache blocks include:</p>
<ul class="simple">
<li><p>Texture cache arbiter</p></li>
<li><p>Texture cache per pipe, also known as vector Level 1 (L1) cache</p></li>
<li><p>Texture cache per channel, also known as known as L2 cache</p></li>
<li><p>Texture cache interface</p></li>
</ul>
</section>
<section id="shader-processor-input-counters">
<span id="spi-counters"></span><h6>Shader processor input counters<a class="headerlink" href="#shader-processor-input-counters" title="Link to this heading">#</a></h6>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Unit</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SPI_CSN_BUSY</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles with outstanding waves</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SPI_CSN_WINDOW_VALID</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles enabled by <code class="docutils literal notranslate"><span class="pre">perfcounter_start</span></code> event</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SPI_CSN_NUM_THREADGROUPS</span></code></p></td>
<td><p>Workgroups</p></td>
<td><p>Number of dispatched workgroups</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SPI_CSN_WAVE</span></code></p></td>
<td><p>Wavefronts</p></td>
<td><p>Number of dispatched wavefronts</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SPI_RA_REQ_NO_ALLOC</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of arbiter cycles with requests but no allocation</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SPI_RA_REQ_NO_ALLOC_CSN</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of arbiter cycles with compute shader (n<sup>th</sup> pipe) requests but no compute shader (n<sup>th</sup> pipe) allocation</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SPI_RA_RES_STALL_CSN</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of arbiter stall cycles due to shortage of compute shader (n<sup>th</sup> pipe) pipeline slots</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SPI_RA_TMP_STALL_CSN</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of stall cycles due to shortage of temp space</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SPI_RA_WAVE_SIMD_FULL_CSN</span></code></p></td>
<td><p>SIMD-cycles</p></td>
<td><p>Accumulated number of single instruction, multiple data (SIMD) per cycle affected by shortage of wave slots for compute shader (n<sup>th</sup> pipe) wave dispatch</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SPI_RA_VGPR_SIMD_FULL_CSN</span></code></p></td>
<td><p>SIMD-cycles</p></td>
<td><p>Accumulated number of SIMDs per cycle affected by shortage of vector general-purpose register (VGPR) slots for compute shader (n<sup>th</sup> pipe) wave dispatch</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SPI_RA_SGPR_SIMD_FULL_CSN</span></code></p></td>
<td><p>SIMD-cycles</p></td>
<td><p>Accumulated number of SIMDs per cycle affected by shortage of scalar general-purpose register (SGPR) slots for compute shader (n<sup>th</sup> pipe) wave dispatch</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SPI_RA_LDS_CU_FULL_CSN</span></code></p></td>
<td><p>CU</p></td>
<td><p>Number of compute units affected by shortage of local data share (LDS) space for compute shader (n<sup>th</sup> pipe) wave dispatch</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SPI_RA_BAR_CU_FULL_CSN</span></code></p></td>
<td><p>CU</p></td>
<td><p>Number of compute units with compute shader (n<sup>th</sup> pipe) waves waiting at a BARRIER</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SPI_RA_BULKY_CU_FULL_CSN</span></code></p></td>
<td><p>CU</p></td>
<td><p>Number of compute units with compute shader (n<sup>th</sup> pipe) waves waiting for BULKY resource</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SPI_RA_TGLIM_CU_FULL_CSN</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of compute shader (n<sup>th</sup> pipe) wave stall cycles due to restriction of <code class="docutils literal notranslate"><span class="pre">tg_limit</span></code> for thread group size</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SPI_RA_WVLIM_STALL_CSN</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles compute shader (n<sup>th</sup> pipe) is stalled due to <code class="docutils literal notranslate"><span class="pre">WAVE_LIMIT</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SPI_VWC_CSC_WR</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles taken to initialize VGPRs when launching waves</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SPI_SWC_CSC_WR</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles taken to initialize SGPRs when launching waves</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="compute-unit-counters">
<span id="id3"></span><h6>Compute unit counters<a class="headerlink" href="#compute-unit-counters" title="Link to this heading">#</a></h6>
<p>The compute unit counters are further classified into instruction mix, matrix fused multiply-add (FMA)
operation counters, level counters, wavefront counters, wavefront cycle counters, and LDS counters.</p>
<section id="instruction-mix">
<h6 aria-level="7">Instruction mix<a class="headerlink" href="#instruction-mix" title="Link to this heading">#</a></h6>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Unit</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of instructions issued</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of vector arithmetic logic unit (VALU) instructions including matrix FMA issued</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_ADD_F16</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of VALU half-precision floating-point (F16) <code class="docutils literal notranslate"><span class="pre">ADD</span></code> or <code class="docutils literal notranslate"><span class="pre">SUB</span></code> instructions issued</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_MUL_F16</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of VALU F16 Multiply instructions issued</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_FMA_F16</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of VALU F16 FMA or multiply-add instructions issued</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_TRANS_F16</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of VALU F16 Transcendental instructions issued</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_ADD_F32</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of VALU full-precision floating-point (F32) <code class="docutils literal notranslate"><span class="pre">ADD</span></code> or <code class="docutils literal notranslate"><span class="pre">SUB</span></code> instructions issued</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_MUL_F32</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of VALU F32 Multiply instructions issued</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_FMA_F32</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of VALU F32 FMAor multiply-add instructions issued</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_TRANS_F32</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of VALU F32 Transcendental instructions issued</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_ADD_F64</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of VALU F64 <code class="docutils literal notranslate"><span class="pre">ADD</span></code> or <code class="docutils literal notranslate"><span class="pre">SUB</span></code> instructions issued</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_MUL_F64</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of VALU F64 Multiply instructions issued</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_FMA_F64</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of VALU F64 FMA or multiply-add instructions issued</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_TRANS_F64</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of VALU F64 Transcendental instructions issued</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_INT32</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of VALU 32-bit integer instructions (signed or unsigned) issued</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_INT64</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of VALU 64-bit integer instructions (signed or unsigned) issued</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_CVT</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of VALU Conversion instructions issued</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_MFMA_I8</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of 8-bit Integer matrix FMA instructions issued</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_MFMA_F16</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of F16 matrix FMA instructions issued</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_MFMA_F32</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of F32 matrix FMA instructions issued</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_MFMA_F64</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of F64 matrix FMA instructions issued</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_MFMA</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of matrix FMA instructions issued</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VMEM_WR</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of vector memory write instructions (including flat) issued</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VMEM_RD</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of vector memory read instructions (including flat) issued</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VMEM</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of vector memory instructions issued, including both flat and buffer instructions</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_SALU</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of scalar arithmetic logic unit (SALU) instructions issued</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_SMEM</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of scalar memory instructions issued</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_SMEM_NORM</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of scalar memory instructions normalized to match <code class="docutils literal notranslate"><span class="pre">smem_level</span></code> issued</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_FLAT</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of flat instructions issued</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_FLAT_LDS_ONLY</span></code></p></td>
<td><p>Instr</p></td>
<td><p><strong>MI200 series only</strong> Number of FLAT instructions that read/write only from/to LDS issued. Works only if <code class="docutils literal notranslate"><span class="pre">EARLY_TA_DONE</span></code> is enabled.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_LDS</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of LDS instructions issued <strong>(MI200: includes flat; MI300: does not include flat)</strong></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_GDS</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of global data share instructions issued</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_EXP_GDS</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of EXP and global data share instructions excluding skipped export instructions issued</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_BRANCH</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of branch instructions issued</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_SENDMSG</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of <code class="docutils literal notranslate"><span class="pre">SENDMSG</span></code> instructions including <code class="docutils literal notranslate"><span class="pre">s_endpgm</span></code> issued</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VSKIPPED</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of vector instructions skipped</p></td>
</tr>
</tbody>
</table>
</div>
<p>Flat instructions allow read, write, and atomic access to a generic memory address pointer that can
resolve to any of the following physical memories:</p>
<ul class="simple">
<li><p>Global Memory</p></li>
<li><p>Scratch (“private”)</p></li>
<li><p>LDS (“shared”)</p></li>
<li><p>Invalid - <code class="docutils literal notranslate"><span class="pre">MEM_VIOL</span></code> TrapStatus</p></li>
</ul>
</section>
<section id="matrix-fused-multiply-add-operation-counters">
<h6 aria-level="7">Matrix fused multiply-add operation counters<a class="headerlink" href="#matrix-fused-multiply-add-operation-counters" title="Link to this heading">#</a></h6>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Unit</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_MFMA_MOPS_I8</span></code></p></td>
<td><p>IOP</p></td>
<td><p>Number of 8-bit integer matrix FMA ops in the unit of 512</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_MFMA_MOPS_F16</span></code></p></td>
<td><p>FLOP</p></td>
<td><p>Number of F16 floating matrix FMA ops in the unit of 512</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_MFMA_MOPS_BF16</span></code></p></td>
<td><p>FLOP</p></td>
<td><p>Number of BF16 floating matrix FMA ops in the unit of 512</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_MFMA_MOPS_F32</span></code></p></td>
<td><p>FLOP</p></td>
<td><p>Number of F32 floating matrix FMA ops in the unit of 512</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VALU_MFMA_MOPS_F64</span></code></p></td>
<td><p>FLOP</p></td>
<td><p>Number of F64 floating matrix FMA ops in the unit of 512</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="level-counters">
<h6 aria-level="7">Level counters<a class="headerlink" href="#level-counters" title="Link to this heading">#</a></h6>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All level counters must be followed by <code class="docutils literal notranslate"><span class="pre">SQ_ACCUM_PREV_HIRES</span></code> counter to measure average latency.</p>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Unit</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_ACCUM_PREV</span></code></p></td>
<td><p>Count</p></td>
<td><p>Accumulated counter sample value where accumulation takes place once every four cycles</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_ACCUM_PREV_HIRES</span></code></p></td>
<td><p>Count</p></td>
<td><p>Accumulated counter sample value where accumulation takes place once every cycle</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_LEVEL_WAVES</span></code></p></td>
<td><p>Waves</p></td>
<td><p>Number of inflight waves</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INST_LEVEL_VMEM</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of inflight vector memory (including flat) instructions</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INST_LEVEL_SMEM</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of inflight scalar memory instructions</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INST_LEVEL_LDS</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of inflight LDS (including flat) instructions</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_IFETCH_LEVEL</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of inflight instruction fetch requests from the cache</p></td>
</tr>
</tbody>
</table>
</div>
<p>Use the following formulae to calculate latencies:</p>
<ul class="simple">
<li><p>Vector memory latency = <code class="docutils literal notranslate"><span class="pre">SQ_ACCUM_PREV_HIRES</span></code> divided by <code class="docutils literal notranslate"><span class="pre">SQ_INSTS_VMEM</span></code></p></li>
<li><p>Wave latency = <code class="docutils literal notranslate"><span class="pre">SQ_ACCUM_PREV_HIRES</span></code> divided by <code class="docutils literal notranslate"><span class="pre">SQ_WAVE</span></code></p></li>
<li><p>LDS latency = <code class="docutils literal notranslate"><span class="pre">SQ_ACCUM_PREV_HIRES</span></code> divided by <code class="docutils literal notranslate"><span class="pre">SQ_INSTS_LDS</span></code></p></li>
<li><p>Scalar memory latency = <code class="docutils literal notranslate"><span class="pre">SQ_ACCUM_PREV_HIRES</span></code> divided by <code class="docutils literal notranslate"><span class="pre">SQ_INSTS_SMEM_NORM</span></code></p></li>
<li><p>Instruction fetch latency = <code class="docutils literal notranslate"><span class="pre">SQ_ACCUM_PREV_HIRES</span></code> divided by <code class="docutils literal notranslate"><span class="pre">SQ_IFETCH</span></code></p></li>
</ul>
</section>
<section id="wavefront-counters">
<h6 aria-level="7">Wavefront counters<a class="headerlink" href="#wavefront-counters" title="Link to this heading">#</a></h6>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Unit</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_WAVES</span></code></p></td>
<td><p>Waves</p></td>
<td><p>Number of wavefronts dispatched to sequencers, including both new and restored wavefronts</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_WAVES_SAVED</span></code></p></td>
<td><p>Waves</p></td>
<td><p>Number of context-saved waves</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_WAVES_RESTORED</span></code></p></td>
<td><p>Waves</p></td>
<td><p>Number of context-restored waves sent to sequencers</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_WAVES_EQ_64</span></code></p></td>
<td><p>Waves</p></td>
<td><p>Number of wavefronts with exactly 64 active threads sent to sequencers</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_WAVES_LT_64</span></code></p></td>
<td><p>Waves</p></td>
<td><p>Number of wavefronts with less than 64 active threads sent to sequencers</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_WAVES_LT_48</span></code></p></td>
<td><p>Waves</p></td>
<td><p>Number of wavefronts with less than 48 active threads sent to sequencers</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_WAVES_LT_32</span></code></p></td>
<td><p>Waves</p></td>
<td><p>Number of wavefronts with less than 32 active threads sent to sequencers</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_WAVES_LT_16</span></code></p></td>
<td><p>Waves</p></td>
<td><p>Number of wavefronts with less than 16 active threads sent to sequencers</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="wavefront-cycle-counters">
<h6 aria-level="7">Wavefront cycle counters<a class="headerlink" href="#wavefront-cycle-counters" title="Link to this heading">#</a></h6>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Unit</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_CYCLES</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Clock cycles</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_BUSY_CYCLES</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles while sequencers reports it to be busy</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_BUSY_CU_CYCLES</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles each compute unit is busy</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_VALU_MFMA_BUSY_CYCLES</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles the matrix FMA arithmetic logic unit (ALU) is busy</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_WAVE_CYCLES</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles spent by waves in the compute units</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_WAIT_ANY</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles spent waiting for anything</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_WAIT_INST_ANY</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles spent waiting for any instruction to be issued</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_ACTIVE_INST_ANY</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles spent by each wave to work on an instruction</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_ACTIVE_INST_VMEM</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles spent by the sequencer instruction arbiter to work on a vector memory instruction</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_ACTIVE_INST_LDS</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles spent by the sequencer instruction arbiter to work on an LDS instruction</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_ACTIVE_INST_VALU</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles spent by the sequencer instruction arbiter to work on a VALU instruction</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_ACTIVE_INST_SCA</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles spent by the sequencer instruction arbiter to work on a SALU or scalar memory instruction</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_ACTIVE_INST_EXP_GDS</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles spent by the sequencer instruction arbiter to work on an <code class="docutils literal notranslate"><span class="pre">EXPORT</span></code> or <code class="docutils literal notranslate"><span class="pre">GDS</span></code> instruction</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_ACTIVE_INST_MISC</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles spent by the sequencer instruction arbiter to work on a <code class="docutils literal notranslate"><span class="pre">BRANCH</span></code> or <code class="docutils literal notranslate"><span class="pre">SENDMSG</span></code> instruction</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_ACTIVE_INST_FLAT</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles spent by the sequencer instruction arbiter to work on a flat instruction</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INST_CYCLES_VMEM_WR</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles spent to send addr and cmd data for vector memory write instructions</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INST_CYCLES_VMEM_RD</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles spent to send addr and cmd data for vector memory read instructions</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INST_CYCLES_SMEM</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles spent to execute scalar memory reads</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_INST_CYCLES_SALU</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles spent to execute non-memory read scalar operations</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_THREAD_CYCLES_VALU</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles spent to execute VALU operations on active threads</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_WAIT_INST_LDS</span></code></p></td>
<td><p>Qcycles</p></td>
<td><p>Number of quad-cycles spent waiting for LDS instruction to be issued</p></td>
</tr>
</tbody>
</table>
</div>
<p><code class="docutils literal notranslate"><span class="pre">SQ_THREAD_CYCLES_VALU</span></code> is similar to <code class="docutils literal notranslate"><span class="pre">INST_CYCLES_VALU</span></code>, but it’s multiplied by the number of
active threads.</p>
</section>
<section id="lds-counters">
<h6 aria-level="7">LDS counters<a class="headerlink" href="#lds-counters" title="Link to this heading">#</a></h6>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Unit</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_LDS_ATOMIC_RETURN</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of atomic return cycles in LDS</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_LDS_BANK_CONFLICT</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles LDS is stalled by bank conflicts</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_LDS_ADDR_CONFLICT</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles LDS is stalled by address conflicts</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_LDS_UNALIGNED_STALL</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles LDS is stalled processing flat unaligned load or store operations</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_LDS_MEM_VIOLATIONS</span></code></p></td>
<td><p>Count</p></td>
<td><p>Number of threads that have a memory violation in the LDS</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_LDS_IDX_ACTIVE</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles LDS is used for indexed operations</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="miscellaneous-counters">
<h6 aria-level="7">Miscellaneous counters<a class="headerlink" href="#miscellaneous-counters" title="Link to this heading">#</a></h6>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Unit</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_IFETCH</span></code></p></td>
<td><p>Count</p></td>
<td><p>Number of instruction fetch requests from L1i, in 32-byte width</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQ_ITEMS</span></code></p></td>
<td><p>Threads</p></td>
<td><p>Number of valid items per wave</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="l1-instruction-cache-l1i-and-scalar-l1-data-cache-l1d-counters">
<span id="l1i-and-sl1d-cache-counters"></span><h6>L1 instruction cache (L1i) and scalar L1 data cache (L1d) counters<a class="headerlink" href="#l1-instruction-cache-l1i-and-scalar-l1-data-cache-l1d-counters" title="Link to this heading">#</a></h6>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Unit</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_ICACHE_REQ</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of L1 instruction (L1i) cache requests</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_ICACHE_HITS</span></code></p></td>
<td><p>Count</p></td>
<td><p>Number of L1i cache hits</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_ICACHE_MISSES</span></code></p></td>
<td><p>Count</p></td>
<td><p>Number of non-duplicate L1i cache misses including uncached requests</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_ICACHE_MISSES_DUPLICATE</span></code></p></td>
<td><p>Count</p></td>
<td><p>Number of duplicate L1i cache misses whose previous lookup miss on the same cache line is not fulfilled yet</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_DCACHE_REQ</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of scalar L1d requests</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_DCACHE_INPUT_VALID_READYB</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles while sequencer input is valid but scalar L1d is not ready</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_DCACHE_HITS</span></code></p></td>
<td><p>Count</p></td>
<td><p>Number of scalar L1d hits</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_DCACHE_MISSES</span></code></p></td>
<td><p>Count</p></td>
<td><p>Number of non-duplicate scalar L1d misses including uncached requests</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_DCACHE_MISSES_DUPLICATE</span></code></p></td>
<td><p>Count</p></td>
<td><p>Number of duplicate scalar L1d misses</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_DCACHE_REQ_READ_1</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of constant cache read requests in a single 32-bit data word</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_DCACHE_REQ_READ_2</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of constant cache read requests in two 32-bit data words</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_DCACHE_REQ_READ_4</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of constant cache read requests in four 32-bit data words</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_DCACHE_REQ_READ_8</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of constant cache read requests in eight 32-bit data words</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_DCACHE_REQ_READ_16</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of constant cache read requests in 16 32-bit data words</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_DCACHE_ATOMIC</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of atomic requests</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_TC_REQ</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of texture cache requests that were issued by instruction and constant caches</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_TC_INST_REQ</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of instruction requests to the L2 cache</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_TC_DATA_READ_REQ</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of data Read requests to the L2 cache</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_TC_DATA_WRITE_REQ</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of data write requests to the L2 cache</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_TC_DATA_ATOMIC_REQ</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of data atomic requests to the L2 cache</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SQC_TC_STALL</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles while the valid requests to the L2 cache are stalled</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="vector-l1-cache-subsystem-counters">
<span id="id4"></span><h6>Vector L1 cache subsystem counters<a class="headerlink" href="#vector-l1-cache-subsystem-counters" title="Link to this heading">#</a></h6>
<p>The vector L1 cache subsystem counters are further classified into texture addressing unit, texture data
unit, vector L1d or texture cache per pipe, and texture cache arbiter counters.</p>
<section id="texture-addressing-unit-counters">
<h6 aria-level="7">Texture addressing unit counters<a class="headerlink" href="#texture-addressing-unit-counters" title="Link to this heading">#</a></h6>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Unit</p></th>
<th class="head"><p>Definition</p></th>
<th class="head"><p>Value range for <code class="docutils literal notranslate"><span class="pre">n</span></code></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TA_TA_BUSY[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Texture addressing unit busy cycles</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TA_TOTAL_WAVEFRONTS[n]</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of wavefronts processed by texture addressing unit</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TA_BUFFER_WAVEFRONTS[n]</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of buffer wavefronts processed by texture addressing unit</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TA_BUFFER_READ_WAVEFRONTS[n]</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of buffer read wavefronts processed by texture addressing unit</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TA_BUFFER_WRITE_WAVEFRONTS[n]</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of buffer write wavefronts processed by texture addressing unit</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TA_BUFFER_ATOMIC_WAVEFRONTS[n]</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of buffer atomic wavefronts processed by texture addressing unit</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TA_BUFFER_TOTAL_CYCLES[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of buffer cycles (including read and write) issued to texture cache</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TA_BUFFER_COALESCED_READ_CYCLES[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of coalesced buffer read cycles issued to texture cache</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TA_BUFFER_COALESCED_WRITE_CYCLES[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of coalesced buffer write cycles issued to texture cache</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TA_ADDR_STALLED_BY_TC_CYCLES[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles texture addressing unit address path is stalled by texture cache</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TA_DATA_STALLED_BY_TC_CYCLES[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles texture addressing unit data path is stalled by texture cache</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TA_ADDR_STALLED_BY_TD_CYCLES[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles texture addressing unit address path is stalled by texture data unit</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TA_FLAT_WAVEFRONTS[n]</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of flat opcode wavefronts processed by texture addressing unit</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TA_FLAT_READ_WAVEFRONTS[n]</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of flat opcode read wavefronts processed by texture addressing unit</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TA_FLAT_WRITE_WAVEFRONTS[n]</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of flat opcode write wavefronts processed by texture addressing unit</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TA_FLAT_ATOMIC_WAVEFRONTS[n]</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of flat opcode atomic wavefronts processed by texture addressing unit</p></td>
<td><p>0-15</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="texture-data-unit-counters">
<h6 aria-level="7">Texture data unit counters<a class="headerlink" href="#texture-data-unit-counters" title="Link to this heading">#</a></h6>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Unit</p></th>
<th class="head"><p>Definition</p></th>
<th class="head"><p>Value range for <code class="docutils literal notranslate"><span class="pre">n</span></code></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TD_TD_BUSY[n]</span></code></p></td>
<td><p>Cycle</p></td>
<td><p>Texture data unit busy cycles while it is processing or waiting for data</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TD_TC_STALL[n]</span></code></p></td>
<td><p>Cycle</p></td>
<td><p>Number of cycles texture data unit is stalled waiting for texture cache data</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TD_SPI_STALL[n]</span></code></p></td>
<td><p>Cycle</p></td>
<td><p>Number of cycles texture data unit is stalled by shader processor input</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TD_LOAD_WAVEFRONT[n]</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of wavefront instructions (read, write, atomic)</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TD_STORE_WAVEFRONT[n]</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of write wavefront instructions</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TD_ATOMIC_WAVEFRONT[n]</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of atomic wavefront instructions</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TD_COALESCABLE_WAVEFRONT[n]</span></code></p></td>
<td><p>Instr</p></td>
<td><p>Number of coalescable wavefronts according to texture addressing unit</p></td>
<td><p>0-15</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="texture-cache-per-pipe-counters">
<h6 aria-level="7">Texture cache per pipe counters<a class="headerlink" href="#texture-cache-per-pipe-counters" title="Link to this heading">#</a></h6>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Unit</p></th>
<th class="head"><p>Definition</p></th>
<th class="head"><p>Value range for <code class="docutils literal notranslate"><span class="pre">n</span></code></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_GATE_EN1[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles vector L1d interface clocks are turned on</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_GATE_EN2[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles vector L1d core clocks are turned on</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TD_TCP_STALL_CYCLES[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles texture data unit stalls vector L1d</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCR_TCP_STALL_CYCLES[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles texture cache router stalls vector L1d</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_READ_TAGCONFLICT_STALL_CYCLES[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles tag RAM conflict stalls on a read</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_WRITE_TAGCONFLICT_STALL_CYCLES[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles tag RAM conflict stalls on a write</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_ATOMIC_TAGCONFLICT_STALL_CYCLES[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles tag RAM conflict stalls on an atomic</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_PENDING_STALL_CYCLES[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles vector L1d is stalled due to data pending from L2 Cache</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCP_TA_DATA_STALL_CYCLES</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles texture cache per pipe stalls texture addressing unit data interface</p></td>
<td><p>NA</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TA_TCP_STATE_READ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of state reads</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_VOLATILE[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of L1 volatile pixels or buffers from texture addressing unit</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TOTAL_ACCESSES[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of vector L1d accesses. Equals <code class="docutils literal notranslate"><span class="pre">TCP_PERF_SEL_TOTAL_READ`+`TCP_PERF_SEL_TOTAL_NONREAD</span></code></p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TOTAL_READ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of vector L1d read accesses</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TOTAL_WRITE[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of vector L1d write accesses</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TOTAL_ATOMIC_WITH_RET[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of vector L1d atomic requests with return</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TOTAL_ATOMIC_WITHOUT_RET[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of vector L1d atomic without return</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TOTAL_WRITEBACK_INVALIDATES[n]</span></code></p></td>
<td><p>Count</p></td>
<td><p>Total number of vector L1d writebacks and invalidates</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_UTCL1_REQUEST[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of address translation requests to unified translation cache (L1)</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_UTCL1_TRANSLATION_HIT[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of unified translation cache (L1) translation hits</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_UTCL1_TRANSLATION_MISS[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of unified translation cache (L1) translation misses</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_UTCL1_PERMISSION_MISS[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of unified translation cache (L1) permission misses</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TOTAL_CACHE_ACCESSES[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of vector L1d cache accesses including hits and misses</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCP_LATENCY[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p><strong>MI200 series only</strong> Accumulated wave access latency to vL1D over all wavefronts</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_READ_REQ_LATENCY[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p><strong>MI200 series only</strong> Total vL1D to L2 request latency over all wavefronts for reads and atomics with return</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_WRITE_REQ_LATENCY[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p><strong>MI200 series only</strong> Total vL1D to L2 request latency over all wavefronts for writes and atomics without return</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_READ_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of read requests to L2 cache</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_WRITE_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of write requests to L2 cache</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_ATOMIC_WITH_RET_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of atomic requests to L2 cache with return</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_ATOMIC_WITHOUT_RET_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of atomic requests to L2 cache without return</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_NC_READ_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of non-coherently cached read requests to L2 cache</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_UC_READ_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of uncached read requests to L2 cache</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_CC_READ_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of coherently cached read requests to L2 cache</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_RW_READ_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of coherently cached with write read requests to L2 cache</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_NC_WRITE_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of non-coherently cached write requests to L2 cache</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_UC_WRITE_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of uncached write requests to L2 cache</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_CC_WRITE_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of coherently cached write requests to L2 cache</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_RW_WRITE_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of coherently cached with write write requests to L2 cache</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_NC_ATOMIC_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of non-coherently cached atomic requests to L2 cache</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_UC_ATOMIC_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of uncached atomic requests to L2 cache</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_CC_ATOMIC_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of coherently cached atomic requests to L2 cache</p></td>
<td><p>0-15</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_RW_ATOMIC_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of coherently cached with write atomic requests to L2 cache</p></td>
<td><p>0-15</p></td>
</tr>
</tbody>
</table>
</div>
<p>Note that:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">TCP_TOTAL_READ[n]</span></code> = <code class="docutils literal notranslate"><span class="pre">TCP_PERF_SEL_TOTAL_HIT_LRU_READ</span></code> + <code class="docutils literal notranslate"><span class="pre">TCP_PERF_SEL_TOTAL_MISS_LRU_READ</span></code> + <code class="docutils literal notranslate"><span class="pre">TCP_PERF_SEL_TOTAL_MISS_EVICT_READ</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TCP_TOTAL_WRITE[n]</span></code> = <code class="docutils literal notranslate"><span class="pre">TCP_PERF_SEL_TOTAL_MISS_LRU_WRITE``+</span> <span class="pre">``TCP_PERF_SEL_TOTAL_MISS_EVICT_WRITE</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TCP_TOTAL_WRITEBACK_INVALIDATES[n]</span></code> = <code class="docutils literal notranslate"><span class="pre">TCP_PERF_SEL_TOTAL_WBINVL1``+</span> <span class="pre">``TCP_PERF_SEL_TOTAL_WBINVL1_VOL``+</span> <span class="pre">``TCP_PERF_SEL_CP_TCP_INVALIDATE``+</span> <span class="pre">``TCP_PERF_SEL_SQ_TCP_INVALIDATE_VOL</span></code></p></li>
</ul>
</section>
<section id="texture-cache-arbiter-counters">
<h6 aria-level="7">Texture cache arbiter counters<a class="headerlink" href="#texture-cache-arbiter-counters" title="Link to this heading">#</a></h6>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Unit</p></th>
<th class="head"><p>Definition</p></th>
<th class="head"><p>Value range for <code class="docutils literal notranslate"><span class="pre">n</span></code></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCA_CYCLE[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of texture cache arbiter cycles</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCA_BUSY[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles texture cache arbiter has a pending request</p></td>
<td><p>0-31</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="l2-cache-access-counters">
<span id="id5"></span><h6>L2 cache access counters<a class="headerlink" href="#l2-cache-access-counters" title="Link to this heading">#</a></h6>
<p>L2 cache is also known as texture cache per channel.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-74" name="sd-tab-set-37" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-74">
MI300 hardware counter</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Unit</p></th>
<th class="head"><p>Definition</p></th>
<th class="head"><p>Value range for <code class="docutils literal notranslate"><span class="pre">n</span></code></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_CYCLE[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of L2 cache free-running clocks</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_BUSY[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of L2 cache busy cycles</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of L2 cache requests of all types (measured at the tag block)</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_STREAMING_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of L2 cache streaming requests (measured at the tag block)</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_NC_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of non-coherently cached requests (measured at the tag block)</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_UC_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of uncached requests. This is measured at the tag block</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_CC_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of coherently cached requests. This is measured at the tag block</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_RW_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of coherently cached with write requests. This is measured at the tag block</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_PROBE[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of probe requests</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_PROBE_ALL[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of external probe requests with <code class="docutils literal notranslate"><span class="pre">EA_TCC_preq_all</span> <span class="pre">==</span> <span class="pre">1</span></code></p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_READ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of L2 cache read requests (includes compressed reads but not metadata reads)</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_WRITE[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of L2 cache write requests</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_ATOMIC[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of L2 cache atomic requests of all types</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_HIT[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of L2 cache hits</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_MISS[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of L2 cache misses</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_WRITEBACK[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of lines written back to the main memory, including writebacks of dirty lines and uncached write or atomic requests</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_WRREQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of 32-byte and 64-byte transactions going over the <code class="docutils literal notranslate"><span class="pre">TC_EA_wrreq</span></code> interface (doesn’t include probe commands)</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_WRREQ_64B[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Total number of 64-byte transactions (write or <code class="docutils literal notranslate"><span class="pre">CMPSWAP</span></code>) going over the <code class="docutils literal notranslate"><span class="pre">TC_EA_wrreq</span></code> interface</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_WR_UNCACHED_32B[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of 32 or 64-byte write or atomic going over the <code class="docutils literal notranslate"><span class="pre">TC_EA_wrreq</span></code> interface due to uncached traffic</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_WRREQ_STALL[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles a write request is stalled</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_WRREQ_IO_CREDIT_STALL[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles an efficiency arbiter write request is stalled due to the interface running out of input-output (IO) credits</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_WRREQ_GMI_CREDIT_STALL[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles an efficiency arbiter write request is stalled due to the interface running out of GMI credits</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_WRREQ_DRAM_CREDIT_STALL[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles an efficiency arbiter write request is stalled due to the interface running out of DRAM credits</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_TOO_MANY_EA_WRREQS_STALL[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles the L2 cache is unable to send an efficiency arbiter write request due to it reaching its maximum capacity of pending efficiency arbiter write requests</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_WRREQ_LEVEL[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>The accumulated number of efficiency arbiter write requests in flight</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_ATOMIC[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of 32-byte or 64-byte atomic requests going over the <code class="docutils literal notranslate"><span class="pre">TC_EA_wrreq</span></code> interface</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_ATOMIC_LEVEL[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>The accumulated number of efficiency arbiter atomic requests in flight</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_RDREQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of 32-byte or 64-byte read requests to efficiency arbiter</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_RDREQ_32B[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of 32-byte read requests to efficiency arbiter</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_RD_UNCACHED_32B[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of 32-byte efficiency arbiter reads due to uncached traffic. A 64-byte request is counted as 2</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_RDREQ_IO_CREDIT_STALL[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles there is a stall due to the read request interface running out of IO credits</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_RDREQ_GMI_CREDIT_STALL[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles there is a stall due to the read request interface running out of GMI credits</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_RDREQ_DRAM_CREDIT_STALL[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles there is a stall due to the read request interface running out of DRAM credits</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_RDREQ_LEVEL[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>The accumulated number of efficiency arbiter read requests in flight</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_RDREQ_DRAM[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of 32-byte or 64-byte efficiency arbiter read requests to High Bandwidth Memory (HBM)</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_WRREQ_DRAM[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of 32-byte or 64-byte efficiency arbiter write requests to HBM</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_TAG_STALL[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles the normal request pipeline in the tag is stalled for any reason</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_NORMAL_WRITEBACK[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of writebacks due to requests that are not writeback requests</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_ALL_TC_OP_WB_WRITEBACK[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of writebacks due to all <code class="docutils literal notranslate"><span class="pre">TC_OP</span></code> writeback requests</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_NORMAL_EVICT[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of evictions due to requests that are not invalidate or probe requests</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_ALL_TC_OP_INV_EVICT[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of evictions due to all <code class="docutils literal notranslate"><span class="pre">TC_OP</span></code> invalidate requests</p></td>
<td><p>0-31</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-75" name="sd-tab-set-37" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-75">
MI200 hardware counter</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Unit</p></th>
<th class="head"><p>Definition</p></th>
<th class="head"><p>Value range for <code class="docutils literal notranslate"><span class="pre">n</span></code></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_CYCLE[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of L2 cache free-running clocks</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_BUSY[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of L2 cache busy cycles</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of L2 cache requests of all types (measured at the tag block)</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_STREAMING_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of L2 cache streaming requests (measured at the tag block)</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_NC_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of non-coherently cached requests (measured at the tag block)</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_UC_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of uncached requests. This is measured at the tag block</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_CC_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of coherently cached requests. This is measured at the tag block</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_RW_REQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of coherently cached with write requests. This is measured at the tag block</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_PROBE[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of probe requests</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_PROBE_ALL[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of external probe requests with <code class="docutils literal notranslate"><span class="pre">EA_TCC_preq_all</span> <span class="pre">==</span> <span class="pre">1</span></code></p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_READ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of L2 cache read requests (includes compressed reads but not metadata reads)</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_WRITE[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of L2 cache write requests</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_ATOMIC[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of L2 cache atomic requests of all types</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_HIT[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of L2 cache hits</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_MISS[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of L2 cache misses</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_WRITEBACK[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of lines written back to the main memory, including writebacks of dirty lines and uncached write or atomic requests</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA_WRREQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of 32-byte and 64-byte transactions going over the <code class="docutils literal notranslate"><span class="pre">TC_EA_wrreq</span></code> interface (doesn’t include probe commands)</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA_WRREQ_64B[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Total number of 64-byte transactions (write or <code class="docutils literal notranslate"><span class="pre">CMPSWAP</span></code>) going over the <code class="docutils literal notranslate"><span class="pre">TC_EA_wrreq</span></code> interface</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA_WR_UNCACHED_32B[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of 32 write or atomic going over the <code class="docutils literal notranslate"><span class="pre">TC_EA_wrreq</span></code> interface due to uncached traffic. A 64-byte request will be counted as 2</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA_WRREQ_STALL[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles a write request is stalled</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA_WRREQ_IO_CREDIT_STALL[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles an efficiency arbiter write request is stalled due to the interface running out of input-output (IO) credits</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA_WRREQ_GMI_CREDIT_STALL[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles an efficiency arbiter write request is stalled due to the interface running out of GMI credits</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA_WRREQ_DRAM_CREDIT_STALL[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles an efficiency arbiter write request is stalled due to the interface running out of DRAM credits</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_TOO_MANY_EA_WRREQS_STALL[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles the L2 cache is unable to send an efficiency arbiter write request due to it reaching its maximum capacity of pending efficiency arbiter write requests</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA_WRREQ_LEVEL[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>The accumulated number of efficiency arbiter write requests in flight</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA_ATOMIC[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of 32-byte or 64-byte atomic requests going over the <code class="docutils literal notranslate"><span class="pre">TC_EA_wrreq</span></code> interface</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA_ATOMIC_LEVEL[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>The accumulated number of efficiency arbiter atomic requests in flight</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA_RDREQ[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of 32-byte or 64-byte read requests to efficiency arbiter</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA_RDREQ_32B[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of 32-byte read requests to efficiency arbiter</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA_RD_UNCACHED_32B[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of 32-byte efficiency arbiter reads due to uncached traffic. A 64-byte request is counted as 2</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA_RDREQ_IO_CREDIT_STALL[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles there is a stall due to the read request interface running out of IO credits</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA_RDREQ_GMI_CREDIT_STALL[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles there is a stall due to the read request interface running out of GMI credits</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA_RDREQ_DRAM_CREDIT_STALL[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles there is a stall due to the read request interface running out of DRAM credits</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA_RDREQ_LEVEL[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>The accumulated number of efficiency arbiter read requests in flight</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA_RDREQ_DRAM[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of 32-byte or 64-byte efficiency arbiter read requests to High Bandwidth Memory (HBM)</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA_WRREQ_DRAM[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of 32-byte or 64-byte efficiency arbiter write requests to HBM</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_TAG_STALL[n]</span></code></p></td>
<td><p>Cycles</p></td>
<td><p>Number of cycles the normal request pipeline in the tag is stalled for any reason</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_NORMAL_WRITEBACK[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of writebacks due to requests that are not writeback requests</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_ALL_TC_OP_WB_WRITEBACK[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of writebacks due to all <code class="docutils literal notranslate"><span class="pre">TC_OP</span></code> writeback requests</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_NORMAL_EVICT[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of evictions due to requests that are not invalidate or probe requests</p></td>
<td><p>0-31</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_ALL_TC_OP_INV_EVICT[n]</span></code></p></td>
<td><p>Req</p></td>
<td><p>Number of evictions due to all <code class="docutils literal notranslate"><span class="pre">TC_OP</span></code> invalidate requests</p></td>
<td><p>0-31</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Note the following:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">TCC_REQ[n]</span></code> may be more than the number of requests arriving at the texture cache per channel,
but it’s a good indication of the total amount of work that needs to be performed.</p></li>
<li><p>For <code class="docutils literal notranslate"><span class="pre">TCC_EA0_WRREQ[n]</span></code>, atomics may travel over the same interface and are generally classified as
write requests.</p></li>
<li><p>CC mtypes can produce uncached requests, and those are included in
<code class="docutils literal notranslate"><span class="pre">TCC_EA0_WR_UNCACHED_32B[n]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_WRREQ_LEVEL[n]</span></code> is primarily intended to measure average efficiency arbiter write latency.</p>
<ul>
<li><p>Average write latency = <code class="docutils literal notranslate"><span class="pre">TCC_PERF_SEL_EA0_WRREQ_LEVEL</span></code> divided by <code class="docutils literal notranslate"><span class="pre">TCC_PERF_SEL_EA0_WRREQ</span></code></p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_ATOMIC_LEVEL[n]</span></code> is primarily intended to measure average efficiency arbiter atomic
latency</p>
<ul>
<li><p>Average atomic latency = <code class="docutils literal notranslate"><span class="pre">TCC_PERF_SEL_EA0_WRREQ_ATOMIC_LEVEL</span></code> divided by <code class="docutils literal notranslate"><span class="pre">TCC_PERF_SEL_EA0_WRREQ_ATOMIC</span></code></p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_RDREQ_LEVEL[n]</span></code> is primarily intended to measure average efficiency arbiter read latency.</p>
<ul>
<li><p>Average read latency = <code class="docutils literal notranslate"><span class="pre">TCC_PERF_SEL_EA0_RDREQ_LEVEL</span></code> divided by <code class="docutils literal notranslate"><span class="pre">TCC_PERF_SEL_EA0_RDREQ</span></code></p></li>
</ul>
</li>
<li><p>Stalls can occur regardless of the need for a read to be performed</p></li>
<li><p>Normally, stalls are measured exactly at one point in the pipeline however in the case of
<code class="docutils literal notranslate"><span class="pre">TCC_TAG_STALL[n]</span></code>, probes can stall the pipeline at a variety of places. There is no single point that
can accurately measure the total stalls</p></li>
</ul>
</section>
</section>
<section id="mi300-and-mi200-series-derived-metrics-list">
<h5>MI300 and MI200 series derived metrics list<a class="headerlink" href="#mi300-and-mi200-series-derived-metrics-list" title="Link to this heading">#</a></h5>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ALUStalledByLDS</span></code></p></td>
<td><p>Percentage of GPU time ALU units are stalled due to the LDS input queue being full or the output queue not being ready (value range: 0% (optimal) to 100%)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">FetchSize</span></code></p></td>
<td><p>Total kilobytes fetched from the video memory; measured with all extra fetches and any cache or memory effects taken into account</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">FlatLDSInsts</span></code></p></td>
<td><p>Average number of flat instructions that read from or write to LDS, run per work item (affected by flow control)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">FlatVMemInsts</span></code></p></td>
<td><p>Average number of flat instructions that read from or write to the video memory, run per work item (affected by flow control). Includes flat instructions that read from or write to scratch</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">GDSInsts</span></code></p></td>
<td><p>Average number of global data share read or write instructions run per work item (affected by flow control)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">GPUBusy</span></code></p></td>
<td><p>Percentage of time GPU is busy</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">L2CacheHit</span></code></p></td>
<td><p>Percentage of fetch, write, atomic, and other instructions that hit the data in L2 cache (value range: 0% (no hit) to 100% (optimal))</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">LDSBankConflict</span></code></p></td>
<td><p>Percentage of GPU time LDS is stalled by bank conflicts (value range: 0% (optimal) to 100%)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">LDSInsts</span></code></p></td>
<td><p>Average number of LDS read or write instructions run per work item (affected by flow control). Excludes flat instructions that read from or write to LDS.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">MemUnitBusy</span></code></p></td>
<td><p>Percentage of GPU time the memory unit is active, which is measured with all extra fetches and writes and any cache or memory effects taken into account (value range: 0% to 100% (fetch-bound))</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">MemUnitStalled</span></code></p></td>
<td><p>Percentage of GPU time the memory unit is stalled (value range: 0% (optimal) to 100%)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">MemWrites32B</span></code></p></td>
<td><p>Total number of effective 32B write transactions to the memory</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCA_BUSY_sum</span></code></p></td>
<td><p>Total number of cycles texture cache arbiter has a pending request, over all texture cache arbiter instances</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCA_CYCLE_sum</span></code></p></td>
<td><p>Total number of cycles over all texture cache arbiter instances</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SALUBusy</span></code></p></td>
<td><p>Percentage of GPU time scalar ALU instructions are processed (value range: 0% to 100% (optimal))</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SALUInsts</span></code></p></td>
<td><p>Average number of scalar ALU instructions run per work item (affected by flow control)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SFetchInsts</span></code></p></td>
<td><p>Average number of scalar fetch instructions from the video memory run per work item (affected by flow control)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">VALUBusy</span></code></p></td>
<td><p>Percentage of GPU time vector ALU instructions are processed (value range: 0% to 100% (optimal))</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">VALUInsts</span></code></p></td>
<td><p>Average number of vector ALU instructions run per work item (affected by flow control)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">VALUUtilization</span></code></p></td>
<td><p>Percentage of active vector ALU threads in a wave, where a lower number can mean either more thread divergence in a wave or that the work-group size is not a multiple of 64 (value range: 0%, 100% (optimal - no thread divergence))</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">VFetchInsts</span></code></p></td>
<td><p>Average number of vector fetch instructions from the video memory run per work-item (affected by flow control); excludes flat instructions that fetch from video memory</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">VWriteInsts</span></code></p></td>
<td><p>Average number of vector write instructions to the video memory run per work-item (affected by flow control); excludes flat instructions that write to video memory</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">Wavefronts</span></code></p></td>
<td><p>Total wavefronts</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">WRITE_REQ_32B</span></code></p></td>
<td><p>Total number of 32-byte effective memory writes</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">WriteSize</span></code></p></td>
<td><p>Total kilobytes written to the video memory; measured with all extra fetches and any cache or memory effects taken into account</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">WriteUnitStalled</span></code></p></td>
<td><p>Percentage of GPU time the write unit is stalled (value range: 0% (optimal) to 100%)</p></td>
</tr>
</tbody>
</table>
</div>
<p>You can lower <code class="docutils literal notranslate"><span class="pre">ALUStalledByLDS</span></code> by reducing LDS bank conflicts or number of LDS accesses.
You can lower <code class="docutils literal notranslate"><span class="pre">MemUnitStalled</span></code> by reducing the number or size of fetches and writes.
<code class="docutils literal notranslate"><span class="pre">MemUnitBusy</span></code> includes the stall time (<code class="docutils literal notranslate"><span class="pre">MemUnitStalled</span></code>).</p>
<section id="hardware-counters-by-and-over-all-texture-addressing-unit-instances">
<h6>Hardware counters by and over all texture addressing unit instances<a class="headerlink" href="#hardware-counters-by-and-over-all-texture-addressing-unit-instances" title="Link to this heading">#</a></h6>
<p>The following table shows the hardware counters <em>by</em> all texture addressing unit instances.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TA_BUFFER_WAVEFRONTS_sum</span></code></p></td>
<td><p>Total number of buffer wavefronts processed</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TA_BUFFER_READ_WAVEFRONTS_sum</span></code></p></td>
<td><p>Total number of buffer read wavefronts processed</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TA_BUFFER_WRITE_WAVEFRONTS_sum</span></code></p></td>
<td><p>Total number of buffer write wavefronts processed</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TA_BUFFER_ATOMIC_WAVEFRONTS_sum</span></code></p></td>
<td><p>Total number of buffer atomic wavefronts processed</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TA_BUFFER_TOTAL_CYCLES_sum</span></code></p></td>
<td><p>Total number of buffer cycles (including read and write) issued to texture cache</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TA_BUFFER_COALESCED_READ_CYCLES_sum</span></code></p></td>
<td><p>Total number of coalesced buffer read cycles issued to texture cache</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TA_BUFFER_COALESCED_WRITE_CYCLES_sum</span></code></p></td>
<td><p>Total number of coalesced buffer write cycles issued to texture cache</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TA_FLAT_READ_WAVEFRONTS_sum</span></code></p></td>
<td><p>Sum of flat opcode reads processed</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TA_FLAT_WRITE_WAVEFRONTS_sum</span></code></p></td>
<td><p>Sum of flat opcode writes processed</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TA_FLAT_WAVEFRONTS_sum</span></code></p></td>
<td><p>Total number of flat opcode wavefronts processed</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TA_FLAT_ATOMIC_WAVEFRONTS_sum</span></code></p></td>
<td><p>Total number of flat opcode atomic wavefronts processed</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TA_TOTAL_WAVEFRONTS_sum</span></code></p></td>
<td><p>Total number of wavefronts processed</p></td>
</tr>
</tbody>
</table>
</div>
<p>The following table shows the hardware counters <em>over</em> all texture addressing unit instances.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TA_ADDR_STALLED_BY_TC_CYCLES_sum</span></code></p></td>
<td><p>Total number of cycles texture addressing unit address path is stalled by texture cache</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TA_ADDR_STALLED_BY_TD_CYCLES_sum</span></code></p></td>
<td><p>Total number of cycles texture addressing unit address path is stalled by texture data unit</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TA_BUSY_avr</span></code></p></td>
<td><p>Average number of busy cycles</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TA_BUSY_max</span></code></p></td>
<td><p>Maximum number of texture addressing unit busy cycles</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TA_BUSY_min</span></code></p></td>
<td><p>Minimum number of texture addressing unit busy cycles</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TA_DATA_STALLED_BY_TC_CYCLES_sum</span></code></p></td>
<td><p>Total number of cycles texture addressing unit data path is stalled by texture cache</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TA_TA_BUSY_sum</span></code></p></td>
<td><p>Total number of texture addressing unit busy cycles</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="hardware-counters-over-all-texture-cache-per-channel-instances">
<h6>Hardware counters over all texture cache per channel instances<a class="headerlink" href="#hardware-counters-over-all-texture-cache-per-channel-instances" title="Link to this heading">#</a></h6>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_ALL_TC_OP_WB_WRITEBACK_sum</span></code></p></td>
<td><p>Total number of writebacks due to all <code class="docutils literal notranslate"><span class="pre">TC_OP</span></code> writeback requests.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_ALL_TC_OP_INV_EVICT_sum</span></code></p></td>
<td><p>Total number of evictions due to all <code class="docutils literal notranslate"><span class="pre">TC_OP</span></code> invalidate requests.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_ATOMIC_sum</span></code></p></td>
<td><p>Total number of L2 cache atomic requests of all types.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_BUSY_avr</span></code></p></td>
<td><p>Average number of L2 cache busy cycles.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_BUSY_sum</span></code></p></td>
<td><p>Total number of L2 cache busy cycles.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_CC_REQ_sum</span></code></p></td>
<td><p>Total number of coherently cached requests.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_CYCLE_sum</span></code></p></td>
<td><p>Total number of L2 cache free running clocks.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_WRREQ_sum</span></code></p></td>
<td><p>Total number of 32-byte and 64-byte transactions going over the <code class="docutils literal notranslate"><span class="pre">TC_EA0_wrreq</span></code> interface. Atomics may travel over the same interface and are generally classified as write requests. This does not include probe commands.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_WRREQ_64B_sum</span></code></p></td>
<td><p>Total number of 64-byte transactions (write or <cite>CMPSWAP</cite>) going over the <code class="docutils literal notranslate"><span class="pre">TC_EA0_wrreq</span></code> interface.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_WR_UNCACHED_32B_sum</span></code></p></td>
<td><p>Total Number of 32-byte write or atomic going over the <code class="docutils literal notranslate"><span class="pre">TC_EA0_wrreq</span></code> interface due to uncached traffic. Note that coherently cached mtypes can produce uncached requests, and those are included in this. A 64-byte request is counted as 2.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_WRREQ_STALL_sum</span></code></p></td>
<td><p>Total Number of cycles a write request is stalled, over all instances.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_WRREQ_IO_CREDIT_STALL_sum</span></code></p></td>
<td><p>Total number of cycles an efficiency arbiter write request is stalled due to the interface running out of IO credits, over all instances.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_WRREQ_GMI_CREDIT_STALL_sum</span></code></p></td>
<td><p>Total number of cycles an efficiency arbiter write request is stalled due to the interface running out of GMI credits, over all instances.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_WRREQ_DRAM_CREDIT_STALL_sum</span></code></p></td>
<td><p>Total number of cycles an efficiency arbiter write request is stalled due to the interface running out of DRAM credits, over all instances.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_WRREQ_LEVEL_sum</span></code></p></td>
<td><p>Total number of efficiency arbiter write requests in flight.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_RDREQ_LEVEL_sum</span></code></p></td>
<td><p>Total number of efficiency arbiter read requests in flight.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_ATOMIC_sum</span></code></p></td>
<td><p>Total Number of 32-byte or 64-byte atomic requests going over the <code class="docutils literal notranslate"><span class="pre">TC_EA0_wrreq</span></code> interface.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_ATOMIC_LEVEL_sum</span></code></p></td>
<td><p>Total number of efficiency arbiter atomic requests in flight.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_RDREQ_sum</span></code></p></td>
<td><p>Total number of 32-byte or 64-byte read requests to efficiency arbiter.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_RDREQ_32B_sum</span></code></p></td>
<td><p>Total number of 32-byte read requests to efficiency arbiter.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_RD_UNCACHED_32B_sum</span></code></p></td>
<td><p>Total number of 32-byte efficiency arbiter reads due to uncached traffic.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_RDREQ_IO_CREDIT_STALL_sum</span></code></p></td>
<td><p>Total number of cycles there is a stall due to the read request interface running out of IO credits.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_RDREQ_GMI_CREDIT_STALL_sum</span></code></p></td>
<td><p>Total number of cycles there is a stall due to the read request interface running out of GMI credits.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_RDREQ_DRAM_CREDIT_STALL_sum</span></code></p></td>
<td><p>Total number of cycles there is a stall due to the read request interface running out of DRAM credits.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_RDREQ_DRAM_sum</span></code></p></td>
<td><p>Total number of 32-byte or 64-byte efficiency arbiter read requests to HBM.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_EA0_WRREQ_DRAM_sum</span></code></p></td>
<td><p>Total number of 32-byte or 64-byte efficiency arbiter write requests to HBM.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_HIT_sum</span></code></p></td>
<td><p>Total number of L2 cache hits.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_MISS_sum</span></code></p></td>
<td><p>Total number of L2 cache misses.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_NC_REQ_sum</span></code></p></td>
<td><p>Total number of non-coherently cached requests.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_NORMAL_WRITEBACK_sum</span></code></p></td>
<td><p>Total number of writebacks due to requests that are not writeback requests.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_NORMAL_EVICT_sum</span></code></p></td>
<td><p>Total number of evictions due to requests that are not invalidate or probe requests.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_PROBE_sum</span></code></p></td>
<td><p>Total number of probe requests.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_PROBE_ALL_sum</span></code></p></td>
<td><p>Total number of external probe requests with <code class="docutils literal notranslate"><span class="pre">EA0_TCC_preq_all</span> <span class="pre">==</span> <span class="pre">1</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_READ_sum</span></code></p></td>
<td><p>Total number of L2 cache read requests (including compressed reads but not metadata reads).</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_REQ_sum</span></code></p></td>
<td><p>Total number of all types of L2 cache requests.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_RW_REQ_sum</span></code></p></td>
<td><p>Total number of coherently cached with write requests.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_STREAMING_REQ_sum</span></code></p></td>
<td><p>Total number of L2 cache streaming requests.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_TAG_STALL_sum</span></code></p></td>
<td><p>Total number of cycles the normal request pipeline in the tag is stalled for any reason.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_TOO_MANY_EA0_WRREQS_STALL_sum</span></code></p></td>
<td><p>Total number of cycles L2 cache is unable to send an efficiency arbiter write request due to it reaching its maximum capacity of pending efficiency arbiter write requests.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_UC_REQ_sum</span></code></p></td>
<td><p>Total number of uncached requests.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_WRITE_sum</span></code></p></td>
<td><p>Total number of L2 cache write requests.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_WRITEBACK_sum</span></code></p></td>
<td><p>Total number of lines written back to the main memory including writebacks of dirty lines and uncached write or atomic requests.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCC_WRREQ_STALL_max</span></code></p></td>
<td><p>Maximum number of cycles a write request is stalled.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="hardware-counters-by-for-or-over-all-texture-cache-per-pipe-instances">
<h6>Hardware counters by, for, or over all texture cache per pipe instances<a class="headerlink" href="#hardware-counters-by-for-or-over-all-texture-cache-per-pipe-instances" title="Link to this heading">#</a></h6>
<p>The following table shows the hardware counters <em>by</em> all texture cache per pipe instances.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TA_TCP_STATE_READ_sum</span></code></p></td>
<td><p>Total number of state reads by ATCPPI</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TOTAL_CACHE_ACCESSES_sum</span></code></p></td>
<td><p>Total number of vector L1d accesses (including hits and misses)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_UTCL1_PERMISSION_MISS_sum</span></code></p></td>
<td><p>Total number of unified translation cache (L1) permission misses</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_UTCL1_REQUEST_sum</span></code></p></td>
<td><p>Total number of address translation requests to unified translation cache (L1)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_UTCL1_TRANSLATION_MISS_sum</span></code></p></td>
<td><p>Total number of unified translation cache (L1) translation misses</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_UTCL1_TRANSLATION_HIT_sum</span></code></p></td>
<td><p>Total number of unified translation cache (L1) translation hits</p></td>
</tr>
</tbody>
</table>
</div>
<p>The following table shows the hardware counters <em>for</em> all texture cache per pipe instances.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_READ_REQ_LATENCY_sum</span></code></p></td>
<td><p>Total vector L1d to L2 request latency over all wavefronts for reads and atomics with return</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_WRITE_REQ_LATENCY_sum</span></code></p></td>
<td><p>Total vector L1d to L2 request latency over all wavefronts for writes and atomics without return</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCP_LATENCY_sum</span></code></p></td>
<td><p>Total wave access latency to vector L1d over all wavefronts</p></td>
</tr>
</tbody>
</table>
</div>
<p>The following table shows the hardware counters <em>over</em> all texture cache per pipe instances.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_ATOMIC_TAGCONFLICT_STALL_CYCLES_sum</span></code></p></td>
<td><p>Total number of cycles tag RAM conflict stalls on an atomic</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_GATE_EN1_sum</span></code></p></td>
<td><p>Total number of cycles vector L1d interface clocks are turned on</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_GATE_EN2_sum</span></code></p></td>
<td><p>Total number of cycles vector L1d core clocks are turned on</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_PENDING_STALL_CYCLES_sum</span></code></p></td>
<td><p>Total number of cycles vector L1d cache is stalled due to data pending from L2 Cache</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_READ_TAGCONFLICT_STALL_CYCLES_sum</span></code></p></td>
<td><p>Total number of cycles tag RAM conflict stalls on a read</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_ATOMIC_WITH_RET_REQ_sum</span></code></p></td>
<td><p>Total number of atomic requests to L2 cache with return</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_ATOMIC_WITHOUT_RET_REQ_sum</span></code></p></td>
<td><p>Total number of atomic requests to L2 cache without return</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_CC_READ_REQ_sum</span></code></p></td>
<td><p>Total number of coherently cached read requests to L2 cache</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_CC_WRITE_REQ_sum</span></code></p></td>
<td><p>Total number of coherently cached write requests to L2 cache</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_CC_ATOMIC_REQ_sum</span></code></p></td>
<td><p>Total number of coherently cached atomic requests to L2 cache</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_NC_READ_REQ_sum</span></code></p></td>
<td><p>Total number of non-coherently cached read requests to L2 cache</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_NC_WRITE_REQ_sum</span></code></p></td>
<td><p>Total number of non-coherently cached write requests to L2 cache</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_NC_ATOMIC_REQ_sum</span></code></p></td>
<td><p>Total number of non-coherently cached atomic requests to L2 cache</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_READ_REQ_sum</span></code></p></td>
<td><p>Total number of read requests to L2 cache</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_RW_READ_REQ_sum</span></code></p></td>
<td><p>Total number of coherently cached with write read requests to L2 cache</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_RW_WRITE_REQ_sum</span></code></p></td>
<td><p>Total number of coherently cached with write write requests to L2 cache</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_RW_ATOMIC_REQ_sum</span></code></p></td>
<td><p>Total number of coherently cached with write atomic requests to L2 cache</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_UC_READ_REQ_sum</span></code></p></td>
<td><p>Total number of uncached read requests to L2 cache</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_UC_WRITE_REQ_sum</span></code></p></td>
<td><p>Total number of uncached write requests to L2 cache</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_UC_ATOMIC_REQ_sum</span></code></p></td>
<td><p>Total number of uncached atomic requests to L2 cache</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCC_WRITE_REQ_sum</span></code></p></td>
<td><p>Total number of write requests to L2 cache</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TCR_TCP_STALL_CYCLES_sum</span></code></p></td>
<td><p>Total number of cycles texture cache router stalls vector L1d</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TD_TCP_STALL_CYCLES_sum</span></code></p></td>
<td><p>Total number of cycles texture data unit stalls vector L1d</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TOTAL_ACCESSES_sum</span></code></p></td>
<td><p>Total number of vector L1d accesses</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TOTAL_READ_sum</span></code></p></td>
<td><p>Total number of vector L1d read accesses</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TOTAL_WRITE_sum</span></code></p></td>
<td><p>Total number of vector L1d write accesses</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TOTAL_ATOMIC_WITH_RET_sum</span></code></p></td>
<td><p>Total number of vector L1d atomic requests with return</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TOTAL_ATOMIC_WITHOUT_RET_sum</span></code></p></td>
<td><p>Total number of vector L1d atomic requests without return</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_TOTAL_WRITEBACK_INVALIDATES_sum</span></code></p></td>
<td><p>Total number of vector L1d writebacks and invalidates</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_VOLATILE_sum</span></code></p></td>
<td><p>Total number of L1 volatile pixels or buffers from texture addressing unit</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TCP_WRITE_TAGCONFLICT_STALL_CYCLES_sum</span></code></p></td>
<td><p>Total number of cycles tag RAM conflict stalls on a write</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="hardware-counter-over-all-texture-data-unit-instances">
<h6>Hardware counter over all texture data unit instances<a class="headerlink" href="#hardware-counter-over-all-texture-data-unit-instances" title="Link to this heading">#</a></h6>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware counter</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TD_ATOMIC_WAVEFRONT_sum</span></code></p></td>
<td><p>Total number of atomic wavefront instructions</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TD_COALESCABLE_WAVEFRONT_sum</span></code></p></td>
<td><p>Total number of coalescable wavefronts according to texture addressing unit</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TD_LOAD_WAVEFRONT_sum</span></code></p></td>
<td><p>Total number of wavefront instructions (read, write, atomic)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TD_SPI_STALL_sum</span></code></p></td>
<td><p>Total number of cycles texture data unit is stalled by shader processor input</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TD_STORE_WAVEFRONT_sum</span></code></p></td>
<td><p>Total number of write wavefront instructions</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TD_TC_STALL_sum</span></code></p></td>
<td><p>Total number of cycles texture data unit is stalled waiting for texture cache data</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">TD_TD_BUSY_sum</span></code></p></td>
<td><p>Total number of texture data unit busy cycles while it is processing or waiting for data</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
</section>
</div>
</section>
<span id="document-conceptual/gpu-arch/mi250"></span><section class="tex2jax_ignore mathjax_ignore" id="amd-instinct-mi250-microarchitecture">
<h3>AMD Instinct™ MI250 microarchitecture<a class="headerlink" href="#amd-instinct-mi250-microarchitecture" title="Link to this heading">#</a></h3>
<p>The microarchitecture of the AMD Instinct MI250 accelerators is based on the
AMD CDNA 2 architecture that targets compute applications such as HPC,
artificial intelligence (AI), and machine learning (ML) and that run on
everything from individual servers to the world’s largest exascale
supercomputers. The overall system architecture is designed for extreme
scalability and compute performance.</p>
<p>The following image shows the components of a single Graphics Compute Die (GCD) of the CDNA 2 architecture. On the top and the bottom are AMD Infinity Fabric™
interfaces and their physical links that are used to connect the GPU die to the
other system-level components of the node (see also Section 2.2). Both
interfaces can drive four AMD Infinity Fabric links. One of the AMD Infinity
Fabric links of the controller at the bottom can be configured as a PCIe link.
Each of the AMD Infinity Fabric links between GPUs can run at up to 25 GT/sec,
which correlates to a peak transfer bandwidth of 50 GB/sec for a 16-wide link (
two bytes per transaction). Section 2.2 has more details on the number of AMD
Infinity Fabric links and the resulting transfer rates between the system-level
components.</p>
<p>To the left and the right are memory controllers that attach the High Bandwidth
Memory (HBM) modules to the GCD. AMD Instinct MI250 GPUs use HBM2e, which offers
a peak memory bandwidth of 1.6 TB/sec per GCD.</p>
<p>The execution units of the GPU are depicted in the following image as Compute
Units (CU). The MI250 GCD has 104 active CUs. Each compute unit is further
subdivided into four SIMD units that process SIMD instructions of 16 data
elements per instruction (for the FP64 data type). This enables the CU to
process 64 work items (a so-called “wavefront”) at a peak clock frequency of 1.7
GHz. Therefore, the theoretical maximum FP64 peak performance per GCD is 22.6
TFLOPS for vector instructions. This equates to 45.3 TFLOPS for vector instructions for both GCDs together. The MI250 compute units also provide specialized
execution units (also called matrix cores), which are geared toward executing
matrix operations like matrix-matrix multiplications. For FP64, the peak
performance of these units amounts to 90.5 TFLOPS.</p>
<p><img alt="Structure of a single GCD in the AMD Instinct MI250 accelerator." src="_images/image001.png"/></p>
<div class="pst-scrollable-table-container"><table class="table" id="mi250-perf-table">
<caption><span class="caption-text">Peak-performance capabilities of the MI250 OAM for different data types.</span><a class="headerlink" href="#mi250-perf-table" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Computation and Data Type</p></th>
<th class="head"><p>FLOPS/CLOCK/CU</p></th>
<th class="head"><p>Peak TFLOPS</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Matrix FP64</p></td>
<td><p>256</p></td>
<td><p>90.5</p></td>
</tr>
<tr class="row-odd"><td><p>Vector FP64</p></td>
<td><p>128</p></td>
<td><p>45.3</p></td>
</tr>
<tr class="row-even"><td><p>Matrix FP32</p></td>
<td><p>256</p></td>
<td><p>90.5</p></td>
</tr>
<tr class="row-odd"><td><p>Packed FP32</p></td>
<td><p>256</p></td>
<td><p>90.5</p></td>
</tr>
<tr class="row-even"><td><p>Vector FP32</p></td>
<td><p>128</p></td>
<td><p>45.3</p></td>
</tr>
<tr class="row-odd"><td><p>Matrix FP16</p></td>
<td><p>1024</p></td>
<td><p>362.1</p></td>
</tr>
<tr class="row-even"><td><p>Matrix BF16</p></td>
<td><p>1024</p></td>
<td><p>362.1</p></td>
</tr>
<tr class="row-odd"><td><p>Matrix INT8</p></td>
<td><p>1024</p></td>
<td><p>362.1</p></td>
</tr>
</tbody>
</table>
</div>
<p>The above table summarizes the aggregated peak performance of the AMD
Instinct MI250 OCP Open Accelerator Modules (OAM, OCP is short for Open Compute
Platform) and its two GCDs for different data types and execution units. The
middle column lists the peak performance (number of data elements processed in a
single instruction) of a single compute unit if a SIMD (or matrix) instruction
is being retired in each clock cycle. The third column lists the theoretical
peak performance of the OAM module. The theoretical aggregated peak memory
bandwidth of the GPU is 3.2 TB/sec (1.6 TB/sec per GCD).</p>
<p><img alt="Dual-GCD architecture of the AMD Instinct MI250 accelerators" src="_images/image002.png"/></p>
<p>The following image shows the block diagram of an OAM package that consists
of two GCDs, each of which constitutes one GPU device in the system. The two
GCDs in the package are connected via four AMD Infinity Fabric links running at
a theoretical peak rate of 25 GT/sec, giving 200 GB/sec peak transfer bandwidth
between the two GCDs of an OAM, or a bidirectional peak transfer bandwidth of
400 GB/sec for the same.</p>
<section id="node-level-architecture">
<h4>Node-level architecture<a class="headerlink" href="#node-level-architecture" title="Link to this heading">#</a></h4>
<p>The following image shows the node-level architecture of a system that is
based on the AMD Instinct MI250 accelerator. The MI250 OAMs attach to the host
system via PCIe Gen 4 x16 links (yellow lines). Each GCD maintains its own PCIe
x16 link to the host part of the system. Depending on the server platform, the
GCD can attach to the AMD EPYC processor directly or via an optional PCIe switch
. Note that some platforms may offer an x8 interface to the GCDs, which reduces
the available host-to-GPU bandwidth.</p>
<p><img alt="Block diagram of AMD Instinct MI250 Accelerators with 3rd Generation AMD EPYC processor" src="_images/image003.png"/></p>
<p>The preceding image shows the node-level architecture of a system with AMD
EPYC processors in a dual-socket configuration and four AMD Instinct MI250
accelerators. The MI250 OAMs attach to the host processors system via PCIe Gen 4
x16 links (yellow lines). Depending on the system design, a PCIe switch may
exist to make more PCIe lanes available for additional components like network
interfaces and/or storage devices. Each GCD maintains its own PCIe x16 link to
the host part of the system or to the PCIe switch. Please note, some platforms
may offer an x8 interface to the GCDs, which will reduce the available
host-to-GPU bandwidth.</p>
<p>Between the OAMs and their respective GCDs, a peer-to-peer (P2P) network allows
for direct data exchange between the GPU dies via AMD Infinity Fabric links (
black, green, and red lines). Each of these 16-wide links connects to one of the
two GPU dies in the MI250 OAM and operates at 25 GT/sec, which corresponds to a
theoretical peak transfer rate of 50 GB/sec per link (or 100 GB/sec
bidirectional peak transfer bandwidth). The GCD pairs 2 and 6 as well as GCDs 0
and 4 connect via two XGMI links, which is indicated by the thicker red line in
the preceding image.</p>
</section>
<div class="toctree-wrapper compound">
</div>
</section>
<span id="document-conceptual/gpu-arch/mi100"></span><section class="tex2jax_ignore mathjax_ignore" id="amd-instinct-mi100-microarchitecture">
<h3>AMD Instinct™ MI100 microarchitecture<a class="headerlink" href="#amd-instinct-mi100-microarchitecture" title="Link to this heading">#</a></h3>
<p>The following image shows the node-level architecture of a system that
comprises two AMD EPYC™ processors and (up to) eight AMD Instinct™ accelerators.
The two EPYC processors are connected to each other with the AMD Infinity™
fabric which provides a high-bandwidth (up to 18 GT/sec) and coherent links such
that each processor can access the available node memory as a single
shared-memory domain in a non-uniform memory architecture (NUMA) fashion. In a
2P, or dual-socket, configuration, three AMD Infinity™ fabric links are
available to connect the processors plus one PCIe Gen 4 x16 link per processor
can attach additional I/O devices such as the host adapters for the network
fabric.</p>
<p><img alt="Structure of a single GCD in the AMD Instinct MI100 accelerator" src="_images/image004.png"/></p>
<p>In a typical node configuration, each processor can host up to four AMD
Instinct™ accelerators that are attached using PCIe Gen 4 links at 16 GT/sec,
which corresponds to a peak bidirectional link bandwidth of 32 GB/sec. Each hive
of four accelerators can participate in a fully connected, coherent AMD
Instinct™ fabric that connects the four accelerators using 23 GT/sec AMD
Infinity fabric links that run at a higher frequency than the inter-processor
links. This inter-GPU link can be established in certified server systems if the
GPUs are mounted in neighboring PCIe slots by installing the AMD Infinity
Fabric™ bridge for the AMD Instinct™ accelerators.</p>
<section id="microarchitecture">
<h4>Microarchitecture<a class="headerlink" href="#microarchitecture" title="Link to this heading">#</a></h4>
<p>The microarchitecture of the AMD Instinct accelerators is based on the AMD CDNA
architecture, which targets compute applications such as high-performance
computing (HPC) and AI &amp; machine learning (ML) that run on everything from
individual servers to the world’s largest exascale supercomputers. The overall
system architecture is designed for extreme scalability and compute performance.</p>
<p><img alt="Structure of the AMD Instinct accelerator (MI100 generation)" src="_images/image005.png"/></p>
<p>The above image shows the AMD Instinct accelerator with its PCIe Gen 4 x16
link (16 GT/sec, at the bottom) that connects the GPU to (one of) the host
processor(s). It also shows the three AMD Infinity Fabric ports that provide
high-speed links (23 GT/sec, also at the bottom) to the other GPUs of the local
hive.</p>
<p>On the left and right of the floor plan, the High Bandwidth Memory (HBM)
attaches via the GPU memory controller.  The MI100 generation of the AMD
Instinct accelerator offers four stacks of HBM generation 2 (HBM2) for a total
of 32GB with a 4,096bit-wide memory interface. The peak memory bandwidth of the
attached HBM2 is 1.228 TB/sec at a memory clock frequency of 1.2 GHz.</p>
<p>The execution units of the GPU are depicted in the above image as Compute
Units (CU). There are a total 120 compute units that are physically organized
into eight Shader Engines (SE) with fifteen compute units per shader engine.
Each compute unit is further sub-divided into four SIMD units that process SIMD
instructions of 16 data elements per instruction. This enables the CU to process
64 data elements (a so-called ‘wavefront’) at a peak clock frequency of 1.5 GHz.
Therefore, the theoretical maximum FP64 peak performance is 11.5 TFLOPS
(<code class="docutils literal notranslate"><span class="pre">4</span> <span class="pre">[SIMD</span> <span class="pre">units]</span> <span class="pre">x</span> <span class="pre">16</span> <span class="pre">[elements</span> <span class="pre">per</span> <span class="pre">instruction]</span> <span class="pre">x</span> <span class="pre">120</span> <span class="pre">[CU]</span> <span class="pre">x</span> <span class="pre">1.5</span> <span class="pre">[GHz]</span></code>).</p>
<p><img alt="Block diagram of an MI100 compute unit with detailed SIMD view of the AMD CDNA architecture" src="_images/image006.png"/></p>
<p>The preceding image shows the block diagram of a single CU of an AMD Instinct™
MI100 accelerator and summarizes how instructions flow through the execution
engines. The CU fetches the instructions via a 32KB instruction cache and moves
them forward to execution via a dispatcher. The CU can handle up to ten
wavefronts at a time and feed their instructions into the execution unit. The
execution unit contains 256 vector general-purpose registers (VGPR) and 800
scalar general-purpose registers (SGPR). The VGPR and SGPR are dynamically
allocated to the executing wavefronts. A wavefront can access a maximum of 102
scalar registers. Excess scalar-register usage will cause register spilling and
thus may affect execution performance.</p>
<p>A wavefront can occupy any number of VGPRs from 0 to 256, directly affecting
occupancy; that is, the number of concurrently active wavefronts in the CU. For
instance, with 119 VGPRs used, only two wavefronts can be active in the CU at
the same time. With the instruction latency of four cycles per SIMD instruction,
the occupancy should be as high as possible such that the compute unit can
improve execution efficiency by scheduling instructions from multiple
wavefronts.</p>
<div class="pst-scrollable-table-container"><table class="table" id="mi100-perf">
<caption><span class="caption-text">Peak-performance capabilities of MI100 for different data types.</span><a class="headerlink" href="#mi100-perf" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Computation and Data Type</p></th>
<th class="head text-center"><p>FLOPS/CLOCK/CU</p></th>
<th class="head text-right"><p>Peak TFLOPS</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Vector FP64</p></td>
<td class="text-center"><p>64</p></td>
<td class="text-right"><p>11.5</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Matrix FP32</p></td>
<td class="text-center"><p>256</p></td>
<td class="text-right"><p>46.1</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Vector FP32</p></td>
<td class="text-center"><p>128</p></td>
<td class="text-right"><p>23.1</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Matrix FP16</p></td>
<td class="text-center"><p>1024</p></td>
<td class="text-right"><p>184.6</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Matrix BF16</p></td>
<td class="text-center"><p>512</p></td>
<td class="text-right"><p>92.3</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<div class="toctree-wrapper compound">
</div>
</section>
</div>
</section>
<span id="document-conceptual/file-reorg"></span><head>
<meta charset="utf-8"/>
<meta content="ROCm Linux Filesystem Hierarchy Standard reorganization" name="description"/>
<meta content="FHS, Linux Filesystem Hierarchy Standard, directory structure,
  AMD, ROCm" name="keywords"/>
</head>
<section class="tex2jax_ignore mathjax_ignore" id="rocm-linux-filesystem-hierarchy-standard-reorganization">
<h2>ROCm Linux Filesystem Hierarchy Standard reorganization<a class="headerlink" href="#rocm-linux-filesystem-hierarchy-standard-reorganization" title="Link to this heading">#</a></h2>
<section id="introduction">
<h3>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h3>
<p>The ROCm Software has adopted the Linux Filesystem Hierarchy Standard (FHS) <a class="reference external" href="https://refspecs.linuxfoundation.org/FHS_3.0/fhs/index.html">https://refspecs.linuxfoundation.org/FHS_3.0/fhs/index.html</a> in order to to ensure ROCm is consistent with standard open source conventions. The following sections specify how current and future releases of ROCm adhere to FHS, how the previous ROCm file system is supported, and how improved versioning specifications are applied to ROCm.</p>
</section>
<section id="adopting-the-fhs">
<h3>Adopting the FHS<a class="headerlink" href="#adopting-the-fhs" title="Link to this heading">#</a></h3>
<p>In order to standardize ROCm directory structure and directory content layout ROCm has adopted the <a class="reference external" href="https://refspecs.linuxfoundation.org/FHS_3.0/fhs/index.html">FHS</a>, adhering to open source conventions for Linux-based distribution. FHS ensures internal consistency within the ROCm stack, as well as external consistency with other systems and distributions. The ROCm proposed file structure is outlined below:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>/opt/rocm-&lt;ver&gt;
    | -- bin
         | -- all public binaries
    | -- lib
         | -- lib&lt;soname&gt;.so-&gt;lib&lt;soname&gt;.so.major-&gt;lib&lt;soname&gt;.so.major.minor.patch
              (public libaries to link with applications)
         | -- &lt;component&gt;
              | -- architecture dependent libraries and binaries used internally by components
         | -- cmake
              | -- &lt;component&gt;
                   | --&lt;component&gt;-config.cmake
    | -- libexec
         | -- &lt;component&gt;
              | -- non ISA/architecture independent executables used internally by components
    | -- include
         | -- &lt;component&gt;
              | -- public header files
    | -- share
         | -- html
              | -- &lt;component&gt;
                   | -- html documentation
         | -- info
              | -- &lt;component&gt;
                   | -- info files
         | -- man
              | -- &lt;component&gt;
                   | -- man pages
         | -- doc
              | -- &lt;component&gt;
                   | -- license files
         | -- &lt;component&gt;
              | -- samples
              | -- architecture independent misc files
</pre></div>
</div>
</section>
<section id="changes-from-earlier-rocm-versions">
<h3>Changes from earlier ROCm versions<a class="headerlink" href="#changes-from-earlier-rocm-versions" title="Link to this heading">#</a></h3>
<p>The following table provides a brief overview of the new ROCm FHS layout, compared to the layout of earlier ROCm versions. Note that /opt/ is used to denote the default rocm-installation-path and should be replaced in case of a non-standard installation location of the ROCm distribution.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> ______________________________________________________
|  New ROCm Layout            |  Previous ROCm Layout  |
|_____________________________|________________________|
| /opt/rocm-&lt;ver&gt;             | /opt/rocm-&lt;ver&gt;        |
|     | -- bin                |     | -- bin           |
|     | -- lib                |     | -- lib           |
|          | -- cmake         |     | -- include       |
|     | -- libexec            |     | -- &lt;component_1&gt; |
|     | -- include            |          | -- bin      |
|          | -- &lt;component_1&gt; |          | -- cmake    |
|     | -- share              |          | -- doc      |
|          | -- html          |          | -- lib      |
|          | -- info          |          | -- include  |
|          | -- man           |          | -- samples  |
|          | -- doc           |     | -- &lt;component_n&gt; |
|          | -- &lt;component_1&gt; |          | -- bin      |
|               | -- samples  |          | -- cmake    |
|               | -- ..       |          | -- doc      |
|          | -- &lt;component_n&gt; |          | -- lib      |
|               | -- samples  |          | -- include  |
|               | -- ..       |          | -- samples  |
|______________________________________________________|
</pre></div>
</div>
</section>
<section id="rocm-fhs-reorganization-backward-compatibility">
<h3>ROCm FHS reorganization: backward compatibility<a class="headerlink" href="#rocm-fhs-reorganization-backward-compatibility" title="Link to this heading">#</a></h3>
<p>The FHS file organization for ROCm was first introduced in the release of ROCm 5.2 . Backward compatibility was implemented to make sure users could still run their ROCm applications while transitioning to the new FHS. ROCm has moved header files and libraries to their new locations as indicated in the above structure, and included symbolic-links and wrapper header files in their old location for backward compatibility. The following sections detail ROCm backward compatibility implementation for wrapper header files, executable files, library files and CMake config files.</p>
<section id="wrapper-header-files">
<h4>Wrapper header files<a class="headerlink" href="#wrapper-header-files" title="Link to this heading">#</a></h4>
<p>Wrapper header files are placed in the old location (
<code class="docutils literal notranslate"><span class="pre">/opt/rocm-&lt;ver&gt;/&lt;component&gt;/include</span></code>) with a warning message to include files
from the new location (<code class="docutils literal notranslate"><span class="pre">/opt/rocm-&lt;ver&gt;/include</span></code>) as shown in the example below.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#pragma message "This file is deprecated. Use file from include path /opt/rocm-ver/include/ and prefix with hip."</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;hip/hip_runtime.h&gt;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Starting at ROCm 5.2 release, the deprecation for backward compatibility wrapper header files is: <code class="docutils literal notranslate"><span class="pre">#pragma</span></code> message announcing <code class="docutils literal notranslate"><span class="pre">#warning</span></code>.</p></li>
<li><p>Starting from ROCm 6.0 (tentatively) backward compatibility for wrapper header files will be removed, and the <code class="docutils literal notranslate"><span class="pre">#pragma</span></code> message will be announcing <code class="docutils literal notranslate"><span class="pre">#error</span></code>.</p></li>
</ul>
</section>
<section id="executable-files">
<h4>Executable files<a class="headerlink" href="#executable-files" title="Link to this heading">#</a></h4>
<p>Executable files are available in the <code class="docutils literal notranslate"><span class="pre">/opt/rocm-&lt;ver&gt;/bin</span></code> folder. For backward
compatibility, the old library location (<code class="docutils literal notranslate"><span class="pre">/opt/rocm-&lt;ver&gt;/&lt;component&gt;/bin</span></code>) has a
soft link to the library at the new location. Soft links will be removed in a
future release, tentatively ROCm v6.0.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>ls<span class="w"> </span>-l<span class="w"> </span>/opt/rocm/hip/bin/
lrwxrwxrwx<span class="w"> </span><span class="m">1</span><span class="w"> </span>root<span class="w"> </span>root<span class="w">   </span><span class="m">24</span><span class="w"> </span>Jan<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">23</span>:32<span class="w"> </span>hipcc<span class="w"> </span>-&gt;<span class="w"> </span>../../bin/hipcc
</pre></div>
</div>
</section>
<section id="library-files">
<h4>Library files<a class="headerlink" href="#library-files" title="Link to this heading">#</a></h4>
<p>Library files are available in the <code class="docutils literal notranslate"><span class="pre">/opt/rocm-&lt;ver&gt;/lib</span></code> folder. For backward
compatibility, the old library location (<code class="docutils literal notranslate"><span class="pre">/opt/rocm-&lt;ver&gt;/&lt;component&gt;/lib</span></code>) has a
soft link to the library at the new location. Soft links will be removed in a
future release, tentatively ROCm v6.0.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>ls<span class="w"> </span>-l<span class="w"> </span>/opt/rocm/hip/lib/
drwxr-xr-x<span class="w"> </span><span class="m">4</span><span class="w"> </span>root<span class="w"> </span>root<span class="w"> </span><span class="m">4096</span><span class="w"> </span>Jan<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">10</span>:45<span class="w"> </span>cmake
lrwxrwxrwx<span class="w"> </span><span class="m">1</span><span class="w"> </span>root<span class="w"> </span>root<span class="w">   </span><span class="m">24</span><span class="w"> </span>Jan<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">23</span>:32<span class="w"> </span>libamdhip64.so<span class="w"> </span>-&gt;<span class="w"> </span>../../lib/libamdhip64.so
</pre></div>
</div>
</section>
<section id="cmake-config-files">
<h4>CMake config files<a class="headerlink" href="#cmake-config-files" title="Link to this heading">#</a></h4>
<p>All CMake configuration files are available in the
<code class="docutils literal notranslate"><span class="pre">/opt/rocm-&lt;ver&gt;/lib/cmake/&lt;component&gt;</span></code> folder. For backward compatibility, the
old CMake locations (<code class="docutils literal notranslate"><span class="pre">/opt/rocm-&lt;ver&gt;/&lt;component&gt;/lib/cmake</span></code>) consist of a soft
link to the new CMake config. Soft links will be removed in a future release,
tentatively ROCm v6.0.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>ls<span class="w"> </span>-l<span class="w"> </span>/opt/rocm/hip/lib/cmake/hip/
lrwxrwxrwx<span class="w"> </span><span class="m">1</span><span class="w"> </span>root<span class="w"> </span>root<span class="w"> </span><span class="m">42</span><span class="w"> </span>Jan<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">23</span>:32<span class="w"> </span>hip-config.cmake<span class="w"> </span>-&gt;<span class="w"> </span>../../../../lib/cmake/hip/hip-config.cmake
</pre></div>
</div>
</section>
</section>
<section id="changes-required-in-applications-using-rocm">
<h3>Changes required in applications using ROCm<a class="headerlink" href="#changes-required-in-applications-using-rocm" title="Link to this heading">#</a></h3>
<p>Applications using ROCm are advised to use the new file paths. As the old files
will be deprecated in a future release. Applications have to make sure to include
correct header file and use correct search paths.</p>
<ol class="arabic">
<li><p><code class="docutils literal notranslate"><span class="pre">#include&lt;header_file.h&gt;</span></code> needs to be changed to
<code class="docutils literal notranslate"><span class="pre">#include</span> <span class="pre">&lt;component/header_file.h&gt;</span></code></p>
<p>For example: <code class="docutils literal notranslate"><span class="pre">#include</span> <span class="pre">&lt;hip.h&gt;</span></code> needs to change
to <code class="docutils literal notranslate"><span class="pre">#include</span> <span class="pre">&lt;hip/hip.h&gt;</span></code></p>
</li>
<li><p>Any variable in CMake or Makefiles pointing to component folder needs to
changed.</p>
<p>For example: <code class="docutils literal notranslate"><span class="pre">VAR1=/opt/rocm/hip</span></code> needs to be changed to <code class="docutils literal notranslate"><span class="pre">VAR1=/opt/rocm</span></code>
<code class="docutils literal notranslate"><span class="pre">VAR2=/opt/rocm/hsa</span></code> needs to be changed to <code class="docutils literal notranslate"><span class="pre">VAR2=/opt/rocm</span></code></p>
</li>
<li><p>Any reference to <code class="docutils literal notranslate"><span class="pre">/opt/rocm/&lt;component&gt;/bin</span></code> or <code class="docutils literal notranslate"><span class="pre">/opt/rocm/&lt;component&gt;/lib</span></code>
needs to be changed to <code class="docutils literal notranslate"><span class="pre">/opt/rocm/bin</span></code> and <code class="docutils literal notranslate"><span class="pre">/opt/rocm/lib/</span></code>, respectively.</p></li>
</ol>
</section>
<section id="changes-in-versioning-specifications">
<h3>Changes in versioning specifications<a class="headerlink" href="#changes-in-versioning-specifications" title="Link to this heading">#</a></h3>
<p>In order to better manage ROCm dependencies specification and allow smoother releases of ROCm while avoiding dependency conflicts, ROCm software shall adhere to the following scheme when numbering and incrementing ROCm files versions:</p>
<p>rocm-&lt;ver&gt;, where &lt;ver&gt; = &lt;x.y.z&gt;</p>
<p>x.y.z denote: MAJOR.MINOR.PATCH</p>
<p>z: PATCH - increment z when implementing backward compatible bug fixes.</p>
<p>y: MINOR - increment y when implementing minor changes that add functionality but are still backward compatible.</p>
<p>x: MAJOR - increment x when implementing major changes that are not backward compatible.</p>
</section>
</section>
<span id="document-conceptual/gpu-isolation"></span><head>
<meta charset="utf-8"/>
<meta content="GPU isolation techniques" name="description"/>
<meta content="GPU isolation techniques, UUID, universally unique identifier,
  environment variables, virtual machines, AMD, ROCm" name="keywords"/>
</head>
<section class="tex2jax_ignore mathjax_ignore" id="gpu-isolation-techniques">
<h2>GPU isolation techniques<a class="headerlink" href="#gpu-isolation-techniques" title="Link to this heading">#</a></h2>
<p>Restricting the access of applications to a subset of GPUs, aka isolating
GPUs allows users to hide GPU resources from programs. The programs by default
will only use the “exposed” GPUs ignoring other (hidden) GPUs in the system.</p>
<p>There are multiple ways to achieve isolation of GPUs in the ROCm software stack,
differing in which applications they apply to and the security they provide.
This page serves as an overview of the techniques.</p>
<section id="environment-variables">
<h3>Environment variables<a class="headerlink" href="#environment-variables" title="Link to this heading">#</a></h3>
<p>The runtimes in the ROCm software stack read these environment variables to
select the exposed or default device to present to applications using them.</p>
<p>Environment variables shouldn’t be used for isolating untrusted applications,
as an application can reset them before initializing the runtime.</p>
<section id="rocr-visible-devices">
<h4><code class="docutils literal notranslate"><span class="pre">ROCR_VISIBLE_DEVICES</span></code><a class="headerlink" href="#rocr-visible-devices" title="Link to this heading">#</a></h4>
<p>A list of device indices or <abbr title="universally unique identifier">UUID</abbr>s
that will be exposed to applications.</p>
<p>Runtime
: ROCm Software Runtime. Applies to all applications using the user mode ROCm
software stack.</p>
<div class="literal-block-wrapper docutils container" id="id2">
<div class="code-block-caption"><span class="caption-text">Example to expose the 1. device and a device based on UUID.</span><a class="headerlink" href="#id2" title="Link to this code">#</a></div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">ROCR_VISIBLE_DEVICES</span><span class="o">=</span><span class="s2">"0,GPU-DEADBEEFDEADBEEF"</span>
</pre></div>
</div>
</div>
</section>
<section id="gpu-device-ordinal">
<h4><code class="docutils literal notranslate"><span class="pre">GPU_DEVICE_ORDINAL</span></code><a class="headerlink" href="#gpu-device-ordinal" title="Link to this heading">#</a></h4>
<p>Devices indices exposed to OpenCL and HIP applications.</p>
<p>Runtime
: ROCm Compute Language Runtime (<code class="docutils literal notranslate"><span class="pre">ROCclr</span></code>). Applies to applications and runtimes
using the <code class="docutils literal notranslate"><span class="pre">ROCclr</span></code> abstraction layer including HIP and OpenCL applications.</p>
<div class="literal-block-wrapper docutils container" id="id3">
<div class="code-block-caption"><span class="caption-text">Example to expose the 1. and 3. device in the system.</span><a class="headerlink" href="#id3" title="Link to this code">#</a></div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">GPU_DEVICE_ORDINAL</span><span class="o">=</span><span class="s2">"0,2"</span>
</pre></div>
</div>
</div>
</section>
<section id="hip-visible-devices">
<span id="id1"></span><h4><code class="docutils literal notranslate"><span class="pre">HIP_VISIBLE_DEVICES</span></code><a class="headerlink" href="#hip-visible-devices" title="Link to this heading">#</a></h4>
<p>Device indices exposed to HIP applications.</p>
<p>Runtime: HIP runtime. Applies only to applications using HIP on the AMD platform.</p>
<div class="literal-block-wrapper docutils container" id="id4">
<div class="code-block-caption"><span class="caption-text">Example to expose the 1. and 3. devices in the system.</span><a class="headerlink" href="#id4" title="Link to this code">#</a></div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">HIP_VISIBLE_DEVICES</span><span class="o">=</span><span class="s2">"0,2"</span>
</pre></div>
</div>
</div>
</section>
<section id="cuda-visible-devices">
<h4><code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code><a class="headerlink" href="#cuda-visible-devices" title="Link to this heading">#</a></h4>
<p>Provided for CUDA compatibility, has the same effect as <code class="docutils literal notranslate"><span class="pre">HIP_VISIBLE_DEVICES</span></code>
on the AMD platform.</p>
<p>Runtime
: HIP or CUDA Runtime. Applies to HIP applications on the AMD or NVIDIA platform
and CUDA applications.</p>
</section>
<section id="omp-default-device">
<h4><code class="docutils literal notranslate"><span class="pre">OMP_DEFAULT_DEVICE</span></code><a class="headerlink" href="#omp-default-device" title="Link to this heading">#</a></h4>
<p>Default device used for OpenMP target offloading.</p>
<p>Runtime
: OpenMP Runtime. Applies only to applications using OpenMP offloading.</p>
<div class="literal-block-wrapper docutils container" id="id5">
<div class="code-block-caption"><span class="caption-text">Example on setting the default device to the third device.</span><a class="headerlink" href="#id5" title="Link to this code">#</a></div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_DEFAULT_DEVICE</span><span class="o">=</span><span class="s2">"2"</span>
</pre></div>
</div>
</div>
</section>
</section>
<section id="docker">
<h3>Docker<a class="headerlink" href="#docker" title="Link to this heading">#</a></h3>
<p>Docker uses Linux kernel namespaces to provide isolated environments for
applications. This isolation applies to most devices by default, including
GPUs. To access them in containers explicit access must be granted, please see
<a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/how-to/docker.html#docker-access-gpus-in-container" title="(in ROCm installation on Linux v6.4.2)"><span>Accessing GPUs in containers</span></a> for details.
Specifically refer to <a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/how-to/docker.html#docker-restrict-gpus" title="(in ROCm installation on Linux v6.4.2)"><span>Restricting GPU access</span></a> on exposing just a subset
of all GPUs.</p>
<p>Docker isolation is more secure than environment variables, and applies
to all programs that use the <code class="docutils literal notranslate"><span class="pre">amdgpu</span></code> kernel module interfaces.
Even programs that don’t use the ROCm runtime, like graphics applications
using OpenGL or Vulkan, can only access the GPUs exposed to the container.</p>
</section>
<section id="gpu-passthrough-to-virtual-machines">
<h3>GPU passthrough to virtual machines<a class="headerlink" href="#gpu-passthrough-to-virtual-machines" title="Link to this heading">#</a></h3>
<p>Virtual machines achieve the highest level of isolation, because even the kernel
of the virtual machine is isolated from the host. Devices physically installed
in the host system can be passed to the virtual machine using PCIe passthrough.
This allows for using the GPU with a different operating systems like a Windows
guest from a Linux host.</p>
<p>Setting up PCIe passthrough is specific to the hypervisor used. ROCm officially
supports <a class="reference external" href="https://www.vmware.com/products/esxi-and-esx.html">VMware ESXi</a>
for select GPUs.</p>
<!--
TODO: This should link to a page about virtualization that explains
      pass-through and SR-IOV and how-tos for maybe `libvirt` and `VMWare`
-->
</section>
</section>
<span id="document-conceptual/cmake-packages"></span><section id="using-cmake">
<h2>Using CMake<a class="headerlink" href="#using-cmake" title="Link to this heading">#</a></h2>
<p>Most components in ROCm support CMake. Projects depending on header-only or
library components typically require CMake 3.5 or higher whereas those wanting
to make use of the CMake HIP language support will require CMake 3.21 or higher.</p>
<section id="finding-dependencies">
<h3>Finding dependencies<a class="headerlink" href="#finding-dependencies" title="Link to this heading">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For a complete
reference on how to deal with dependencies in CMake, refer to the CMake docs
on <a class="reference external" href="https://cmake.org/cmake/help/latest/command/find_package.html">find_package</a> and the
<a class="reference external" href="https://cmake.org/cmake/help/latest/guide/using-dependencies/index.html">Using Dependencies Guide</a>
to get an overview of CMake related facilities.</p>
</div>
<p>In short, CMake supports finding dependencies in two ways:</p>
<ul class="simple">
<li><p>In Module mode, it consults a file <code class="docutils literal notranslate"><span class="pre">Find&lt;PackageName&gt;.cmake</span></code> which tries to find the component
in typical install locations and layouts. CMake ships a few dozen such scripts, but users and projects
may ship them as well.</p></li>
<li><p>In Config mode, it locates a file named <code class="docutils literal notranslate"><span class="pre">&lt;packagename&gt;-config.cmake</span></code> or
<code class="docutils literal notranslate"><span class="pre">&lt;PackageName&gt;Config.cmake</span></code> which describes the installed component in all regards needed to
consume it.</p></li>
</ul>
<p>ROCm predominantly relies on Config mode, one notable exception being the Module
driving the compilation of HIP programs on NVIDIA runtimes. As such, when
dependencies are not found in standard system locations, one either has to
instruct CMake to search for package config files in additional folders using
the <code class="docutils literal notranslate"><span class="pre">CMAKE_PREFIX_PATH</span></code> variable (a semi-colon separated list of file system
paths), or using <code class="docutils literal notranslate"><span class="pre">&lt;PackageName&gt;_ROOT</span></code> variable on a project-specific basis.</p>
<p>There are nearly a dozen ways to set these variables. One may be more convenient
over the other depending on your workflow. Conceptually the simplest is adding
it to your CMake configuration command on the command line via
<code class="docutils literal notranslate"><span class="pre">-D</span> <span class="pre">CMAKE_PREFIX_PATH=....</span></code> . AMD packaged ROCm installs can typically be
added to the config file search paths such as:</p>
<ul class="simple">
<li><p>Windows: <code class="docutils literal notranslate"><span class="pre">-D</span> <span class="pre">CMAKE_PREFIX_PATH=${env:HIP_PATH}</span></code></p></li>
<li><p>Linux: <code class="docutils literal notranslate"><span class="pre">-D</span> <span class="pre">CMAKE_PREFIX_PATH=/opt/rocm</span></code></p></li>
</ul>
<p>ROCm provides the respective <em>config-file</em> packages, and this enables
<code class="docutils literal notranslate"><span class="pre">find_package</span></code> to be used directly. ROCm does not require any Find module as
the <em>config-file</em> packages are shipped with the upstream projects, such as
rocPRIM and other ROCm libraries.</p>
<p>For a complete guide on where and how ROCm may be installed on a system, refer
to the installation guides for
<a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/tutorial/quick-start.html">Linux</a>
and
<a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-windows/en/latest/index.html">Windows</a>.</p>
</section>
<section id="using-hip-in-cmake">
<h3>Using HIP in CMake<a class="headerlink" href="#using-hip-in-cmake" title="Link to this heading">#</a></h3>
<p>ROCm components providing a C/C++ interface support consumption via any
C/C++ toolchain that CMake knows how to drive. ROCm also supports the CMake HIP
language features, allowing users to program using the HIP single-source
programming model. When a program (or translation-unit) uses the HIP API without
compiling any GPU device code, HIP can be treated in CMake as a simple C/C++
library.</p>
<section id="using-the-hip-single-source-programming-model">
<h4>Using the HIP single-source programming model<a class="headerlink" href="#using-the-hip-single-source-programming-model" title="Link to this heading">#</a></h4>
<p>Source code written in the HIP dialect of C++ typically uses the <cite>.hip</cite>
extension. When the HIP CMake language is enabled, it will automatically
associate such source files with the HIP toolchain being used.</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">cmake_minimum_required</span><span class="p">(</span><span class="s">VERSION</span><span class="w"> </span><span class="s">3.21</span><span class="p">)</span><span class="w"> </span><span class="c"># HIP language support requires 3.21</span>
<span class="nb">cmake_policy</span><span class="p">(</span><span class="s">VERSION</span><span class="w"> </span><span class="s">3.21.3...3.27</span><span class="p">)</span>
<span class="nb">project</span><span class="p">(</span><span class="s">MyProj</span><span class="w"> </span><span class="s">LANGUAGES</span><span class="w"> </span><span class="s">HIP</span><span class="p">)</span>
<span class="nb">add_executable</span><span class="p">(</span><span class="s">MyApp</span><span class="w"> </span><span class="s">Main.hip</span><span class="p">)</span>
</pre></div>
</div>
<p>Should you have existing CUDA code that is from the source compatible subset of
HIP, you can tell CMake that despite their <cite>.cu</cite> extension, they’re HIP sources.
Do note that this mostly facilitates compiling kernel code-only source files,
as host-side CUDA API won’t compile in this fashion.</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">add_library</span><span class="p">(</span><span class="s">MyLib</span><span class="w"> </span><span class="s">MyLib.cu</span><span class="p">)</span>
<span class="nb">set_source_files_properties</span><span class="p">(</span><span class="s">MyLib.cu</span><span class="w"> </span><span class="s">PROPERTIES</span><span class="w"> </span><span class="s">LANGUAGE</span><span class="w"> </span><span class="s">HIP</span><span class="p">)</span>
</pre></div>
</div>
<p>CMake itself only hosts part of the HIP language support, such as defining
HIP-specific properties, etc. while the other half ships with the HIP
implementation, such as ROCm. CMake will search for a file
<cite>hip-lang-config.cmake</cite> describing how the the properties defined by CMake
translate to toolchain invocations. If one installs ROCm using non-standard
methods or layouts and CMake can’t locate this file or detect parts of the SDK,
there’s a catch-all, last resort variable consulted locating this file,
<code class="docutils literal notranslate"><span class="pre">-D</span> <span class="pre">CMAKE_HIP_COMPILER_ROCM_ROOT:PATH=</span></code> which should be set the root of the
ROCm installation.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Imported targets defined by <cite>hip-lang-config.cmake</cite> are for internal use
only.</p>
</div>
<p>If the user doesn’t provide a semi-colon delimited list of device architectures
via <code class="docutils literal notranslate"><span class="pre">CMAKE_HIP_ARCHITECTURES</span></code>, CMake will select some sensible default. It is
advised though that if a user knows what devices they wish to target, then set
this variable explicitly.</p>
</section>
<section id="consuming-rocm-c-c-libraries">
<h4>Consuming ROCm C/C++ libraries<a class="headerlink" href="#consuming-rocm-c-c-libraries" title="Link to this heading">#</a></h4>
<p>Libraries such as rocBLAS, rocFFT, MIOpen, etc. behave as C/C++ libraries.
Illustrated in the example below is a C++ application using MIOpen from CMake.
It calls <code class="docutils literal notranslate"><span class="pre">find_package(miopen)</span></code>, which provides the <code class="docutils literal notranslate"><span class="pre">MIOpen</span></code> imported
target. This can be linked with <code class="docutils literal notranslate"><span class="pre">target_link_libraries</span></code></p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">cmake_minimum_required</span><span class="p">(</span><span class="s">VERSION</span><span class="w"> </span><span class="s">3.5</span><span class="p">)</span><span class="w"> </span><span class="c"># find_package(miopen) requires 3.5</span>
<span class="nb">cmake_policy</span><span class="p">(</span><span class="s">VERSION</span><span class="w"> </span><span class="s">3.5...3.27</span><span class="p">)</span>
<span class="nb">project</span><span class="p">(</span><span class="s">MyProj</span><span class="w"> </span><span class="s">LANGUAGES</span><span class="w"> </span><span class="s">CXX</span><span class="p">)</span>
<span class="nb">find_package</span><span class="p">(</span><span class="s">miopen</span><span class="p">)</span>
<span class="nb">add_library</span><span class="p">(</span><span class="s">MyLib</span><span class="w"> </span><span class="s">...</span><span class="p">)</span>
<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">MyLib</span><span class="w"> </span><span class="s">PUBLIC</span><span class="w"> </span><span class="s">MIOpen</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Most libraries are designed as host-only API, so using a GPU device
compiler is not necessary for downstream projects unless they use GPU device
code.</p>
</div>
</section>
<section id="consuming-the-hip-api-in-c-code">
<h4>Consuming the HIP API in C++ code<a class="headerlink" href="#consuming-the-hip-api-in-c-code" title="Link to this heading">#</a></h4>
<p>Consuming the HIP API without compiling single-source GPU device code can be
done using any C++ compiler. The <code class="docutils literal notranslate"><span class="pre">find_package(hip)</span></code> provides the
<code class="docutils literal notranslate"><span class="pre">hip::host</span></code> imported target to use HIP in this scenario.</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">cmake_minimum_required</span><span class="p">(</span><span class="s">VERSION</span><span class="w"> </span><span class="s">3.5</span><span class="p">)</span><span class="w"> </span><span class="c"># find_package(hip) requires 3.5</span>
<span class="nb">cmake_policy</span><span class="p">(</span><span class="s">VERSION</span><span class="w"> </span><span class="s">3.5...3.27</span><span class="p">)</span>
<span class="nb">project</span><span class="p">(</span><span class="s">MyProj</span><span class="w"> </span><span class="s">LANGUAGES</span><span class="w"> </span><span class="s">CXX</span><span class="p">)</span>
<span class="nb">find_package</span><span class="p">(</span><span class="s">hip</span><span class="w"> </span><span class="s">REQUIRED</span><span class="p">)</span>
<span class="nb">add_executable</span><span class="p">(</span><span class="s">MyApp</span><span class="w"> </span><span class="s">...</span><span class="p">)</span>
<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">MyApp</span><span class="w"> </span><span class="s">PRIVATE</span><span class="w"> </span><span class="s">hip::host</span><span class="p">)</span>
</pre></div>
</div>
<p>When mixing such <code class="docutils literal notranslate"><span class="pre">CXX</span></code> sources with <code class="docutils literal notranslate"><span class="pre">HIP</span></code> sources holding device-code, link
only to <cite>hip::host</cite>. If HIP sources don’t have <cite>.hip</cite> as their extension, use
<cite>set_source_files_properties(&lt;hip_sources&gt;… PROPERTIES LANGUAGE HIP)</cite> on them.
Linking to <cite>hip::host</cite> will set all the necessary flags for the <code class="docutils literal notranslate"><span class="pre">CXX</span></code> sources
while <code class="docutils literal notranslate"><span class="pre">HIP</span></code> sources inherit all flags from the built-in language support.
Having HIP sources in a target will turn the <a class="reference external" href="https://cmake.org/cmake/help/latest/prop_tgt/LINKER_LANGUAGE.html"><code class="docutils literal notranslate"><span class="pre">LINKER_LANGUAGE</span></code></a> into <code class="docutils literal notranslate"><span class="pre">HIP</span></code>.</p>
</section>
<section id="compiling-device-code-in-c-language-mode">
<h4>Compiling device code in C++ language mode<a class="headerlink" href="#compiling-device-code-in-c-language-mode" title="Link to this heading">#</a></h4>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>The workflow detailed here is considered legacy and is shown for
understanding’s sake. It pre-dates the existence of HIP language support in
CMake. If source code has HIP device code in it, it is a HIP source file
and should be compiled as such. Only resort to the method below if your
HIP-enabled CMake code path can’t mandate CMake version 3.21.</p>
</div>
<p>If code uses the HIP API and compiles GPU device code, it requires using a
device compiler. The compiler for CMake can be set using either the
<code class="docutils literal notranslate"><span class="pre">CMAKE_C_COMPILER</span></code> and <code class="docutils literal notranslate"><span class="pre">CMAKE_CXX_COMPILER</span></code> variable or using the <code class="docutils literal notranslate"><span class="pre">CC</span></code>
and <code class="docutils literal notranslate"><span class="pre">CXX</span></code> environment variables. This can be set when configuring CMake or
put into a CMake toolchain file. The device compiler must be set to a
compiler that supports AMD GPU targets, which is usually Clang.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">find_package(hip)</span></code> provides the <code class="docutils literal notranslate"><span class="pre">hip::device</span></code> imported target to add
all the flags necessary for device compilation.</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">cmake_minimum_required</span><span class="p">(</span><span class="s">VERSION</span><span class="w"> </span><span class="s">3.8</span><span class="p">)</span><span class="w"> </span><span class="c"># cxx_std_11 requires 3.8</span>
<span class="nb">cmake_policy</span><span class="p">(</span><span class="s">VERSION</span><span class="w"> </span><span class="s">3.8...3.27</span><span class="p">)</span>
<span class="nb">project</span><span class="p">(</span><span class="s">MyProj</span><span class="w"> </span><span class="s">LANGUAGES</span><span class="w"> </span><span class="s">CXX</span><span class="p">)</span>
<span class="nb">find_package</span><span class="p">(</span><span class="s">hip</span><span class="w"> </span><span class="s">REQUIRED</span><span class="p">)</span>
<span class="nb">add_library</span><span class="p">(</span><span class="s">MyLib</span><span class="w"> </span><span class="s">...</span><span class="p">)</span>
<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">MyLib</span><span class="w"> </span><span class="s">PRIVATE</span><span class="w"> </span><span class="s">hip::device</span><span class="p">)</span>
<span class="nb">target_compile_features</span><span class="p">(</span><span class="s">MyLib</span><span class="w"> </span><span class="s">PRIVATE</span><span class="w"> </span><span class="s">cxx_std_11</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Compiling for the GPU device requires at least C++11.</p>
</div>
<p>This project can then be configured with the following CMake commands:</p>
<ul class="simple">
<li><p>Windows: <code class="docutils literal notranslate"><span class="pre">cmake</span> <span class="pre">-D</span> <span class="pre">CMAKE_CXX_COMPILER:PATH=${env:HIP_PATH}\bin\clang++.exe</span></code></p></li>
<li><p>Linux: <code class="docutils literal notranslate"><span class="pre">cmake</span> <span class="pre">-D</span> <span class="pre">CMAKE_CXX_COMPILER:PATH=/opt/rocm/bin/amdclang++</span></code></p></li>
</ul>
<p>Which use the device compiler provided from the binary packages of
<a class="reference external" href="https://www.amd.com/en/developer/resources/rocm-hub/hip-sdk.html">ROCm HIP SDK</a> and
<a class="reference external" href="https://repo.radeon.com">repo.radeon.com</a> respectively.</p>
<p>When using the <code class="docutils literal notranslate"><span class="pre">CXX</span></code> language support to compile HIP device code, selecting the
target GPU architectures is done via setting the <code class="docutils literal notranslate"><span class="pre">GPU_TARGETS</span></code> variable.
<code class="docutils literal notranslate"><span class="pre">CMAKE_HIP_ARCHITECTURES</span></code> only exists when the HIP language is enabled. By
default, this is set to some subset of the currently supported architectures of
AMD ROCm. It can be set to the CMake option <code class="docutils literal notranslate"><span class="pre">-D</span> <span class="pre">GPU_TARGETS="gfx1032;gfx1035"</span></code>.</p>
</section>
<section id="rocm-cmake-packages">
<h4>ROCm CMake packages<a class="headerlink" href="#rocm-cmake-packages" title="Link to this heading">#</a></h4>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Package</p></th>
<th class="head"><p>Targets</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>HIP</p></td>
<td><p>hip</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">hip::host</span></code>, <code class="docutils literal notranslate"><span class="pre">hip::device</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>rocPRIM</p></td>
<td><p>rocprim</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">roc::rocprim</span></code></p></td>
</tr>
<tr class="row-even"><td><p>rocThrust</p></td>
<td><p>rocthrust</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">roc::rocthrust</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>hipCUB</p></td>
<td><p>hipcub</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">hip::hipcub</span></code></p></td>
</tr>
<tr class="row-even"><td><p>rocRAND</p></td>
<td><p>rocrand</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">roc::rocrand</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>rocBLAS</p></td>
<td><p>rocblas</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">roc::rocblas</span></code></p></td>
</tr>
<tr class="row-even"><td><p>rocSOLVER</p></td>
<td><p>rocsolver</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">roc::rocsolver</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>hipBLAS</p></td>
<td><p>hipblas</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">roc::hipblas</span></code></p></td>
</tr>
<tr class="row-even"><td><p>rocFFT</p></td>
<td><p>rocfft</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">roc::rocfft</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>hipFFT</p></td>
<td><p>hipfft</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">hip::hipfft</span></code></p></td>
</tr>
<tr class="row-even"><td><p>rocSPARSE</p></td>
<td><p>rocsparse</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">roc::rocsparse</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>hipSPARSE</p></td>
<td><p>hipsparse</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">roc::hipsparse</span></code></p></td>
</tr>
<tr class="row-even"><td><p>rocALUTION</p></td>
<td><p>rocalution</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">roc::rocalution</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>RCCL</p></td>
<td><p>rccl</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">rccl</span></code></p></td>
</tr>
<tr class="row-even"><td><p>MIOpen</p></td>
<td><p>miopen</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">MIOpen</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>MIGraphX</p></td>
<td><p>migraphx</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">migraphx::migraphx</span></code>, <code class="docutils literal notranslate"><span class="pre">migraphx::migraphx_c</span></code>,
<code class="docutils literal notranslate"><span class="pre">migraphx::migraphx_cpu</span></code>, <code class="docutils literal notranslate"><span class="pre">migraphx::migraphx_gpu</span></code>,
<code class="docutils literal notranslate"><span class="pre">migraphx::migraphx_onnx</span></code>, <code class="docutils literal notranslate"><span class="pre">migraphx::migraphx_tf</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="using-cmake-presets">
<h3>Using CMake presets<a class="headerlink" href="#using-cmake-presets" title="Link to this heading">#</a></h3>
<p>CMake command lines depending on how specific users like to be when compiling
code can grow to unwieldy lengths. This is the primary reason why projects tend
to bake script snippets into their build definitions controlling compiler
warning levels, changing CMake defaults (<code class="docutils literal notranslate"><span class="pre">CMAKE_BUILD_TYPE</span></code> or
<code class="docutils literal notranslate"><span class="pre">BUILD_SHARED_LIBS</span></code> just to name a few) and all sorts anti-patterns, all in
the name of convenience.</p>
<p>Load on the command-line interface (CLI) starts immediately by selecting a
toolchain, the set of utilities used to compile programs. To ease some of the
toolchain related pains, CMake does consult the <code class="docutils literal notranslate"><span class="pre">CC</span></code> and <code class="docutils literal notranslate"><span class="pre">CXX</span></code> environmental
variables when setting a default <code class="docutils literal notranslate"><span class="pre">CMAKE_C[XX]_COMPILER</span></code> respectively, but that
is just the tip of the iceberg. There’s a fair number of variables related to
just the toolchain itself (typically supplied using
<a class="reference external" href="https://cmake.org/cmake/help/latest/manual/cmake-toolchains.7.html">toolchain files</a>
), and then we still haven’t talked about user preference or project-specific
options.</p>
<p>IDEs supporting CMake (Visual Studio, Visual Studio Code, CLion, etc.) all came
up with their own way to register command-line fragments of different purpose in
a setup-and-forget fashion for quick assembly using graphical front-ends. This is
all nice, but configurations aren’t portable, nor can they be reused in
Continuous Integration (CI) pipelines. CMake has condensed existing practice
into a portable JSON format that works in all IDEs and can be invoked from any
command line. This is
<a class="reference external" href="https://cmake.org/cmake/help/latest/manual/cmake-presets.7.html">CMake Presets</a>.</p>
<p>There are two types of preset files: one supplied by the project, called
<code class="docutils literal notranslate"><span class="pre">CMakePresets.json</span></code> which is meant to be committed to version control,
typically used to drive CI; and one meant for the user to provide, called
<code class="docutils literal notranslate"><span class="pre">CMakeUserPresets.json</span></code>, typically used to house user preference and adapting
the build to the user’s environment. These JSON files are allowed to include
other JSON files and the user presets always implicitly includes the non-user
variant.</p>
<section id="using-hip-with-presets">
<h4>Using HIP with presets<a class="headerlink" href="#using-hip-with-presets" title="Link to this heading">#</a></h4>
<p>Following is an example <code class="docutils literal notranslate"><span class="pre">CMakeUserPresets.json</span></code> file which actually compiles
the <a class="reference external" href="https://github.com/amd/rocm-examples">amd/rocm-examples</a> suite of sample
applications on a typical ROCm installation:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">"version"</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span>
<span class="w">  </span><span class="nt">"cmakeMinimumRequired"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">"major"</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span>
<span class="w">    </span><span class="nt">"minor"</span><span class="p">:</span><span class="w"> </span><span class="mi">21</span><span class="p">,</span>
<span class="w">    </span><span class="nt">"patch"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="nt">"configurePresets"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"layout"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"hidden"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"binaryDir"</span><span class="p">:</span><span class="w"> </span><span class="s2">"${sourceDir}/build/${presetName}"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"installDir"</span><span class="p">:</span><span class="w"> </span><span class="s2">"${sourceDir}/install/${presetName}"</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generator-ninja-multi-config"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"hidden"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"generator"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Ninja Multi-Config"</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"toolchain-makefiles-c/c++-amdclang"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"hidden"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"cacheVariables"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">"CMAKE_C_COMPILER"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/opt/rocm/bin/amdclang"</span><span class="p">,</span>
<span class="w">        </span><span class="nt">"CMAKE_CXX_COMPILER"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/opt/rocm/bin/amdclang++"</span><span class="p">,</span>
<span class="w">        </span><span class="nt">"CMAKE_HIP_COMPILER"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/opt/rocm/bin/amdclang++"</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"clang-strict-iso-high-warn"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"hidden"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"cacheVariables"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">"CMAKE_C_FLAGS"</span><span class="p">:</span><span class="w"> </span><span class="s2">"-Wall -Wextra -pedantic"</span><span class="p">,</span>
<span class="w">        </span><span class="nt">"CMAKE_CXX_FLAGS"</span><span class="p">:</span><span class="w"> </span><span class="s2">"-Wall -Wextra -pedantic"</span><span class="p">,</span>
<span class="w">        </span><span class="nt">"CMAKE_HIP_FLAGS"</span><span class="p">:</span><span class="w"> </span><span class="s2">"-Wall -Wextra -pedantic"</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ninja-mc-rocm"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"displayName"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Ninja Multi-Config ROCm"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"inherits"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="s2">"layout"</span><span class="p">,</span>
<span class="w">        </span><span class="s2">"generator-ninja-multi-config"</span><span class="p">,</span>
<span class="w">        </span><span class="s2">"toolchain-makefiles-c/c++-amdclang"</span><span class="p">,</span>
<span class="w">        </span><span class="s2">"clang-strict-iso-high-warn"</span>
<span class="w">      </span><span class="p">]</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">],</span>
<span class="w">  </span><span class="nt">"buildPresets"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ninja-mc-rocm-debug"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"displayName"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Debug"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"configuration"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Debug"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"configurePreset"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ninja-mc-rocm"</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ninja-mc-rocm-release"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"displayName"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Release"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"configuration"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Release"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"configurePreset"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ninja-mc-rocm"</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ninja-mc-rocm-debug-verbose"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"displayName"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Debug (verbose)"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"configuration"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Debug"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"configurePreset"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ninja-mc-rocm"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"verbose"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ninja-mc-rocm-release-verbose"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"displayName"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Release (verbose)"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"configuration"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Release"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"configurePreset"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ninja-mc-rocm"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"verbose"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">],</span>
<span class="w">  </span><span class="nt">"testPresets"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ninja-mc-rocm-debug"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"displayName"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Debug"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"configuration"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Debug"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"configurePreset"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ninja-mc-rocm"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"execution"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">"jobs"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ninja-mc-rocm-release"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"displayName"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Release"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"configuration"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Release"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"configurePreset"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ninja-mc-rocm"</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"execution"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">"jobs"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Getting presets to work reliably on Windows requires some CMake improvements
and/or support from compiler vendors. (Refer to
<a class="reference external" href="https://gitlab.kitware.com/cmake/cmake/-/issues/24245">Add support to the Visual Studio generators</a>
and <a class="reference external" href="https://gitlab.kitware.com/cmake/cmake/-/issues/21619">Sourcing environment scripts</a>
.)</p>
</div>
</section>
</section>
</section>
<span id="document-conceptual/ai-pytorch-inception"></span><head>
<meta charset="utf-8"/>
<meta content="Inception V3 with PyTorch" name="description"/>
<meta content="PyTorch, Inception V3, deep-learning, training data, optimization
  algorithm, AMD, ROCm" name="keywords"/>
</head>
<section class="tex2jax_ignore mathjax_ignore" id="deep-learning-inception-v3-with-pytorch">
<h2>Deep learning: Inception V3 with PyTorch<a class="headerlink" href="#deep-learning-inception-v3-with-pytorch" title="Link to this heading">#</a></h2>
<section id="deep-learning-training">
<h3>Deep learning training<a class="headerlink" href="#deep-learning-training" title="Link to this heading">#</a></h3>
<p>Deep-learning models are designed to capture the complexity of the problem and the underlying data. These models are “deep,” comprising multiple component layers. Training is finding the best parameters for each model layer to achieve a well-defined objective.</p>
<p>The training data consists of input features in supervised learning, similar to what the learned model is expected to see during the evaluation or inference phase. The target output is also included, which serves to teach the model. A loss metric is defined as part of training that evaluates the model’s performance during the training process.</p>
<p>Training also includes the choice of an optimization algorithm that reduces the loss by adjusting the model’s parameters. Training is an iterative process where training data is fed in, usually split into different batches, with the entirety of the training data passed during one training epoch. Training usually is run for multiple epochs.</p>
</section>
<section id="training-phases">
<h3>Training phases<a class="headerlink" href="#training-phases" title="Link to this heading">#</a></h3>
<p>Training occurs in multiple phases for every batch of training data. the following table provides an explanation of the types of training phases.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1">
<caption><span class="caption-text">Types of Training Phases</span><a class="headerlink" href="#id1" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Types of Phases</p></th>
<th class="head"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Forward Pass</p></td>
<td><p>The input features are fed into the model, whose parameters may be randomly initialized initially. Activations (outputs) of each layer are retained during this pass to help in the loss gradient computation during the backward pass.</p></td>
</tr>
<tr class="row-odd"><td><p>Loss Computation</p></td>
<td><p>The output is compared against the target outputs, and the loss is computed.</p></td>
</tr>
<tr class="row-even"><td><p>Backward Pass</p></td>
<td><p>The loss is propagated backward, and the model’s error gradients are computed and stored for each trainable parameter.</p></td>
</tr>
<tr class="row-odd"><td><p>Optimization Pass</p></td>
<td><p>The optimization algorithm updates the model parameters using the stored error gradients.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Training is different from inference, particularly from the hardware perspective. The following table shows the contrast between training and inference.</p>
<div class="pst-scrollable-table-container"><table class="table" id="training-inference">
<caption><span class="caption-text">Training vs. Inference</span><a class="headerlink" href="#training-inference" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Training</p></th>
<th class="head"><p>Inference</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Training is measured in hours/days.</p></td>
<td><p>The inference is measured in minutes.</p></td>
</tr>
<tr class="row-odd"><td><p>Training is generally run offline in a data center or cloud setting.</p></td>
<td><p>The inference is made on edge devices.</p></td>
</tr>
<tr class="row-even"><td><p>The memory requirements for training are higher than inference due to storing intermediate data, such as activations and error gradients.</p></td>
<td><p>The memory requirements are lower for inference than training.</p></td>
</tr>
<tr class="row-odd"><td><p>Data for training is available on the disk before the training process and is generally significant. The training performance is measured by how fast the data batches can be processed.</p></td>
<td><p>Inference data usually arrive stochastically, which may be batched to improve performance. Inference performance is generally measured in throughput speed to process the batch of data and the delay in responding to the input (latency).</p></td>
</tr>
</tbody>
</table>
</div>
<p>Different quantization data types are typically chosen between training (FP32, BF16) and inference (FP16, INT8). The computation hardware has different specializations from other data types, leading to improvement in performance if a faster datatype can be selected for the corresponding task.</p>
</section>
<section id="case-studies">
<h3>Case studies<a class="headerlink" href="#case-studies" title="Link to this heading">#</a></h3>
<p>The following sections contain case studies for the Inception V3 model.</p>
<section id="inception-v3-with-pytorch">
<h4>Inception V3 with PyTorch<a class="headerlink" href="#inception-v3-with-pytorch" title="Link to this heading">#</a></h4>
<p>Convolution Neural Networks are forms of artificial neural networks commonly used for image processing. One of the core layers of such a network is the convolutional layer, which convolves the input with a weight tensor and passes the result to the next layer. Inception V3 is an architectural development over the ImageNet competition-winning entry, AlexNet, using more profound and broader networks while attempting to meet computational and memory budgets.</p>
<p>The implementation uses PyTorch as a framework. This case study utilizes <a class="reference external" href="https://pytorch.org/vision/stable/index.html">TorchVision</a>, a repository of popular datasets and model architectures, for obtaining the model. TorchVision also provides pre-trained weights as a starting point to develop new models or fine-tune the model for a new task.</p>
<section id="evaluating-a-pre-trained-model">
<h5>Evaluating a pre-trained model<a class="headerlink" href="#evaluating-a-pre-trained-model" title="Link to this heading">#</a></h5>
<p>The Inception V3 model introduces a simple image classification task with the pre-trained model. This does not involve training but utilizes an already pre-trained model from TorchVision.</p>
<p>This example is adapted from the PyTorch research hub page on <a class="reference external" href="https://pytorch.org/vision/master/models/inception.html">Inception V3</a>.</p>
<p>Follow these steps:</p>
<ol class="arabic">
<li><p>Run the PyTorch ROCm-based Docker image or refer to the section <a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/pytorch-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">Installing PyTorch</span></a> for setting up a PyTorch environment on ROCm.</p>
<div class="highlight-dockerfile notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>-v<span class="w"> </span><span class="nv">$HOME</span>:/data<span class="w"> </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span>--group-add<span class="w"> </span>video<span class="w"> </span>--ipc<span class="o">=</span>host<span class="w"> </span>--shm-size<span class="w"> </span>8G<span class="w"> </span>rocm/pytorch:latest
</pre></div>
</div>
</li>
<li><p>Run the Python shell and import packages and libraries for model creation.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
</pre></div>
</div>
</li>
<li><p>Set the model in evaluation mode. Evaluation mode directs PyTorch not to store intermediate data, which would have been used in training.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">'pytorch/vision:v0.10.0'</span><span class="p">,</span> <span class="s1">'inception_v3'</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
</li>
<li><p>Download a sample image for inference.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">urllib</span>
<span class="n">url</span><span class="p">,</span> <span class="n">filename</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"https://github.com/pytorch/hub/raw/master/images/dog.jpg"</span><span class="p">,</span> <span class="s2">"dog.jpg"</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span> <span class="n">urllib</span><span class="o">.</span><span class="n">URLopener</span><span class="p">()</span><span class="o">.</span><span class="n">retrieve</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span> <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Import torchvision and PILImage support libraries.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">transforms</span>
<span class="n">input_image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Apply preprocessing and normalization.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">preprocess</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">299</span><span class="p">),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">299</span><span class="p">),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]),</span>
<span class="p">])</span>
</pre></div>
</div>
</li>
<li><p>Use input tensors and unsqueeze them later.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">input_image</span><span class="p">)</span>
<span class="n">input_batch</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">input_batch</span> <span class="o">=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">'cuda'</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">'cuda'</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Find out probabilities.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>To understand the probabilities, download and examine the ImageNet labels.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">raw</span><span class="o">.</span><span class="n">githubusercontent</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">hub</span><span class="o">/</span><span class="n">master</span><span class="o">/</span><span class="n">imagenet_classes</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</li>
<li><p>Read the categories and show the top categories for the image.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"imagenet_classes.txt"</span><span class="p">,</span> <span class="s2">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">categories</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()]</span>
<span class="n">top5_prob</span><span class="p">,</span> <span class="n">top5_catid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">top5_prob</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">categories</span><span class="p">[</span><span class="n">top5_catid</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">top5_prob</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="training-inception-v3">
<h5>Training Inception V3<a class="headerlink" href="#training-inception-v3" title="Link to this heading">#</a></h5>
<p>The previous section focused on downloading and using the Inception V3 model for a simple image classification task. This section walks through training the model on a new dataset.</p>
<p>Follow these steps:</p>
<ol class="arabic">
<li><p>Run the PyTorch ROCm Docker image or refer to the section <a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/pytorch-install.html" title="(in ROCm installation on Linux v6.4.2)"><span class="xref std std-doc">Installing PyTorch</span></a> for setting up a PyTorch environment on ROCm.</p>
<div class="highlight-dockerfile notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>rocm/pytorch:latest
docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span>--group-add<span class="w"> </span>video<span class="w"> </span>--ipc<span class="o">=</span>host<span class="w"> </span>--shm-size<span class="w"> </span>8G<span class="w"> </span>rocm/pytorch:latest
</pre></div>
</div>
</li>
<li><p>Download an ImageNet database. For this example, the <code class="docutils literal notranslate"><span class="pre">tiny-imagenet-200</span></code>, a smaller ImageNet variant with 200 image classes and a training dataset with 100,000 images, was downsized to 64x64 color images.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>http://cs231n.stanford.edu/tiny-imagenet-200.zip
</pre></div>
</div>
</li>
<li><p>Process the database to set the validation directory to the format expected by PyTorch’s <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>.</p></li>
<li><p>Run the following script:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">io</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">glob</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">shutil</span><span class="w"> </span><span class="kn">import</span> <span class="n">move</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">os.path</span><span class="w"> </span><span class="kn">import</span> <span class="n">join</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">os</span><span class="w"> </span><span class="kn">import</span> <span class="n">listdir</span><span class="p">,</span> <span class="n">rmdir</span>
<span class="n">target_folder</span> <span class="o">=</span> <span class="s1">'./tiny-imagenet-200/val/'</span>
<span class="n">val_dict</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">'./tiny-imagenet-200/val/val_annotations.txt'</span><span class="p">,</span> <span class="s1">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">():</span>
        <span class="n">split_line</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'</span><span class="se">\t</span><span class="s1">'</span><span class="p">)</span>
        <span class="n">val_dict</span><span class="p">[</span><span class="n">split_line</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">split_line</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="n">paths</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">'./tiny-imagenet-200/val/images/*'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">paths</span><span class="p">:</span>
    <span class="n">file</span> <span class="o">=</span> <span class="n">path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'/'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">folder</span> <span class="o">=</span> <span class="n">val_dict</span><span class="p">[</span><span class="n">file</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">target_folder</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">folder</span><span class="p">)):</span>
        <span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">target_folder</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">folder</span><span class="p">))</span>
        <span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">target_folder</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">folder</span><span class="p">)</span> <span class="o">+</span> <span class="s1">'/images'</span><span class="p">)</span>

<span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">paths</span><span class="p">:</span>
    <span class="n">file</span> <span class="o">=</span> <span class="n">path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'/'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">folder</span> <span class="o">=</span> <span class="n">val_dict</span><span class="p">[</span><span class="n">file</span><span class="p">]</span>
    <span class="n">dest</span> <span class="o">=</span> <span class="n">target_folder</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">folder</span><span class="p">)</span> <span class="o">+</span> <span class="s1">'/images/'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
    <span class="n">move</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">dest</span><span class="p">)</span>

<span class="n">rmdir</span><span class="p">(</span><span class="s1">'./tiny-imagenet-200/val/images'</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Open a Python shell.</p></li>
<li><p>Import dependencies, including Torch, OS, and <a class="reference external" href="https://github.com/pytorch/vision">TorchVision</a>.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.transforms.functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">InterpolationMode</span>
</pre></div>
</div>
</li>
<li><p>Set parameters to guide the training process.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The device is set to <code class="docutils literal notranslate"><span class="pre">"cuda"</span></code>. In PyTorch, <code class="docutils literal notranslate"><span class="pre">"cuda"</span></code> is a generic keyword to denote a GPU.</p>
</div>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span>
</pre></div>
</div>
</li>
<li><p>Set the data_path to the location of the training and validation data. In this case, the <code class="docutils literal notranslate"><span class="pre">tiny-imagenet-200</span></code> is present as a subdirectory to the current directory.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">data_path</span> <span class="o">=</span> <span class="s2">"tiny-imagenet-200"</span>
</pre></div>
</div>
<p>The training image size is cropped for input into Inception V3.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">train_crop_size</span> <span class="o">=</span> <span class="mi">299</span>
</pre></div>
</div>
</li>
<li><p>To smooth the image, use bilinear interpolation, a resampling method that uses the distance weighted average of the four nearest pixel values to estimate a new pixel value.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">interpolation</span> <span class="o">=</span> <span class="s2">"bilinear"</span>
</pre></div>
</div>
<p>The next parameters control the size to which the validation image is cropped and resized.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">val_crop_size</span> <span class="o">=</span> <span class="mi">299</span>
<span class="n">val_resize_size</span> <span class="o">=</span> <span class="mi">342</span>
</pre></div>
</div>
<p>The pre-trained Inception V3 model is chosen to be downloaded from torchvision.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s2">"inception_v3"</span>
<span class="n">pretrained</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
<p>During each training step, a batch of images is processed to compute the loss gradient and perform the optimization. In the following setting, the size of the batch is determined.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
</pre></div>
</div>
<p>This refers to the number of CPU threads the data loader uses to perform efficient multi-process data loading.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">num_workers</span> <span class="o">=</span> <span class="mi">16</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">torch.optim</span></code> package provides methods to adjust the learning rate as the training progresses. This example uses the <code class="docutils literal notranslate"><span class="pre">StepLR</span></code> scheduler, which decays the learning rate by <code class="docutils literal notranslate"><span class="pre">lr_gamma</span></code> at every <code class="docutils literal notranslate"><span class="pre">lr_step_size</span></code> number of epochs.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">1e-4</span>
<span class="n">lr_step_size</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">lr_gamma</span> <span class="o">=</span> <span class="mf">0.1</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One training epoch is when the neural network passes an entire dataset forward and backward.</p>
</div>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">90</span>
</pre></div>
</div>
<p>The train and validation directories are determined.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">train_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">"train"</span><span class="p">)</span>
<span class="n">val_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">"val"</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Set up the training and testing data loaders.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">interpolation</span> <span class="o">=</span> <span class="n">InterpolationMode</span><span class="p">(</span><span class="n">interpolation</span><span class="p">)</span>

<span class="n">TRAIN_TRANSFORM_IMG</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
<span class="n">Normalizaing</span> <span class="ow">and</span> <span class="n">standardardizing</span> <span class="n">the</span> <span class="n">image</span>
<span class="n">transforms</span><span class="o">.</span><span class="n">RandomResizedCrop</span><span class="p">(</span><span class="n">train_crop_size</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">interpolation</span><span class="p">),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">PILToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ConvertImageDtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                        <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]</span> <span class="p">)</span>
    <span class="p">])</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span>
    <span class="n">train_dir</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">TRAIN_TRANSFORM_IMG</span>
<span class="p">)</span>
<span class="n">TEST_TRANSFORM_IMG</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="n">val_resize_size</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">interpolation</span><span class="p">),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="n">val_crop_size</span><span class="p">),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">PILToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ConvertImageDtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                        <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]</span> <span class="p">)</span>
    <span class="p">])</span>

<span class="n">dataset_test</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span>
    <span class="n">val_dir</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">TEST_TRANSFORM_IMG</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Creating data loaders"</span><span class="p">)</span>
<span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">RandomSampler</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">test_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">SequentialSampler</span><span class="p">(</span><span class="n">dataset_test</span><span class="p">)</span>

<span class="n">data_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
    <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="n">data_loader_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">dataset_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">test_sampler</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Use torchvision to obtain the Inception V3 model. Use the pre-trained model weights to speed up training.</p>
</div>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Creating model"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Num classes = "</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">classes</span><span class="p">))</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="n">model_name</span><span class="p">](</span><span class="n">pretrained</span><span class="o">=</span><span class="n">pretrained</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Adapt Inception V3 for the current dataset. <code class="docutils literal notranslate"><span class="pre">tiny-imagenet-200</span></code> contains only 200 classes, whereas Inception V3 is designed for 1,000-class output. The last layer of Inception V3 is replaced to match the output features required.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">classes</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">aux_logits</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">model</span><span class="o">.</span><span class="n">AuxLogits</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
</li>
<li><p>Move the model to the GPU device.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Set the loss criteria. For this example, Cross Entropy Loss is used.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</pre></div>
</div>
</li>
<li><p>Set the optimizer to Stochastic Gradient Descent.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
    <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span>
<span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Set the learning rate scheduler.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="n">lr_step_size</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">lr_gamma</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Iterate over epochs. Each epoch is a complete pass through the training data.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Start training"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">len_dataset</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</li>
<li><p>Iterate over steps. The data is processed in batches, and each step passes through a full batch.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
</pre></div>
</div>
</li>
<li><p>Pass the image and target to the GPU device.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">image</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>The following is the core training logic:</p>
<p>a. The image is fed into the model.</p>
<p>b. The output is compared with the target in the training data to obtain the loss.</p>
<p>c. This loss is back propagated to all parameters that require optimization.</p>
<p>d. The optimizer updates the parameters based on the selected optimization algorithm.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span>        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>The epoch loss is updated, and the step loss prints.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span>        <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">len_dataset</span> <span class="o">+=</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'Epoch: '</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="s1">'| step : </span><span class="si">%d</span><span class="s1">'</span> <span class="o">%</span> <span class="n">step</span><span class="p">,</span> <span class="s1">'| train loss : </span><span class="si">%0.4f</span><span class="s1">'</span> <span class="o">%</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="p">)</span>
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="n">len_dataset</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Epoch: '</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="s1">'| train loss :  </span><span class="si">%0.4f</span><span class="s1">'</span> <span class="o">%</span> <span class="n">epoch_loss</span> <span class="p">)</span>
</pre></div>
</div>
<p>The learning rate is updated at the end of each epoch.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>After training for the epoch, the model evaluates against the validation dataset.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
        <span class="n">running_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader_test</span><span class="p">):</span>
            <span class="n">image</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

            <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="n">running_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader_test</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Epoch: '</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="s1">'| test loss : </span><span class="si">%0.4f</span><span class="s1">'</span> <span class="o">%</span> <span class="n">running_loss</span> <span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Save the model for use in inferencing tasks.</p></li>
</ol>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="c1"># save model</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">"trained_inception_v3.pt"</span><span class="p">)</span>
</pre></div>
</div>
<p>Plotting the train and test loss shows both metrics reducing over training epochs. This is demonstrated in the following image.</p>
<p><img alt="Inception V3 train and loss graph" src="_images/inception-v3.png"/></p>
</section>
</section>
<section id="custom-model-with-cifar-10-on-pytorch">
<h4>Custom model with CIFAR-10 on PyTorch<a class="headerlink" href="#custom-model-with-cifar-10-on-pytorch" title="Link to this heading">#</a></h4>
<p>The Canadian Institute for Advanced Research (CIFAR)-10 dataset is a subset of the Tiny Images dataset (which contains 80 million images of 32x32 collected from the Internet) and consists of 60,000 32x32 color images. The images are labeled with one of 10 mutually exclusive classes: airplane, motor car, bird, cat, deer, dog, frog, cruise ship, stallion, and truck (but not pickup truck). There are 6,000 images per class, with 5,000 training and 1,000 testing images per class. Let us prepare a custom model for classifying these images using the PyTorch framework and go step-by-step as illustrated below.</p>
<p>Follow these steps:</p>
<ol class="arabic">
<li><p>Import dependencies, including Torch, OS, and <a class="reference external" href="https://github.com/pytorch/vision">TorchVision</a>.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.transforms</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">transforms</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plot</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</pre></div>
</div>
</li>
<li><p>The output of torchvision datasets is <code class="docutils literal notranslate"><span class="pre">PILImage</span></code> images of range [0, 1]. Transform them to Tensors of normalized range [-1, 1].</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span>
        <span class="p">[</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
            <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>
</pre></div>
</div>
<p>During each training step, a batch of images is processed to compute the loss gradient and perform the optimization. In the following setting, the size of the batch is determined.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4</span>
</pre></div>
</div>
</li>
<li><p>Download the dataset train and test datasets as follows. Specify the batch size, shuffle the dataset once, and specify the number of workers to the number of CPU threads used by the data loader to perform efficient multi-process data loading.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">train_set</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Follow the same procedure for the testing set.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">test_set</span> <span class="o">=</span> <span class="n">TorchVision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">"teast set and test loader"</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Specify the defined classes of images belonging to this dataset.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">classes</span> <span class="o">=</span> <span class="p">(</span><span class="s1">'Aeroplane'</span><span class="p">,</span> <span class="s1">'motorcar'</span><span class="p">,</span> <span class="s1">'bird'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">,</span> <span class="s1">'deer'</span><span class="p">,</span> <span class="s1">'puppy'</span><span class="p">,</span> <span class="s1">'frog'</span><span class="p">,</span> <span class="s1">'stallion'</span><span class="p">,</span> <span class="s1">'cruise'</span><span class="p">,</span> <span class="s1">'truck'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"defined classes"</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Denormalize the images and then iterate over them.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="k">global</span> <span class="n">image_number</span>
<span class="n">image_number</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">def</span><span class="w"> </span><span class="nf">show_image</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">image_number</span>
    <span class="n">image_number</span> <span class="o">=</span> <span class="n">image_number</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span>     <span class="c1"># de-normalizing input image</span>
    <span class="n">npimg</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">plot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">npimg</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>
    <span class="n">plot</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">"fig</span><span class="si">{}</span><span class="s2">.jpg"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">image_number</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"fig</span><span class="si">{}</span><span class="s2">.jpg"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">image_number</span><span class="p">))</span>
    <span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">data_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data_iter</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
<span class="n">show_image</span><span class="p">(</span><span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">images</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">'</span><span class="si">%5s</span><span class="s1">'</span> <span class="o">%</span> <span class="n">classes</span><span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">j</span><span class="p">]]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"image created and saved "</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Import the <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> for constructing neural networks and <code class="docutils literal notranslate"><span class="pre">torch.nn.functional</span></code> to use the convolution functions.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
</pre></div>
</div>
</li>
<li><p>Define the CNN (Convolution Neural Networks) and relevant activation functions.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># flatten all dimensions except batch</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"created Net() "</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Set the optimizer to Stochastic Gradient Descent.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
</pre></div>
</div>
</li>
<li><p>Set the loss criteria. For this example, Cross Entropy Loss is used.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Iterate over epochs. Each epoch is a complete pass through the training data.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>  <span class="c1"># loop over the dataset multiple times</span>

    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
        <span class="c1"># get the inputs; data is a list of [inputs, labels]</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>

        <span class="c1"># zero the parameter gradients</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># forward + backward + optimize</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># print statistics</span>
        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2000</span> <span class="o">==</span> <span class="mi">1999</span><span class="p">:</span>    <span class="c1"># print every 2000 mini-batches</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'[</span><span class="si">%d</span><span class="s1">, </span><span class="si">%5d</span><span class="s1">] loss: </span><span class="si">%.3f</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">running_loss</span> <span class="o">/</span> <span class="mi">2000</span><span class="p">))</span>
            <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Finished Training'</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">PATH</span> <span class="o">=</span> <span class="s1">'./cifar_net.pth'</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"saved model to path :"</span><span class="p">,</span><span class="n">PATH</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">net</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"loding back saved model"</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Predicted: '</span><span class="p">,</span> <span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">'</span><span class="si">%5s</span><span class="s1">'</span> <span class="o">%</span> <span class="n">classes</span><span class="p">[</span><span class="n">predicted</span><span class="p">[</span><span class="n">j</span><span class="p">]]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)))</span>
<span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
<p>As this is not training, calculating the gradients for outputs is not required.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="c1"># calculate outputs by running images through the network</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
        <span class="c1"># calculate outputs by running images through the network</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="c1"># the class with the highest energy is what you can choose as prediction</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Accuracy of the network on the 10000 test images: </span><span class="si">%d</span><span class="s1"> </span><span class="si">%%</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span><span class="p">))</span>
<span class="c1"># prepare to count predictions for each class</span>
<span class="n">correct_pred</span> <span class="o">=</span> <span class="p">{</span><span class="n">classname</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">classname</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">}</span>
<span class="n">total_pred</span> <span class="o">=</span> <span class="p">{</span><span class="n">classname</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">classname</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">}</span>
</pre></div>
</div>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="c1"># again no gradients needed</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># collect the correct predictions for each class</span>
        <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">prediction</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">label</span> <span class="o">==</span> <span class="n">prediction</span><span class="p">:</span>
                <span class="n">correct_pred</span><span class="p">[</span><span class="n">classes</span><span class="p">[</span><span class="n">label</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">total_pred</span><span class="p">[</span><span class="n">classes</span><span class="p">[</span><span class="n">label</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="c1"># print accuracy for each class</span>
<span class="k">for</span> <span class="n">classname</span><span class="p">,</span> <span class="n">correct_count</span> <span class="ow">in</span> <span class="n">correct_pred</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="n">correct_count</span><span class="p">)</span> <span class="o">/</span> <span class="n">total_pred</span><span class="p">[</span><span class="n">classname</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy for class </span><span class="si">{:5s}</span><span class="s2"> is: </span><span class="si">{:.1f}</span><span class="s2"> %"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">classname</span><span class="p">,</span><span class="n">accuracy</span><span class="p">))</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="case-study-tensorflow-with-fashion-mnist">
<h4>Case study: TensorFlow with Fashion-MNIST<a class="headerlink" href="#case-study-tensorflow-with-fashion-mnist" title="Link to this heading">#</a></h4>
<p>Fashion-MNIST is a dataset that contains 70,000 grayscale images in 10 categories.</p>
<p>Implement and train a neural network model using the TensorFlow framework to classify images of clothing, like sneakers and shirts.</p>
<p>The dataset has 60,000 images you will use to train the network and 10,000 to evaluate how accurately the network learned to classify images. The Fashion-MNIST dataset can be accessed via TensorFlow internal libraries.</p>
<p>Access the source code from the following repository:</p>
<p><a class="github reference external" href="https://github.com/ROCm/tensorflow_fashionmnist/blob/main/fashion_mnist.py">ROCm/tensorflow_fashionmnist</a></p>
<p>To understand the code step by step, follow these steps:</p>
<ol class="arabic">
<li><p>Import libraries like TensorFlow, NumPy, and Matplotlib to train the neural network and calculate and plot graphs.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
</pre></div>
</div>
</li>
<li><p>To verify that TensorFlow is installed, print the version of TensorFlow by using the below print statement:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">_version__</span><span class="p">)</span> <span class="n">r</span>
</pre></div>
</div>
</li>
<li><p>Load the dataset from the available internal libraries to analyze and train a neural network upon the Fashion-MNIST dataset. Loading the dataset returns four NumPy arrays. The model uses the training set arrays, train_images and train_labels, to learn.</p></li>
<li><p>The model is tested against the test set, test_images, and test_labels arrays.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">fashion_mnist</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">fashion_mnist</span>
<span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">),</span> <span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span> <span class="o">=</span> <span class="n">fashion_mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
</pre></div>
</div>
<p>Since you have 10 types of images in the dataset, assign labels from zero to nine. Each image is assigned one label. The images are 28x28 NumPy arrays, with pixel values ranging from zero to 255.</p>
</li>
<li><p>Each image is mapped to a single label. Since the class names are not included with the dataset, store them, and later use them when plotting the images:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">class_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'T-shirt/top'</span><span class="p">,</span> <span class="s1">'Trouser'</span><span class="p">,</span> <span class="s1">'Pullover'</span><span class="p">,</span> <span class="s1">'Dress'</span><span class="p">,</span> <span class="s1">'Coat'</span><span class="p">,</span><span class="s1">'Sandal'</span><span class="p">,</span> <span class="s1">'Shirt'</span><span class="p">,</span> <span class="s1">'Sneaker'</span><span class="p">,</span> <span class="s1">'Bag'</span><span class="p">,</span> <span class="s1">'Ankle boot'</span><span class="p">]</span>
</pre></div>
</div>
</li>
<li><p>Use this code to explore the dataset by knowing its dimensions:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">train_images</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</li>
<li><p>Use this code to print the size of this training set:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_labels</span><span class="p">))</span>
</pre></div>
</div>
</li>
<li><p>Use this code to print the labels of this training set:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">train_labels</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Preprocess the data before training the network, and you can start inspecting the first image, as its pixels will fall in the range of zero to 255.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">train_images</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt=" " src="_images/mnist-1.png"/></p>
</li>
<li><p>From the above picture, you can see that values are from zero to 255. Before training this on the neural network, you must bring them in the range of zero to one. Hence, divide the values by 255.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">train_images</span> <span class="o">=</span> <span class="n">train_images</span> <span class="o">/</span> <span class="mf">255.0</span>

<span class="n">test_images</span> <span class="o">=</span> <span class="n">test_images</span> <span class="o">/</span> <span class="mf">255.0</span>
</pre></div>
</div>
</li>
<li><p>To ensure the data is in the correct format and ready to build and train the network, display the first 25 images from the training set and the class name below each image.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">25</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">train_images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">binary</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">train_labels</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt=" " src="_images/mnist-2.png"/></p>
<p>The basic building block of a neural network is the layer. Layers extract representations from the data fed into them. Deep learning consists of chaining together simple layers. Most layers, such as <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Dense</span></code>, have parameters that are learned during training.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="p">])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The first layer in this network <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Flatten</span></code> transforms the format of the images from a two-dimensional array (of 28 x 28 pixels) to a one-dimensional array (of 28 * 28 = 784 pixels). Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data.</p></li>
<li><p>After the pixels are flattened, the network consists of a sequence of two <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Dense</span></code> layers. These are densely connected or fully connected neural layers. The first Dense layer has 128 nodes (or neurons). The second (and last) layer returns a logits array with a length of 10. Each node contains a score that indicates the current image belongs to one of the 10 classes.</p></li>
</ul>
</li>
<li><p>You must add the Loss function, Metrics, and Optimizer at the time of model compilation.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Loss function —This measures how accurate the model is during training when you are looking to minimize this function to “steer” the model in the right direction.</p></li>
<li><p>Optimizer —This is how the model is updated based on the data it sees and its loss function.</p></li>
<li><p>Metrics —This is used to monitor the training and testing steps.</p></li>
</ul>
<p>The following example uses accuracy, the fraction of the correctly classified images.</p>
<p>To train the neural network model, follow these steps:</p>
<ol class="arabic">
<li><p>Feed the training data to the model. The training data is in the train_images and train_labels arrays in this example. The model learns to associate images and labels.</p></li>
<li><p>Ask the model to make predictions about a test set—in this example, the test_images array.</p></li>
<li><p>Verify that the predictions match the labels from the test_labels array.</p></li>
<li><p>To start training, call the model.fit method because it “fits” the model to the training data.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Compare how the model will perform on the test dataset.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span>  <span class="n">test_labels</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">Test accuracy:'</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>With the model trained, you can use it to make predictions about some images: the model’s linear outputs and logits. Attach a softmax layer to convert the logits to probabilities, making it easier to interpret.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">probability_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span><span class="n">model</span><span class="p">,</span>
                                        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()])</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">probability_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_images</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>The model has predicted the label for each image in the testing set. Look at the first prediction:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>A prediction is an array of 10 numbers. They represent the model’s “confidence” that the image corresponds to each of the 10 different articles of clothing. You can see which label has the highest confidence value:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</li>
<li><p>Plot a graph to look at the complete set of 10 class predictions.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">plot_image</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">predictions_array</span><span class="p">,</span> <span class="n">true_label</span><span class="p">,</span> <span class="n">img</span><span class="p">):</span>
<span class="n">true_label</span><span class="p">,</span> <span class="n">img</span> <span class="o">=</span> <span class="n">true_label</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">img</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">binary</span><span class="p">)</span>

<span class="n">predicted_label</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions_array</span><span class="p">)</span>
<span class="k">if</span> <span class="n">predicted_label</span> <span class="o">==</span> <span class="n">true_label</span><span class="p">:</span>
    <span class="n">color</span> <span class="o">=</span> <span class="s1">'blue'</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"</span><span class="si">{}</span><span class="s2"> </span><span class="si">{:2.0f}</span><span class="s2">% (</span><span class="si">{}</span><span class="s2">)"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">predicted_label</span><span class="p">],</span>
                                <span class="mi">100</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">predictions_array</span><span class="p">),</span>
                                <span class="n">class_names</span><span class="p">[</span><span class="n">true_label</span><span class="p">]),</span>
                                <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">plot_value_array</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">predictions_array</span><span class="p">,</span> <span class="n">true_label</span><span class="p">):</span>
<span class="n">true_label</span> <span class="o">=</span> <span class="n">true_label</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">thisplot</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">predictions_array</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"#777777"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">predicted_label</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions_array</span><span class="p">)</span>

<span class="n">thisplot</span><span class="p">[</span><span class="n">predicted_label</span><span class="p">]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">'red'</span><span class="p">)</span>
<span class="n">thisplot</span><span class="p">[</span><span class="n">true_label</span><span class="p">]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">'blue'</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>With the model trained, you can use it to make predictions about some images. Review the 0<sup>th</sup> image predictions and the prediction array. Correct prediction labels are blue, and incorrect prediction labels are red. The number gives the percentage (out of 100) for the predicted label.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plot_image</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">test_labels</span><span class="p">,</span> <span class="n">test_images</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plot_value_array</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>  <span class="n">test_labels</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt=" " src="_images/mnist-3.png"/></p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plot_image</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">test_labels</span><span class="p">,</span> <span class="n">test_images</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plot_value_array</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>  <span class="n">test_labels</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt=" " src="_images/mnist-4.png"/></p>
</li>
<li><p>Use the trained model to predict a single image.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="c1"># Grab an image from the test dataset.</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">test_images</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">tf.keras</span></code> models are optimized to make predictions on a batch, or collection, of examples at once. Accordingly, even though you are using a single image, you must add it to a list.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add the image to a batch where it's the only member.</span>
<span class="n">img</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Predict the correct label for this image.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">predictions_single</span> <span class="o">=</span> <span class="n">probability_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">predictions_single</span><span class="p">)</span>

<span class="n">plot_value_array</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">predictions_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">test_labels</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">class_names</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt=" " src="_images/mnist-5.png"/></p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">tf.keras.Model.predict</span></code> returns a list of lists—one for each image in the batch of data. Grab the predictions for our (only) image in the batch.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions_single</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</li>
</ol>
</li>
</ol>
</section>
<section id="case-study-tensorflow-with-text-classification">
<h4>Case study: TensorFlow with text classification<a class="headerlink" href="#case-study-tensorflow-with-text-classification" title="Link to this heading">#</a></h4>
<p>This procedure demonstrates text classification starting from plain text files stored on disk. You will train a binary classifier to perform sentiment analysis on an IMDB dataset. At the end of the notebook, there is an exercise for you to try in which you will train a multi-class classifier to predict the tag for a programming question on Stack Overflow.</p>
<p>Follow these steps:</p>
<ol class="arabic">
<li><p>Import the necessary libraries.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">shutil</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">string</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras</span><span class="w"> </span><span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras</span><span class="w"> </span><span class="kn">import</span> <span class="n">losses</span>
</pre></div>
</div>
</li>
<li><p>Get the data for the text classification, and extract the database from the given link of IMDB.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">url</span> <span class="o">=</span> <span class="s2">"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">get_file</span><span class="p">(</span><span class="s2">"aclImdb_v1"</span><span class="p">,</span> <span class="n">url</span><span class="p">,</span>
                                    <span class="n">untar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="s1">'.'</span><span class="p">,</span>
                                    <span class="n">cache_subdir</span><span class="o">=</span><span class="s1">''</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Downloading<span class="w"> </span>data<span class="w"> </span>from<span class="w"> </span>https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
<span class="m">84131840</span>/84125825<span class="w"> </span><span class="o">[==============================]</span><span class="w"> </span>–<span class="w"> </span>1s<span class="w"> </span>0us/step
<span class="m">84149932</span>/84125825<span class="w"> </span><span class="o">[==============================]</span><span class="w"> </span>–<span class="w"> </span>1s<span class="w"> </span>0us/step
</pre></div>
</div>
</li>
<li><p>Fetch the data from the directory.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">dataset_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">dataset</span><span class="p">),</span> <span class="s1">'aclImdb'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">dataset_dir</span><span class="p">))</span>
</pre></div>
</div>
</li>
<li><p>Load the data for training purposes.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">train_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dataset_dir</span><span class="p">,</span> <span class="s1">'train'</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">train_dir</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">'labeledBow.feat'</span><span class="p">,</span>
<span class="s1">'urls_pos.txt'</span><span class="p">,</span>
<span class="s1">'urls_unsup.txt'</span><span class="p">,</span>
<span class="s1">'unsup'</span><span class="p">,</span>
<span class="s1">'pos'</span><span class="p">,</span>
<span class="s1">'unsupBow.feat'</span><span class="p">,</span>
<span class="s1">'urls_neg.txt'</span><span class="p">,</span>
<span class="s1">'neg'</span><span class="p">]</span>
</pre></div>
</div>
</li>
<li><p>The directories contain many text files, each of which is a single movie review. To look at one of them, use the following:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">sample_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">train_dir</span><span class="p">,</span> <span class="s1">'pos/1181_9.txt'</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">sample_file</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
</pre></div>
</div>
</li>
<li><p>As the IMDB dataset contains additional folders, remove them before using this utility.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">remove_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">train_dir</span><span class="p">,</span> <span class="s1">'unsup'</span><span class="p">)</span>
<span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">remove_dir</span><span class="p">)</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>
</pre></div>
</div>
</li>
<li><p>The IMDB dataset has already been divided into train and test but lacks a validation set. Create a validation set using an 80:20 split of the training data by using the validation_split argument below:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">raw_train_ds</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">text_dataset_from_directory</span><span class="p">(</span><span class="s1">'aclImdb/train'</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">subset</span><span class="o">=</span><span class="s1">'training'</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>As you will see in a moment, you can train a model by passing a dataset directly to <code class="docutils literal notranslate"><span class="pre">model.fit</span></code>. If you are new to <code class="docutils literal notranslate"><span class="pre">tf.data</span></code>, you can also iterate over the dataset and print a few examples as follows:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">text_batch</span><span class="p">,</span> <span class="n">label_batch</span> <span class="ow">in</span> <span class="n">raw_train_ds</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Review"</span><span class="p">,</span> <span class="n">text_batch</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="n">i</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Label"</span><span class="p">,</span> <span class="n">label_batch</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="n">i</span><span class="p">])</span>
</pre></div>
</div>
</li>
<li><p>The labels are zero or one. To see which of these correspond to positive and negative movie reviews, check the class_names property on the dataset.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Label 0 corresponds to"</span><span class="p">,</span> <span class="n">raw_train_ds</span><span class="o">.</span><span class="n">class_names</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Label 1 corresponds to"</span><span class="p">,</span> <span class="n">raw_train_ds</span><span class="o">.</span><span class="n">class_names</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</li>
<li><p>Next, create validation and test the dataset. Use the remaining 5,000 reviews from the training set for validation into two classes of 2,500 reviews each.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">raw_val_ds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">text_dataset_from_directory</span><span class="p">(</span><span class="s1">'aclImdb/train'</span><span class="p">,</span>
<span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">subset</span><span class="o">=</span><span class="s1">'validation'</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

<span class="n">raw_test_ds</span> <span class="o">=</span>
<span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">text_dataset_from_directory</span><span class="p">(</span>
    <span class="s1">'aclImdb/test'</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
<p>To prepare the data for training, follow these steps:</p>
<ol class="arabic">
<li><p>Standardize, tokenize, and vectorize the data using the helpful <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.TextVectorization</span></code> layer.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">custom_standardization</span><span class="p">(</span><span class="n">input_data</span><span class="p">):</span>
<span class="n">lowercase</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
<span class="n">stripped_html</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">regex_replace</span><span class="p">(</span><span class="n">lowercase</span><span class="p">,</span> <span class="s1">'&lt;br/&gt;'</span><span class="p">,</span> <span class="s1">' '</span><span class="p">)</span>
<span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">regex_replace</span><span class="p">(</span><span class="n">stripped_html</span><span class="p">,</span>                                 <span class="s1">'[</span><span class="si">%s</span><span class="s1">]'</span> <span class="o">%</span> <span class="n">re</span><span class="o">.</span><span class="n">escape</span><span class="p">(</span><span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">),</span><span class="s1">''</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> layer. Use this layer to standardize, tokenize, and vectorize our data. Set the output_mode to int to create unique integer indices for each token. Note that we are using the default split function and the custom standardization function you defined above. You will also define some constants for the model, like an explicit maximum sequence_length, which will cause the layer to pad or truncate sequences to exactly sequence_length values.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">max_features</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">250</span>
<span class="n">vectorize_layer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">TextVectorization</span><span class="p">(</span>
    <span class="n">standardize</span><span class="o">=</span><span class="n">custom_standardization</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span>
    <span class="n">output_mode</span><span class="o">=</span><span class="s1">'int'</span><span class="p">,</span>
    <span class="n">output_sequence_length</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Call adapt to fit the state of the preprocessing layer to the dataset. This causes the model to build an index of strings to integers.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make a text-only dataset (without labels), then call adapt</span>
<span class="n">train_text</span> <span class="o">=</span> <span class="n">raw_train_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span>
<span class="n">vectorize_layer</span><span class="o">.</span><span class="n">adapt</span><span class="p">(</span><span class="n">train_text</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Create a function to see the result of using this layer to preprocess some data.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">vectorize_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">return</span> <span class="n">vectorize_layer</span><span class="p">(</span><span class="n">text</span><span class="p">),</span> <span class="n">label</span>

<span class="n">text_batch</span><span class="p">,</span> <span class="n">label_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">raw_train_ds</span><span class="p">))</span>
<span class="n">first_review</span><span class="p">,</span> <span class="n">first_label</span> <span class="o">=</span> <span class="n">text_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Review"</span><span class="p">,</span> <span class="n">first_review</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Label"</span><span class="p">,</span> <span class="n">raw_train_ds</span><span class="o">.</span><span class="n">class_names</span><span class="p">[</span><span class="n">first_label</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Vectorized review"</span><span class="p">,</span> <span class="n">vectorize_text</span><span class="p">(</span><span class="n">first_review</span><span class="p">,</span> <span class="n">first_label</span><span class="p">))</span>
</pre></div>
</div>
<p><img alt=" " src="_images/TextClassification-3.png"/></p>
</li>
<li><p>As you can see above, each token has been replaced by an integer. Look up the token (string) that each integer corresponds to by calling get_vocabulary() on the layer.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"1287 ---&gt; "</span><span class="p">,</span><span class="n">vectorize_layer</span><span class="o">.</span><span class="n">get_vocabulary</span><span class="p">()[</span><span class="mi">1287</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">" 313 ---&gt; "</span><span class="p">,</span><span class="n">vectorize_layer</span><span class="o">.</span><span class="n">get_vocabulary</span><span class="p">()[</span><span class="mi">313</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Vocabulary size: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vectorize_layer</span><span class="o">.</span><span class="n">get_vocabulary</span><span class="p">())))</span>
</pre></div>
</div>
</li>
<li><p>You are nearly ready to train your model. As a final preprocessing step, apply the <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> layer we created earlier to train, validate, and test the dataset.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">train_ds</span> <span class="o">=</span> <span class="n">raw_train_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">vectorize_text</span><span class="p">)</span>
<span class="n">val_ds</span> <span class="o">=</span> <span class="n">raw_val_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">vectorize_text</span><span class="p">)</span>
<span class="n">test_ds</span> <span class="o">=</span> <span class="n">raw_test_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">vectorize_text</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">cache()</span></code> function keeps data in memory after it is loaded off disk. This ensures the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache, which is more efficient to read than many small files.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">prefetch()</span></code> function overlaps data preprocessing and model execution while training.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">AUTOTUNE</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">AUTOTUNE</span>

<span class="n">train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">AUTOTUNE</span><span class="p">)</span>
<span class="n">val_ds</span> <span class="o">=</span> <span class="n">val_ds</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">AUTOTUNE</span><span class="p">)</span>
<span class="n">test_ds</span> <span class="o">=</span> <span class="n">test_ds</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">AUTOTUNE</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Create your neural network.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_features</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">),</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span><span class="n">layers</span><span class="o">.</span><span class="n">GlobalAveragePooling1D</span><span class="p">(),</span>
<span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)])</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt=" " src="_images/TextClassification-4.png"/></p>
</li>
<li><p>A model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), use <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy"><code class="docutils literal notranslate"><span class="pre">losses.BinaryCrossentropy</span></code></a> loss function.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
<span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span><span class="n">metrics</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">BinaryAccuracy</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">0.0</span><span class="p">))</span>
</pre></div>
</div>
</li>
<li><p>Train the model by passing the dataset object to the fit method.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="n">val_ds</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt=" " src="_images/TextClassification-5.png"/></p>
</li>
<li><p>See how the model performs. Two values are returned: loss (a number representing our error; lower values are better) and accuracy.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_ds</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Loss: "</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: "</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">model.fit()</span></code> returns a History object that contains a dictionary with everything that happened during
training.</p>
</div>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">history_dict</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span>
<span class="n">history_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
</li>
<li><p>Four entries are for each monitored metric during training and validation. Use these to plot the training and validation loss for comparison, as well as the training and validation accuracy:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">acc</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s1">'binary_accuracy'</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s1">'val_binary_accuracy'</span><span class="p">]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s1">'loss'</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s1">'val_loss'</span><span class="p">]</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># "bo" is for "blue dot"</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">'bo'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Training loss'</span><span class="p">)</span>
<span class="c1"># b is for "solid blue line"</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s1">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Training and validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Epochs'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>The following images illustrate the training and validation loss and the training and validation accuracy.</p>
<p><img alt="Training and validation loss" src="_images/TextClassification-6.png"/></p>
<p><img alt="Training and validation accuracy" src="_images/TextClassification-7.png"/></p>
</li>
<li><p>Export the model.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">export_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
<span class="n">vectorize_layer</span><span class="p">,</span>
<span class="n">model</span><span class="p">,</span>
<span class="n">layers</span><span class="o">.</span><span class="n">Activation</span><span class="p">(</span><span class="s1">'sigmoid'</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">export_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">optimizer</span><span class="o">=</span><span class="s2">"adam"</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># Test it with `raw_test_ds`, which yields raw strings</span>
<span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="n">export_model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">raw_test_ds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>To get predictions for new examples, call model.predict().</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">examples</span> <span class="o">=</span> <span class="p">[</span>
<span class="s2">"The movie was great!"</span><span class="p">,</span>
<span class="s2">"The movie was okay."</span><span class="p">,</span>
<span class="s2">"The movie was terrible..."</span>
<span class="p">]</span>

<span class="n">export_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-reference/api-libraries"></span><head>
<meta charset="utf-8"/>
<meta content="ROCm API libraries &amp; tools" name="description"/>
<meta content="ROCm, API, libraries, tools, artificial intelligence, development,
  Communications, C++ primitives, Fast Fourier transforms, FFTs, random number generators, linear
  algebra, AMD" name="keywords"/>
</head>
<section class="tex2jax_ignore mathjax_ignore" id="rocm-libraries">
<h2>ROCm libraries<a class="headerlink" href="#rocm-libraries" title="Link to this heading">#</a></h2>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 rocm-doc-grid docutils">
<div class="sd-row sd-row-cols-1 sd-row-cols-xs-1 sd-row-cols-sm-2 sd-row-cols-md-2 sd-row-cols-lg-2 sd-g-3 sd-g-xs-3 sd-g-sm-3 sd-g-md-3 sd-g-lg-3 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body rocm-card-banner rocm-hue-3 docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
Machine Learning and Computer Vision</div>
<ul class="simple" id="artificial-intelligence-apis">
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/composable_kernel/en/latest/index.html" title="(in Composable Kernel Documentation v1.1.0)"><span class="xref std std-doc">Composable Kernel</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/AMDMIGraphX/en/latest/index.html" title="(in MIGraphX v2.12.0)"><span class="xref std std-doc">MIGraphX</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/MIOpen/en/latest/index.html" title="(in MIOpen Documentation v3.4.0)"><span class="xref std std-doc">MIOpen</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/MIVisionX/en/latest/index.html" title="(in MIVisionX Documentation v3.2.0)"><span class="xref std std-doc">MIVisionX</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocAL/en/latest/index.html" title="(in rocAL Documentation v2.2.0)"><span class="xref std std-doc">rocAL</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocDecode/en/latest/index.html" title="(in rocDecode documentation v0.10.0)"><span class="xref std std-doc">rocDecode</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocPyDecode/en/latest/index.html" title="(in rocPyDecode v0.3.1)"><span class="xref std std-doc">rocPyDecode</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocJPEG/en/latest/index.html" title="(in rocJPEG Documentation v0.8.0)"><span class="xref std std-doc">rocJPEG</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rpp/en/latest/index.html" title="(in RPP documentation v1.9.10)"><span class="xref std std-doc">ROCm Performance Primitives (RPP)</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body rocm-card-banner rocm-hue-12 docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
Primitives</div>
<ul class="simple" id="cpp-primitives">
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipCUB/en/latest/index.html" title="(in hipCUB Documentation v3.4.0)"><span class="xref std std-doc">hipCUB</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipTensor/en/latest/index.html" title="(in hipTensor Documentation v1.5.0)"><span class="xref std std-doc">hipTensor</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocPRIM/en/latest/index.html" title="(in rocPRIM Documentation v3.4.1)"><span class="xref std std-doc">rocPRIM</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocThrust/en/latest/index.html" title="(in rocThrust Documentation v3.3.0)"><span class="xref std std-doc">rocThrust</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body rocm-card-banner rocm-hue-7 docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
Communication</div>
<ul class="simple" id="communication-libraries">
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rccl/en/latest/index.html" title="(in RCCL Documentation v2.22.3)"><span class="xref std std-doc">RCCL</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocSHMEM/en/latest/index.html" title="(in rocSHMEM v2.0.1)"><span class="xref std std-doc">rocSHMEM</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body rocm-card-banner rocm-hue-6 docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
Math</div>
<ul class="simple" id="math-apis">
<li><p class="sd-card-text"><a class="reference external" href="https://github.com/ROCm/half">half</a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipBLAS/en/latest/index.html" title="(in hipBLAS Documentation v2.4.0)"><span class="xref std std-doc">hipBLAS</span></a> / <a class="reference external" href="https://rocm.docs.amd.com/projects/rocBLAS/en/latest/index.html" title="(in rocBLAS Documentation v4.4.1)"><span class="xref std std-doc">rocBLAS</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipBLASLt/en/latest/index.html" title="(in hipBLASLt Documentation v0.12.1)"><span class="xref std std-doc">hipBLASLt</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipFFT/en/latest/index.html" title="(in hipFFT Documentation v1.0.18)"><span class="xref std std-doc">hipFFT</span></a> / <a class="reference external" href="https://rocm.docs.amd.com/projects/rocFFT/en/latest/index.html" title="(in rocFFT Documentation v1.0.32)"><span class="xref std std-doc">rocFFT</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipfort/en/latest/index.html" title="(in hipfort Documentation v0.6.0)"><span class="xref std std-doc">hipfort</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipRAND/en/latest/index.html" title="(in hipRAND Documentation v2.12.0)"><span class="xref std std-doc">hipRAND</span></a> / <a class="reference external" href="https://rocm.docs.amd.com/projects/rocRAND/en/latest/index.html" title="(in rocRAND Documentation v3.3.0)"><span class="xref std std-doc">rocRAND</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipSOLVER/en/latest/index.html" title="(in hipSOLVER Documentation v2.4.0)"><span class="xref std std-doc">hipSOLVER</span></a> / <a class="reference external" href="https://rocm.docs.amd.com/projects/rocSOLVER/en/latest/index.html" title="(in rocSOLVER Documentation v3.28.2)"><span class="xref std std-doc">rocSOLVER</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipSPARSE/en/latest/index.html" title="(in hipSPARSE Documentation v3.2.0)"><span class="xref std std-doc">hipSPARSE</span></a> / <a class="reference external" href="https://rocm.docs.amd.com/projects/rocSPARSE/en/latest/index.html" title="(in rocSPARSE Documentation v3.4.0)"><span class="xref std std-doc">rocSPARSE</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/hipSPARSELt/en/latest/index.html" title="(in hipSPARSELt Documentation v0.2.3)"><span class="xref std std-doc">hipSPARSELt</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocALUTION/en/latest/index.html" title="(in rocALUTION Documentation v3.2.3)"><span class="xref std std-doc">rocALUTION</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocWMMA/en/latest/index.html" title="(in rocWMMA Documentation v1.7.0)"><span class="xref std std-doc">rocWMMA</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/Tensile/en/latest/src/index.html" title="(in Tensile Documentation v4.43.0)"><span class="xref std std-doc">Tensile</span></a></p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<span id="document-reference/rocm-tools"></span><head>
<meta charset="utf-8"/>
<meta content="ROCm API libraries &amp; tools" name="description"/>
<meta content="ROCm, API, libraries, tools, AI, artificial intelligence, development,
  Communications, C++ primitives, Fast Fourier transforms, FFTs, random number generators, linear
  algebra, AMD" name="keywords"/>
</head>
<section class="tex2jax_ignore mathjax_ignore" id="rocm-tools-compilers-and-runtimes">
<h2>ROCm tools, compilers, and runtimes<a class="headerlink" href="#rocm-tools-compilers-and-runtimes" title="Link to this heading">#</a></h2>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 rocm-doc-grid docutils">
<div class="sd-row sd-row-cols-1 sd-row-cols-xs-1 sd-row-cols-sm-2 sd-row-cols-md-2 sd-row-cols-lg-2 sd-g-3 sd-g-xs-3 sd-g-sm-3 sd-g-md-3 sd-g-lg-3 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body rocm-card-banner rocm-hue-1 docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
System Management</div>
<ul class="simple" id="system-tools">
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/amdsmi/en/latest/index.html" title="(in AMD SMI v25.5.1)"><span class="xref std std-doc">AMD SMI</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rdc/en/latest/index.html" title="(in ROCm Data Center Documentation)"><span class="xref std std-doc">ROCm Data Center Tool</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocminfo/en/latest/index.html" title="(in rocminfo v1.0.0)"><span class="xref std std-doc">rocminfo</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocm_smi_lib/en/latest/index.html" title="(in ROCm SMI LIB Documentation v7.7.0)"><span class="xref std std-doc">ROCm SMI</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCmValidationSuite/en/latest/index.html" title="(in RVS Documentation v1.1.0)"><span class="xref std std-doc">ROCm Validation Suite</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body rocm-card-banner rocm-hue-6 docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
Performance</div>
<ul class="simple" id="performance-tools">
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocm_bandwidth_test/en/latest/index.html" title="(in rocm_bandwidth_test)"><span class="xref std std-doc">ROCm Bandwidth Test</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler-compute/en/latest/index.html" title="(in ROCm Compute Profiler v3.1.1)"><span class="xref std std-doc">ROCm Compute Profiler</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler-systems/en/latest/index.html" title="(in rocprofiler-systems v1.0.2)"><span class="xref std std-doc">ROCm Systems Profiler</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler/en/latest/index.html" title="(in rocprofiler Documentation v2.0.0)"><span class="xref std std-doc">ROCProfiler</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler-sdk/en/latest/index.html" title="(in Rocprofiler SDK v0.6.0)"><span class="xref std std-doc">ROCprofiler-SDK</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/roctracer/en/latest/index.html" title="(in roctracer Documentation v4.1.0)"><span class="xref std std-doc">ROCTracer</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body rocm-card-banner rocm-hue-1 docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
Development</div>
<ul class="simple" id="development-tools">
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCmCMakeBuildTools/en/latest/index.html" title="(in ROCm CMake Build Tools v0.14.0)"><span class="xref std std-doc">ROCm CMake</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/HIPIFY/en/latest/index.html" title="(in HIPIFY Documentation)"><span class="xref std std-doc">HIPIFY</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCdbgapi/en/latest/index.html" title="(in ROCdbgapi Documentation v0.77.2)"><span class="xref std std-doc">ROCdbgapi</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCgdb/en/latest/index.html" title="(in ROCgdb Documentation v15.2)"><span class="xref std std-doc">ROCm Debugger (ROCgdb)</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/rocr_debug_agent/en/latest/index.html" title="(in rocr_debug_agent v2.0.4)"><span class="xref std std-doc">ROCr Debug Agent</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body rocm-card-banner rocm-hue-8 docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
Compilers</div>
<ul class="simple" id="compilers">
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/index.html" title="(in llvm-project Documentation v19.0.0)"><span class="xref std std-doc">ROCm Compilers</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/HIPCC/en/latest/index.html" title="(in HIPCC Documentation v1.1.1)"><span class="xref std std-doc">HIPCC</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://github.com/ROCm/flang/">FLANG</a></p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body rocm-card-banner rocm-hue-12 docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
Runtimes</div>
<ul class="simple" id="runtimes">
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/understand/amd_clr.html" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-doc">AMD Compute Language Runtime (CLR)</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/index.html" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-doc">HIP</span></a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="https://rocm.docs.amd.com/projects/ROCR-Runtime/en/latest/index.html" title="(in ROCR Documentation v1.15.0)"><span class="xref std std-doc">ROCR-Runtime</span></a></p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<span id="document-reference/gpu-arch-specs"></span><section id="accelerator-and-gpu-hardware-specifications">
<h2>Accelerator and GPU hardware specifications<a class="headerlink" href="#accelerator-and-gpu-hardware-specifications" title="Link to this heading">#</a></h2>
<p>The following tables provide an overview of the hardware specifications for AMD Instinct™ accelerators, and AMD Radeon™ PRO and Radeon™ GPUs.</p>
<p>For more information about ROCm hardware compatibility, see the ROCm <a class="reference external" href="https://rocm.docs.amd.com/en/latest/compatibility/compatibility-matrix.html">Compatibility matrix</a>.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-76" name="sd-tab-set-38" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-76">
AMD Instinct accelerators</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table" id="instinct-arch-spec-table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Architecture</p></th>
<th class="head"><p>LLVM target name</p></th>
<th class="head"><p>VRAM (GiB)</p></th>
<th class="head"><p>Compute Units</p></th>
<th class="head"><p>Wavefront Size</p></th>
<th class="head"><p>LDS (KiB)</p></th>
<th class="head"><p>L3 Cache (MiB)</p></th>
<th class="head"><p>L2 Cache (MiB)</p></th>
<th class="head"><p>L1 Vector Cache (KiB)</p></th>
<th class="head"><p>L1 Scalar Cache (KiB)</p></th>
<th class="head"><p>L1 Instruction Cache (KiB)</p></th>
<th class="head"><p>VGPR File (KiB)</p></th>
<th class="head"><p>SGPR File (KiB)</p></th>
<th class="head"><p>GFXIP Major version</p></th>
<th class="head"><p>GFXIP Minor version</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>MI325X</p></td>
<td><p>CDNA3</p></td>
<td><p>gfx942</p></td>
<td><p>256</p></td>
<td><p>304 (38 per XCD)</p></td>
<td><p>64</p></td>
<td><p>64</p></td>
<td><p>256</p></td>
<td><p>32 (4 per XCD)</p></td>
<td><p>32</p></td>
<td><p>16 per 2 CUs</p></td>
<td><p>64 per 2 CUs</p></td>
<td><p>512</p></td>
<td><p>12.5</p></td>
<td><p>9</p></td>
<td><p>4</p></td>
</tr>
<tr class="row-odd"><td><p>MI300X</p></td>
<td><p>CDNA3</p></td>
<td><p>gfx942</p></td>
<td><p>192</p></td>
<td><p>304 (38 per XCD)</p></td>
<td><p>64</p></td>
<td><p>64</p></td>
<td><p>256</p></td>
<td><p>32 (4 per XCD)</p></td>
<td><p>32</p></td>
<td><p>16 per 2 CUs</p></td>
<td><p>64 per 2 CUs</p></td>
<td><p>512</p></td>
<td><p>12.5</p></td>
<td><p>9</p></td>
<td><p>4</p></td>
</tr>
<tr class="row-even"><td><p>MI300A</p></td>
<td><p>CDNA3</p></td>
<td><p>gfx942</p></td>
<td><p>128</p></td>
<td><p>228 (38 per XCD)</p></td>
<td><p>64</p></td>
<td><p>64</p></td>
<td><p>256</p></td>
<td><p>24 (4 per XCD)</p></td>
<td><p>32</p></td>
<td><p>16 per 2 CUs</p></td>
<td><p>64 per 2 CUs</p></td>
<td><p>512</p></td>
<td><p>12.5</p></td>
<td><p>9</p></td>
<td><p>4</p></td>
</tr>
<tr class="row-odd"><td><p>MI250X</p></td>
<td><p>CDNA2</p></td>
<td><p>gfx90a</p></td>
<td><p>128</p></td>
<td><p>220 (110 per GCD)</p></td>
<td><p>64</p></td>
<td><p>64</p></td>
<td></td>
<td><p>16 (8 per GCD)</p></td>
<td><p>16</p></td>
<td><p>16 per 2 CUs</p></td>
<td><p>32 per 2 CUs</p></td>
<td><p>512</p></td>
<td><p>12.5</p></td>
<td><p>9</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>MI250</p></td>
<td><p>CDNA2</p></td>
<td><p>gfx90a</p></td>
<td><p>128</p></td>
<td><p>208 (104 per GCD)</p></td>
<td><p>64</p></td>
<td><p>64</p></td>
<td></td>
<td><p>16 (8 per GCD)</p></td>
<td><p>16</p></td>
<td><p>16 per 2 CUs</p></td>
<td><p>32 per 2 CUs</p></td>
<td><p>512</p></td>
<td><p>12.5</p></td>
<td><p>9</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>MI210</p></td>
<td><p>CDNA2</p></td>
<td><p>gfx90a</p></td>
<td><p>64</p></td>
<td><p>104</p></td>
<td><p>64</p></td>
<td><p>64</p></td>
<td></td>
<td><p>8</p></td>
<td><p>16</p></td>
<td><p>16 per 2 CUs</p></td>
<td><p>32 per 2 CUs</p></td>
<td><p>512</p></td>
<td><p>12.5</p></td>
<td><p>9</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>MI100</p></td>
<td><p>CDNA</p></td>
<td><p>gfx908</p></td>
<td><p>32</p></td>
<td><p>120</p></td>
<td><p>64</p></td>
<td><p>64</p></td>
<td></td>
<td><p>8</p></td>
<td><p>16</p></td>
<td><p>16 per 3 CUs</p></td>
<td><p>32 per 3 CUs</p></td>
<td><p>256 VGPR and 256 AccVGPR</p></td>
<td><p>12.5</p></td>
<td><p>9</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>MI60</p></td>
<td><p>GCN5.1</p></td>
<td><p>gfx906</p></td>
<td><p>32</p></td>
<td><p>64</p></td>
<td><p>64</p></td>
<td><p>64</p></td>
<td></td>
<td><p>4</p></td>
<td><p>16</p></td>
<td><p>16 per 3 CUs</p></td>
<td><p>32 per 3 CUs</p></td>
<td><p>256</p></td>
<td><p>12.5</p></td>
<td><p>9</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>MI50 (32GB)</p></td>
<td><p>GCN5.1</p></td>
<td><p>gfx906</p></td>
<td><p>32</p></td>
<td><p>60</p></td>
<td><p>64</p></td>
<td><p>64</p></td>
<td></td>
<td><p>4</p></td>
<td><p>16</p></td>
<td><p>16 per 3 CUs</p></td>
<td><p>32 per 3 CUs</p></td>
<td><p>256</p></td>
<td><p>12.5</p></td>
<td><p>9</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>MI50 (16GB)</p></td>
<td><p>GCN5.1</p></td>
<td><p>gfx906</p></td>
<td><p>16</p></td>
<td><p>60</p></td>
<td><p>64</p></td>
<td><p>64</p></td>
<td></td>
<td><p>4</p></td>
<td><p>16</p></td>
<td><p>16 per 3 CUs</p></td>
<td><p>32 per 3 CUs</p></td>
<td><p>256</p></td>
<td><p>12.5</p></td>
<td><p>9</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>MI25</p></td>
<td><p>GCN5.0</p></td>
<td><p>gfx900</p></td>
<td><p>16</p></td>
<td><p>64</p></td>
<td><p>64</p></td>
<td><p>64</p></td>
<td></td>
<td><p>4</p></td>
<td><p>16</p></td>
<td><p>16 per 3 CUs</p></td>
<td><p>32 per 3 CUs</p></td>
<td><p>256</p></td>
<td><p>12.5</p></td>
<td><p>9</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>MI8</p></td>
<td><p>GCN3.0</p></td>
<td><p>gfx803</p></td>
<td><p>4</p></td>
<td><p>64</p></td>
<td><p>64</p></td>
<td><p>64</p></td>
<td></td>
<td><p>2</p></td>
<td><p>16</p></td>
<td><p>16 per 4 CUs</p></td>
<td><p>32 per 4 CUs</p></td>
<td><p>256</p></td>
<td><p>12.5</p></td>
<td><p>8</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>MI6</p></td>
<td><p>GCN4.0</p></td>
<td><p>gfx803</p></td>
<td><p>16</p></td>
<td><p>36</p></td>
<td><p>64</p></td>
<td><p>64</p></td>
<td></td>
<td><p>2</p></td>
<td><p>16</p></td>
<td><p>16 per 4 CUs</p></td>
<td><p>32 per 4 CUs</p></td>
<td><p>256</p></td>
<td><p>12.5</p></td>
<td><p>8</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-77" name="sd-tab-set-38" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-77">
AMD Radeon PRO GPUs</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table" id="radeon-pro-arch-spec-table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Architecture</p></th>
<th class="head"><p>LLVM target name</p></th>
<th class="head"><p>VRAM (GiB)</p></th>
<th class="head"><p>Compute Units</p></th>
<th class="head"><p>Wavefront Size</p></th>
<th class="head"><p>LDS (KiB)</p></th>
<th class="head"><p>Infinity Cache (MiB)</p></th>
<th class="head"><p>L2 Cache (MiB)</p></th>
<th class="head"><p>Graphics L1 Cache (KiB)</p></th>
<th class="head"><p>L0 Vector Cache (KiB)</p></th>
<th class="head"><p>L0 Scalar Cache (KiB)</p></th>
<th class="head"><p>L0 Instruction Cache (KiB)</p></th>
<th class="head"><p>VGPR File (KiB)</p></th>
<th class="head"><p>SGPR File (KiB)</p></th>
<th class="head"><p>GFXIP Major version</p></th>
<th class="head"><p>GFXIP Minor version</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Radeon AI PRO R9700</p></td>
<td><p>RDNA4</p></td>
<td><p>gfx1201</p></td>
<td><p>16</p></td>
<td><p>64</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>64</p></td>
<td><p>8</p></td>
<td><p>N/A</p></td>
<td><p>32</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>768</p></td>
<td><p>32</p></td>
<td><p>12</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>Radeon PRO V710</p></td>
<td><p>RDNA3</p></td>
<td><p>gfx1101</p></td>
<td><p>28</p></td>
<td><p>54</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>56</p></td>
<td><p>4</p></td>
<td><p>256</p></td>
<td><p>32</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>768</p></td>
<td><p>32</p></td>
<td><p>11</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>Radeon PRO W7900 Dual Slot</p></td>
<td><p>RDNA3</p></td>
<td><p>gfx1100</p></td>
<td><p>48</p></td>
<td><p>96</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>96</p></td>
<td><p>6</p></td>
<td><p>256</p></td>
<td><p>32</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>768</p></td>
<td><p>32</p></td>
<td><p>11</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>Radeon PRO W7900</p></td>
<td><p>RDNA3</p></td>
<td><p>gfx1100</p></td>
<td><p>48</p></td>
<td><p>96</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>96</p></td>
<td><p>6</p></td>
<td><p>256</p></td>
<td><p>32</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>768</p></td>
<td><p>32</p></td>
<td><p>11</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>Radeon PRO W7800 48GB</p></td>
<td><p>RDNA3</p></td>
<td><p>gfx1100</p></td>
<td><p>48</p></td>
<td><p>70</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>96</p></td>
<td><p>6</p></td>
<td><p>256</p></td>
<td><p>32</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>768</p></td>
<td><p>32</p></td>
<td><p>11</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>Radeon PRO W7800</p></td>
<td><p>RDNA3</p></td>
<td><p>gfx1100</p></td>
<td><p>32</p></td>
<td><p>70</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>64</p></td>
<td><p>6</p></td>
<td><p>256</p></td>
<td><p>32</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>768</p></td>
<td><p>32</p></td>
<td><p>11</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>Radeon PRO W7700</p></td>
<td><p>RDNA3</p></td>
<td><p>gfx1101</p></td>
<td><p>16</p></td>
<td><p>48</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>64</p></td>
<td><p>4</p></td>
<td><p>256</p></td>
<td><p>32</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>768</p></td>
<td><p>32</p></td>
<td><p>11</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>Radeon PRO W6800</p></td>
<td><p>RDNA2</p></td>
<td><p>gfx1030</p></td>
<td><p>32</p></td>
<td><p>60</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>128</p></td>
<td><p>4</p></td>
<td><p>128</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>512</p></td>
<td><p>32</p></td>
<td><p>10</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-even"><td><p>Radeon PRO W6600</p></td>
<td><p>RDNA2</p></td>
<td><p>gfx1032</p></td>
<td><p>8</p></td>
<td><p>28</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>32</p></td>
<td><p>2</p></td>
<td><p>128</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>512</p></td>
<td><p>32</p></td>
<td><p>10</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-odd"><td><p>Radeon PRO V620</p></td>
<td><p>RDNA2</p></td>
<td><p>gfx1030</p></td>
<td><p>32</p></td>
<td><p>72</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>128</p></td>
<td><p>4</p></td>
<td><p>128</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>512</p></td>
<td><p>32</p></td>
<td><p>10</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-even"><td><p>Radeon Pro W5500</p></td>
<td><p>RDNA</p></td>
<td><p>gfx1012</p></td>
<td><p>8</p></td>
<td><p>22</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td></td>
<td><p>4</p></td>
<td><p>128</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>512</p></td>
<td><p>20</p></td>
<td><p>10</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>Radeon Pro VII</p></td>
<td><p>GCN5.1</p></td>
<td><p>gfx906</p></td>
<td><p>16</p></td>
<td><p>60</p></td>
<td><p>64</p></td>
<td><p>64</p></td>
<td></td>
<td><p>4</p></td>
<td></td>
<td><p>16</p></td>
<td><p>16 per 3 CUs</p></td>
<td><p>32 per 3 CUs</p></td>
<td><p>256</p></td>
<td><p>12.5</p></td>
<td><p>9</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-78" name="sd-tab-set-38" type="radio"/>
<label class="sd-tab-label" for="sd-tab-item-78">
AMD Radeon GPUs</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table" id="radeon-arch-spec-table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Architecture</p></th>
<th class="head"><p>LLVM target name</p></th>
<th class="head"><p>VRAM (GiB)</p></th>
<th class="head"><p>Compute Units</p></th>
<th class="head"><p>Wavefront Size</p></th>
<th class="head"><p>LDS (KiB)</p></th>
<th class="head"><p>Infinity Cache (MiB)</p></th>
<th class="head"><p>L2 Cache (MiB)</p></th>
<th class="head"><p>Graphics L1 Cache (KiB)</p></th>
<th class="head"><p>L0 Vector Cache (KiB)</p></th>
<th class="head"><p>L0 Scalar Cache (KiB)</p></th>
<th class="head"><p>L0 Instruction Cache (KiB)</p></th>
<th class="head"><p>VGPR File (KiB)</p></th>
<th class="head"><p>SGPR File (KiB)</p></th>
<th class="head"><p>GFXIP Major version</p></th>
<th class="head"><p>GFXIP Minor version</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Radeon RX 9070 XT</p></td>
<td><p>RDNA4</p></td>
<td><p>gfx1201</p></td>
<td><p>16</p></td>
<td><p>64</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>64</p></td>
<td><p>8</p></td>
<td><p>N/A</p></td>
<td><p>32</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>768</p></td>
<td><p>32</p></td>
<td><p>12</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>Radeon RX 9070 GRE</p></td>
<td><p>RDNA4</p></td>
<td><p>gfx1201</p></td>
<td><p>16</p></td>
<td><p>48</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>48</p></td>
<td><p>6</p></td>
<td><p>N/A</p></td>
<td><p>32</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>768</p></td>
<td><p>32</p></td>
<td><p>12</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>Radeon RX 9070</p></td>
<td><p>RDNA4</p></td>
<td><p>gfx1201</p></td>
<td><p>16</p></td>
<td><p>56</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>64</p></td>
<td><p>8</p></td>
<td><p>N/A</p></td>
<td><p>32</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>768</p></td>
<td><p>32</p></td>
<td><p>12</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>Radeon RX 9060 XT</p></td>
<td><p>RDNA4</p></td>
<td><p>gfx1200</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>32</p></td>
<td><p>4</p></td>
<td><p>N/A</p></td>
<td><p>32</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>768</p></td>
<td><p>32</p></td>
<td><p>12</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>Radeon RX 7900 XTX</p></td>
<td><p>RDNA3</p></td>
<td><p>gfx1100</p></td>
<td><p>24</p></td>
<td><p>96</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>96</p></td>
<td><p>6</p></td>
<td><p>256</p></td>
<td><p>32</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>768</p></td>
<td><p>32</p></td>
<td><p>11</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>Radeon RX 7900 XT</p></td>
<td><p>RDNA3</p></td>
<td><p>gfx1100</p></td>
<td><p>20</p></td>
<td><p>84</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>80</p></td>
<td><p>6</p></td>
<td><p>256</p></td>
<td><p>32</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>768</p></td>
<td><p>32</p></td>
<td><p>11</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>Radeon RX 7900 GRE</p></td>
<td><p>RDNA3</p></td>
<td><p>gfx1100</p></td>
<td><p>16</p></td>
<td><p>80</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>64</p></td>
<td><p>6</p></td>
<td><p>256</p></td>
<td><p>32</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>768</p></td>
<td><p>32</p></td>
<td><p>11</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>Radeon RX 7800 XT</p></td>
<td><p>RDNA3</p></td>
<td><p>gfx1101</p></td>
<td><p>16</p></td>
<td><p>60</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>64</p></td>
<td><p>4</p></td>
<td><p>256</p></td>
<td><p>32</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>768</p></td>
<td><p>32</p></td>
<td><p>11</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>Radeon RX 7700 XT</p></td>
<td><p>RDNA3</p></td>
<td><p>gfx1101</p></td>
<td><p>12</p></td>
<td><p>54</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>48</p></td>
<td><p>4</p></td>
<td><p>256</p></td>
<td><p>32</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>768</p></td>
<td><p>32</p></td>
<td><p>11</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>Radeon RX 7600</p></td>
<td><p>RDNA3</p></td>
<td><p>gfx1102</p></td>
<td><p>8</p></td>
<td><p>32</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>32</p></td>
<td><p>2</p></td>
<td><p>256</p></td>
<td><p>32</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>512</p></td>
<td><p>32</p></td>
<td><p>11</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>Radeon RX 6950 XT</p></td>
<td><p>RDNA2</p></td>
<td><p>gfx1030</p></td>
<td><p>16</p></td>
<td><p>80</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>128</p></td>
<td><p>4</p></td>
<td><p>128</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>512</p></td>
<td><p>32</p></td>
<td><p>10</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-odd"><td><p>Radeon RX 6900 XT</p></td>
<td><p>RDNA2</p></td>
<td><p>gfx1030</p></td>
<td><p>16</p></td>
<td><p>80</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>128</p></td>
<td><p>4</p></td>
<td><p>128</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>512</p></td>
<td><p>32</p></td>
<td><p>10</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-even"><td><p>Radeon RX 6800 XT</p></td>
<td><p>RDNA2</p></td>
<td><p>gfx1030</p></td>
<td><p>16</p></td>
<td><p>72</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>128</p></td>
<td><p>4</p></td>
<td><p>128</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>512</p></td>
<td><p>32</p></td>
<td><p>10</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-odd"><td><p>Radeon RX 6800</p></td>
<td><p>RDNA2</p></td>
<td><p>gfx1030</p></td>
<td><p>16</p></td>
<td><p>60</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>128</p></td>
<td><p>4</p></td>
<td><p>128</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>512</p></td>
<td><p>32</p></td>
<td><p>10</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-even"><td><p>Radeon RX 6750 XT</p></td>
<td><p>RDNA2</p></td>
<td><p>gfx1031</p></td>
<td><p>12</p></td>
<td><p>40</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>96</p></td>
<td><p>3</p></td>
<td><p>128</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>512</p></td>
<td><p>32</p></td>
<td><p>10</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-odd"><td><p>Radeon RX 6700 XT</p></td>
<td><p>RDNA2</p></td>
<td><p>gfx1031</p></td>
<td><p>12</p></td>
<td><p>40</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>96</p></td>
<td><p>3</p></td>
<td><p>128</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>512</p></td>
<td><p>32</p></td>
<td><p>10</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-even"><td><p>Radeon RX 6700</p></td>
<td><p>RDNA2</p></td>
<td><p>gfx1031</p></td>
<td><p>10</p></td>
<td><p>36</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>80</p></td>
<td><p>3</p></td>
<td><p>128</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>512</p></td>
<td><p>32</p></td>
<td><p>10</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-odd"><td><p>Radeon RX 6650 XT</p></td>
<td><p>RDNA2</p></td>
<td><p>gfx1032</p></td>
<td><p>8</p></td>
<td><p>32</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>32</p></td>
<td><p>2</p></td>
<td><p>128</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>512</p></td>
<td><p>32</p></td>
<td><p>10</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-even"><td><p>Radeon RX 6600 XT</p></td>
<td><p>RDNA2</p></td>
<td><p>gfx1032</p></td>
<td><p>8</p></td>
<td><p>32</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>32</p></td>
<td><p>2</p></td>
<td><p>128</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>512</p></td>
<td><p>32</p></td>
<td><p>10</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-odd"><td><p>Radeon RX 6600</p></td>
<td><p>RDNA2</p></td>
<td><p>gfx1032</p></td>
<td><p>8</p></td>
<td><p>28</p></td>
<td><p>32 or 64</p></td>
<td><p>128</p></td>
<td><p>32</p></td>
<td><p>2</p></td>
<td><p>128</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>512</p></td>
<td><p>32</p></td>
<td><p>10</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-even"><td><p>Radeon VII</p></td>
<td><p>GCN5.1</p></td>
<td><p>gfx906</p></td>
<td><p>16</p></td>
<td><p>60</p></td>
<td><p>64</p></td>
<td><p>64 per CU</p></td>
<td></td>
<td><p>4</p></td>
<td></td>
<td><p>16</p></td>
<td><p>16 per 3 CUs</p></td>
<td><p>32 per 3 CUs</p></td>
<td><p>256</p></td>
<td><p>12.5</p></td>
<td><p>9</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section id="glossary">
<h2>Glossary<a class="headerlink" href="#glossary" title="Link to this heading">#</a></h2>
<p>For more information about the terms used, see the
<a class="reference internal" href="#gpu-arch-documentation"><span class="std std-ref">specific documents and guides</span></a>, or
<a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/understand/programming_model.html" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-doc">Understanding the HIP programming model</span></a>.</p>
<p><strong>LLVM target name</strong></p>
<p>Argument to pass to clang in <code class="docutils literal notranslate"><span class="pre">--offload-arch</span></code> to compile code for the given
architecture.</p>
<p><strong>VRAM</strong></p>
<p>Amount of memory available on the GPU.</p>
<p><strong>Compute Units</strong></p>
<p>Number of compute units on the GPU.</p>
<p><strong>Wavefront Size</strong></p>
<p>Amount of work items that execute in parallel on a single compute unit. This
is equivalent to the warp size in HIP.</p>
<p><strong>LDS</strong></p>
<p>The Local Data Share (LDS) is a low-latency, high-bandwidth scratch pad
memory. It is local to the compute units, and can be shared by all work items
in a work group. In HIP, the LDS can be used for shared memory, which is
shared by all threads in a block.</p>
<p><strong>L3 Cache (CDNA/GCN only)</strong></p>
<p>Size of the level 3 cache. Shared by all compute units on the same GPU. Caches
data and instructions. Similar to the Infinity Cache on RDNA architectures.</p>
<p><strong>Infinity Cache (RDNA only)</strong></p>
<p>Size of the infinity cache. Shared by all compute units on the same GPU. Caches
data and instructions. Similar to the L3 Cache on CDNA/GCN architectures.</p>
<p><strong>L2 Cache</strong></p>
<p>Size of the level 2 cache. Shared by all compute units on the same GCD. Caches
data and instructions.</p>
<p><strong>Graphics L1 Cache (RDNA only)</strong></p>
<p>An additional cache level that only exists in RDNA architectures. Local to a
shader array.</p>
<p><strong>L1 Vector Cache (CDNA/GCN only)</strong></p>
<p>Size of the level 1 vector data cache. Local to a compute unit. This is the L0
vector cache in RDNA architectures.</p>
<p><strong>L1 Scalar Cache (CDNA/GCN only)</strong></p>
<p>Size of the level 1 scalar data cache. Usually shared by several compute
units. This is the L0 scalar cache in RDNA architectures.</p>
<p><strong>L1 Instruction Cache (CDNA/GCN only)</strong></p>
<p>Size of the level 1 instruction cache. Usually shared by several compute
units. This is the L0 instruction cache in RDNA architectures.</p>
<p><strong>L0 Vector Cache (RDNA only)</strong></p>
<p>Size of the level 0 vector data cache. Local to a compute unit. This is the L1
vector cache in CDNA/GCN architectures.</p>
<p><strong>L0 Scalar Cache (RDNA only)</strong></p>
<p>Size of the level 0 scalar data cache. Usually shared by several compute
units. This is the L1 scalar cache in CDNA/GCN architectures.</p>
<p><strong>L0 Instruction Cache (RDNA only)</strong></p>
<p>Size of the level 0 instruction cache. Usually shared by several compute
units. This is the L1 instruction cache in CDNA/GCN architectures.</p>
<p><strong>VGPR File</strong></p>
<p>Size of the Vector General Purpose Register (VGPR) file and. It holds data used in
vector instructions.
GPUs with matrix cores also have AccVGPRs, which are Accumulation General
Purpose Vector Registers, used specifically in matrix instructions.</p>
<p><strong>SGPR File</strong></p>
<p>Size of the Scalar General Purpose Register (SGPR) file. Holds data used in
scalar instructions.</p>
<p><strong>GFXIP</strong></p>
<p>GFXIP (Graphics IP) is a versioning system used by AMD to identify the GPU
architecture and its instruction set. It helps categorize different generations
of GPUs and their feature sets.</p>
<p><strong>GFXIP major version</strong></p>
<p>Defines the GPU’s core instruction set and architecture, which determines
compatibility with software stacks such as HIP and OpenCL. For example, a GFXIP
11 major version corresponds to the RDNA 3 (Navi 3x) architecture, influencing
driver support and available compute features.</p>
<p><strong>GFXIP minor version</strong></p>
<p>Represents specific variations within a GFXIP major version and affects feature sets,
optimizations, and driver behavior in software stacks such as HIP and OpenCL. Different
GPU models within the same major version can have unique capabilities, impacting
performance and supported instructions.</p>
<p><strong>GCD</strong></p>
<p>Graphics Compute Die.</p>
<p><strong>XCD</strong></p>
<p>Accelerator Complex Die.</p>
</section>
<span id="document-reference/gpu-atomics-operation"></span><section id="hardware-atomics-operation-support">
<span id="hw-atomics-operation-support"></span><h2>Hardware atomics operation support<a class="headerlink" href="#hardware-atomics-operation-support" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/how-to/hip_cpp_language_extensions.html#atomic-functions" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-ref">Atomic operations</span></a> guarantee that the operation is
completed as an indivisible unit, preventing race conditions where simultaneous
access to the same memory location could lead to incorrect or undefined
behavior.</p>
<p>This document details the various support of atomic read-modify-write
(atomicRMW) operations on gfx9, gfx10, gfx11, gfx12, MI100, MI200 and MI300 AMD
GPUs. The atomics operation type behavior effected by the memory locations,
memory granularity or scope of operations.</p>
<p>Memory locations:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/how-to/hip_runtime_api/memory_management/device_memory.html#device-memory" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-ref">Device memory</span></a>, i.e. VRAM, the RAM on a discrete GPU
device or in framebuffer carveout for APUs. This includes peer-device memory
within an Infinity Fabric™ hive.</p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/how-to/hip_runtime_api/memory_management/host_memory.html#host-memory" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-ref">Host memory</span></a>: in DRAM associated with the CPU (or
peer device memory using PCIe® (PCI Express) peer-to-peer). This can be two sub-types:</p>
<ul>
<li><p>Migratable memory: memory that is currently residing in host DRAM, but
which can be migrated back to device memory. For example,
<code class="docutils literal notranslate"><span class="pre">hipMallocManaged()</span></code> or <a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/how-to/hip_runtime_api/memory_management/unified_memory.html#unified-memory" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-ref">unified memory</span></a>
allocations.</p></li>
<li><p><a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/how-to/hip_runtime_api/memory_management/host_memory.html#pinned-host-memory" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-ref">Pinned memory</span></a>: memory that is in host memory
and cannot be migrated to the device (not necessarily pinned to a particular
physical address, but can’t be moved to device memory). <code class="docutils literal notranslate"><span class="pre">hipHostMalloc()</span></code>,
for example.</p></li>
</ul>
</li>
</ul>
<p>Memory granularity or <a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/how-to/hip_runtime_api/memory_management/coherence_control.html#coherence-control" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-ref">coherence</span></a>:</p>
<ul class="simple">
<li><p>Coarse-grained memory</p>
<ul>
<li><p>This memory can be used for device-scope synchronization during the
execution of a single GPU kernel. Any system-scope atomics sent to this type
of memory will not achieve system-scope coherency and will instead be
downgraded to device-scope as per the programming model.</p></li>
<li><p>This type of memory only available on AMD GPUs.</p></li>
</ul>
</li>
<li><p>Fine-grained memory</p>
<ul>
<li><p>This memory can be used for device and system-scope synchronization during
the execution of a single GPU kernel.</p></li>
</ul>
</li>
</ul>
<p>Scopes of operations:</p>
<ul class="simple">
<li><p>Device-scope or agent-scope</p>
<ul>
<li><p>This atomic should happen atomically from the point of view of every thread
within the device that the atomic-executing thread is in.</p></li>
</ul>
</li>
<li><p>System-scope</p>
<ul>
<li><p>This atomic should happen atomically from the point of view of every thread
in all devices and in the CPUs.</p></li>
</ul>
</li>
</ul>
</section>
<section id="support-summary">
<h2>Support summary<a class="headerlink" href="#support-summary" title="Link to this heading">#</a></h2>
<section id="amd-instinct-accelerators">
<h3>AMD Instinct™ accelerators<a class="headerlink" href="#amd-instinct-accelerators" title="Link to this heading">#</a></h3>
<p><strong>MI300</strong></p>
<ul class="simple">
<li><p>All atomicRMW operations are forwarded out to the Infinity Fabric.</p></li>
<li><p>Infinity Fabric supports common integer and bitwise atomics, FP32 atomic add,
packed-FP16 atomic add, packed-BF16 atomic add, and FP64 add, min, and max.</p></li>
<li><p>In discrete GPUs (dGPUs), if the data is stored in host memory, the atomic
will be forwarded from the Infinity Fabric to PCIe.</p></li>
<li><p>If the PCIe bus does not support the requested atomic, the GPU’s PCIe
controller changes it into a load-op-store sequence. All waves on the chip
submitting atomics to that address will stall waiting for the load-op-store.
It will seem like atomics to the wave, but the CPU sees it as a non-atomic
load-op-store sequence. This downgrades system-scope atomics to device-scope.</p></li>
</ul>
<p><strong>MI200</strong></p>
<ul class="simple">
<li><p>L2 cache and Infinity Fabric both support common integer and bitwise atomics.</p></li>
<li><p>L2 cache supports FP32 atomic add, packed-FP16 atomic add, and FP64 add,
min, and max.</p></li>
<li><p>The Infinity Fabric does not support FP32 atomic add, packed-FP16 atomic add,
and FP64 add, min, and max atomics and these commands cannot be sent to the
Infinity Fabric.</p></li>
<li><p>Coarse-grained memory is marked as cacheable, and atomic operations will be
processed in the L2 cache.</p></li>
<li><p>Fine-grained memory is marked write-uncacheable through the page tables.</p></li>
<li><p>Atomics that hit write-uncached memory are forwarded to the Infinity Fabric.</p></li>
<li><p>If the uncached data is stored in host memory on a PCIe system, the atomic
will be forwarded from Infinity Fabric to PCIe. Any atomic not supported by
the PCIe bus will be a NOP and give incorrect result.</p></li>
<li><p>If the uncached data is stored in host memory on an A+A system (system with
AMD CPU and AMD GPU connected via Infinity Fabric), the atomic operation will
be forwarded to the remote location and will succeed if supported by Infinity
Fabric.</p></li>
<li><p>If the float atomics access write-uncached memory, they cannot be forwarded to
the Infinity Fabric, resulting in a NOP and an incorrect outcome.</p></li>
</ul>
<p><strong>MI100</strong></p>
<ul class="simple">
<li><p>L2 cache and Infinity Fabric both support common integer and bitwise atomics.</p></li>
<li><p>L2 cache supports no returns (NoReturn) versions of packed-FP16 and FP32
atomic adds, that cannot return data.</p></li>
<li><p>The Infinity Fabric does not support packed-FP16 or FP32 atomic adds,
preventing these commands from being transmitted through it.</p></li>
<li><p>Coarse-grained memory is marked as cacheable, and atomic operations will be
processed in the L2 cache.</p></li>
<li><p>Fine-grained memory is marked uncacheable through the page tables.</p></li>
<li><p>Atomics that hit uncached memory are forwarded to the Infinity Fabric.</p></li>
<li><p>If the uncached data is stored in host memory, the atomic will be forwarded
from Infinity Fabric to PCIe. Any atomic not supported by the PCIe bus will
be a NOP and give incorrect result.</p></li>
<li><p>If an float atomic add hits uncached memory, it cannot be forwarded to the
Infinity Fabric so it will NOP and give incorrect result.</p></li>
</ul>
</section>
<section id="amd-gfx-generic-targets">
<h3>AMD gfx generic targets<a class="headerlink" href="#amd-gfx-generic-targets" title="Link to this heading">#</a></h3>
<p><strong>gfx9</strong></p>
<ul class="simple">
<li><p>L2 cache and Infinity Fabric both support common integer and bitwise atomics.</p></li>
<li><p>Coarse-grained memory is marked as cacheable, and atomic operations will be
processed in the L2 cache.</p></li>
<li><p>Fine-grained memory is marked uncacheable through the page tables.</p></li>
<li><p>Atomics that hit uncached memory are forwarded to the Infinity Fabric.</p></li>
<li><p>In a dGPU: if the uncached data is stored in host memory, the atomic will be
forwarded from Infinity Fabric to PCIe. Any atomic not supported by the PCIe
bus will be a NOP and.</p></li>
</ul>
<p><strong>gfx10</strong></p>
<ul class="simple">
<li><p>L2 cache and Infinity Fabric both support common integer and bitwise atomics.</p></li>
<li><p>Coarse-grained memory is marked as cacheable, and atomic operations will be
processed in the L2 cache.</p></li>
<li><p>Fine-grained memory is marked uncacheable through the page tables.</p></li>
<li><p>Atomics that hit uncached memory are forwarded to the Infinity Fabric.</p></li>
<li><p>In a dGPU: if the uncached data is stored in host memory, the atomic will be
forwarded from Infinity Fabric to PCIe. Any atomic not supported by the PCIe
bus will be a NOP and give incorrect result.</p></li>
<li><p>Supports floating-point atomic min/max.</p></li>
<li><p>The Infinity Fabric does not support floating-point atomic min/max atomics
and these commands cannot be sent to the Infinity Fabric.</p></li>
<li><p>If the floating-point atomics hit uncached memory, they cannot be forwarded to
the Infinity Fabric, so they will NOP and give incorrect result.</p></li>
</ul>
<p><strong>gfx11</strong></p>
<ul class="simple">
<li><p>L2 cache and Infinity Fabric both support common integer and bitwise atomics.</p></li>
<li><p>L2 cache supports FP32 atomic add, min and max.</p></li>
<li><p>The Infinity Fabric does not support FP32 atomic add, min and max atomics and
these commands cannot be sent to the Infinity Fabric.</p></li>
<li><p>Coarse-grained memory is marked as cacheable, and atomic operations will be
processed in the L2 cache.</p></li>
<li><p>Fine-grained memory is marked uncacheable through the page tables.</p></li>
<li><p>Atomics that hit write-uncached memory are forwarded to the Infinity Fabric.</p></li>
<li><p>In a dGPU: if the uncached data is stored in host memory, the atomic will be
forwarded from Infinity Fabric to PCIe. Any atomic not supported by the PCIe
bus will be a NOP and give incorrect result.</p></li>
<li><p>If the float atomics hit uncached memory, they cannot be forwarded to the
Infinity Fabric, so they will NOP and give incorrect result.</p></li>
</ul>
<p><strong>gfx12</strong></p>
<ul class="simple">
<li><p>L2 cache and Infinity Fabric both support common integer and bitwise atomics.</p></li>
<li><p>L2 cache and Infinity Fabric both also support FP32 atomic add, min and max,
and packed-FP16 atomic add, and packed-BF16 atomic add.</p></li>
<li><p>Coarse-grained memory is marked as cacheable, and atomic operations will be
processed in the L2 cache.</p></li>
<li><p>Fine-grained device memory is marked uncacheable through the page tables.</p>
<ul>
<li><p>Atomics that hit write-uncached memory are forwarded to the Infinity Fabric.</p></li>
</ul>
</li>
<li><p>Fine-grained system memory is marked as cacheable through the page tables.</p>
<ul>
<li><p>Device-scope atomic operations will process in the L2 cache.</p></li>
<li><p>System-scope atomic operations will bypass the L2 cache and be forwarded to
the Infinity Fabric.</p></li>
</ul>
</li>
<li><p>Atomics that hit write-uncached memory are forwarded to the Infinity Fabric.</p></li>
<li><p>In dGPUs, if the data is stored in host memory, the atomic will be forwarded
from the Infinity Fabric to PCIe.</p></li>
<li><p>If the PCIe bus does not support the requested atomic, the GPU’s PCIe
controller changes it into a load-op-store sequence. All waves on the chip
submitting atomics to that address will stall waiting for the load-op-store.
It will seem like atomics to the wave, but the CPU sees it as a non-atomic
load-op-store sequence. This downgrades system-scope atomics to device-scope.</p></li>
</ul>
</section>
</section>
<section id="gpus-atomics-support">
<h2>GPUs atomics support<a class="headerlink" href="#gpus-atomics-support" title="Link to this heading">#</a></h2>
<p>This section presents a series of tables that show the level of atomic
operations support for the different hardware devices described above, and
different datatypes, different operations and different scopes.</p>
<p>Hardware atomics support refers to the ability of GPUs to natively perform
atomic operations—special low-level operations that ensure data consistency when
multiple threads access and modify memory concurrently.</p>
<p>CAS (Compare-and-Swap) atomic support refers to the hardware or software
capability to perform an atomic Compare-and-Swap operation.</p>
<p>PCIe atomics are a feature of the PCIe interface that enable
atomic operations between devices and hosts across the PCIe bus. For further
information, please check <a class="reference external" href="https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/conceptual/pcie-atomics.html">How ROCm uses PCIe atomics</a>.</p>
<p>The tables that follow show the correctness of atomics operations on the
hardware using the following notations:</p>
<ul class="simple">
<li><p>✅: Produces the correct answer.</p></li>
<li><p>⚠️: Produces the correct answer, but works only at a weaker scope.</p></li>
<li><p>❌: The atomics operation fails.</p></li>
</ul>
<p>The tables show the different types of atomic operations used by specific
devices:</p>
<ul class="simple">
<li><p>Native: Computes the correct result using a hardware-native atomic
instruction.</p></li>
<li><p>CAS: Generates the correct result, but the atomic operation is implemented by
the compiler for this ISA using a compare-and-swap emulation loop.</p></li>
<li><p>✅ NoReturn: Produces the correct correct result but does not precisely
conform to the atomic API.</p></li>
<li><p>Scope Downgrade: Generates the correct result but operates at a weaker scope
than requested. For example, if a user specifies a system-scope atomic, the
operation may only function at the device scope.</p></li>
<li><p>NOP: The atomic operation is not executed on the target location, and the
requesting thread receives back 0 as a return value.</p></li>
<li><p>n/a: The atomic type is not supported and cannot be executed on the specific
hardware.</p></li>
</ul>
<p>The tables selectors or options are the following:</p>
<ul class="simple">
<li><p>Highest level option:</p>
<ul>
<li><p>“HW atomics”, where software attempts to use hardware atomics.</p></li>
<li><p>“CAS emulation”, where software attempts to use CAS emulation.</p></li>
</ul>
</li>
<li><p>Second-level option:</p>
<ul>
<li><p>“No PCIe atomics” means the system does not support PCIe atomics between
the accelerator and peer/host-memory.</p></li>
<li><p>“PCIe atomics” means the system supports PCIe atomics between the
accelerator and peer/host-memory.</p></li>
</ul>
</li>
<li><p>The third-level option is the memory granularity of the memory target.</p></li>
<li><p>The final option is the scope of atomic access.</p></li>
</ul>
<section id="integer-atomics-operations">
<h3>Integer atomics operations<a class="headerlink" href="#integer-atomics-operations" title="Link to this heading">#</a></h3>
<p>The integer type atomic operations that are supported by different hardware.</p>
<ul class="simple">
<li><p>32 bit integer</p>
<ul>
<li><p>Add</p></li>
<li><p>Subtract</p></li>
<li><p>Min</p></li>
<li><p>Max</p></li>
<li><p>IncDec</p></li>
</ul>
</li>
<li><p>64 bit integer</p>
<ul>
<li><p>Add</p></li>
<li><p>Min</p></li>
<li><p>Max</p></li>
</ul>
</li>
</ul>
<section id="id1">
<h4>AMD Instinct accelerators<a class="headerlink" href="#id1" title="Link to this heading">#</a></h4>
<p>The integer type atomic operations that are supported by different AMD
Instinct accelerators listed in the following table.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-79" name="sd-tab-set-39" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="hw-atomics" for="sd-tab-item-79">
HW atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-81" name="sd-tab-set-40" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="nopcie" for="sd-tab-item-81">
No PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-83" name="sd-tab-set-41" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-83">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-86" name="sd-tab-set-42" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-86">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-88" name="sd-tab-set-43" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-88">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-89" name="sd-tab-set-43" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-89">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-87" name="sd-tab-set-42" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-87">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-90" name="sd-tab-set-44" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-90">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-91" name="sd-tab-set-44" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-91">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-84" name="sd-tab-set-41" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-84">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-92" name="sd-tab-set-45" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-92">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-94" name="sd-tab-set-46" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-94">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-95" name="sd-tab-set-46" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-95">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-93" name="sd-tab-set-45" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-93">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-96" name="sd-tab-set-47" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-96">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-97" name="sd-tab-set-47" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-97">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-85" name="sd-tab-set-41" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-85">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-98" name="sd-tab-set-48" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-98">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-100" name="sd-tab-set-49" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-100">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-101" name="sd-tab-set-49" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-101">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-99" name="sd-tab-set-48" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-99">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-102" name="sd-tab-set-50" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-102">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-103" name="sd-tab-set-50" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-103">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-82" name="sd-tab-set-40" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pcie" for="sd-tab-item-82">
PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-104" name="sd-tab-set-51" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-104">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-107" name="sd-tab-set-52" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-107">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-109" name="sd-tab-set-53" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-109">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-110" name="sd-tab-set-53" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-110">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-108" name="sd-tab-set-52" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-108">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-111" name="sd-tab-set-54" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-111">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-112" name="sd-tab-set-54" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-112">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-105" name="sd-tab-set-51" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-105">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-113" name="sd-tab-set-55" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-113">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-115" name="sd-tab-set-56" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-115">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-116" name="sd-tab-set-56" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-116">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-114" name="sd-tab-set-55" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-114">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-117" name="sd-tab-set-57" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-117">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-118" name="sd-tab-set-57" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-118">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-106" name="sd-tab-set-51" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-106">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-119" name="sd-tab-set-58" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-119">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-121" name="sd-tab-set-59" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-121">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-122" name="sd-tab-set-59" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-122">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-120" name="sd-tab-set-58" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-120">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-123" name="sd-tab-set-60" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-123">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-124" name="sd-tab-set-60" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-124">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-80" name="sd-tab-set-39" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="cas-atomics" for="sd-tab-item-80">
CAS emulation</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-125" name="sd-tab-set-61" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="nopcie" for="sd-tab-item-125">
No PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-127" name="sd-tab-set-62" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-127">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-130" name="sd-tab-set-63" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-130">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-132" name="sd-tab-set-64" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-132">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-133" name="sd-tab-set-64" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-133">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-131" name="sd-tab-set-63" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-131">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-134" name="sd-tab-set-65" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-134">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-135" name="sd-tab-set-65" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-135">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-128" name="sd-tab-set-62" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-128">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-136" name="sd-tab-set-66" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-136">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-138" name="sd-tab-set-67" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-138">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-139" name="sd-tab-set-67" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-139">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-137" name="sd-tab-set-66" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-137">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-140" name="sd-tab-set-68" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-140">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-141" name="sd-tab-set-68" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-141">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-129" name="sd-tab-set-62" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-129">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-142" name="sd-tab-set-69" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-142">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-144" name="sd-tab-set-70" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-144">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-145" name="sd-tab-set-70" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-145">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-143" name="sd-tab-set-69" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-143">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-146" name="sd-tab-set-71" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-146">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-147" name="sd-tab-set-71" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-147">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-126" name="sd-tab-set-61" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pcie" for="sd-tab-item-126">
PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-148" name="sd-tab-set-72" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-148">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-151" name="sd-tab-set-73" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-151">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-153" name="sd-tab-set-74" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-153">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-154" name="sd-tab-set-74" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-154">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-152" name="sd-tab-set-73" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-152">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-155" name="sd-tab-set-75" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-155">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-156" name="sd-tab-set-75" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-156">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-149" name="sd-tab-set-72" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-149">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-157" name="sd-tab-set-76" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-157">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-159" name="sd-tab-set-77" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-159">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-160" name="sd-tab-set-77" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-160">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-158" name="sd-tab-set-76" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-158">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-161" name="sd-tab-set-78" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-161">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-162" name="sd-tab-set-78" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-162">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-150" name="sd-tab-set-72" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-150">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-163" name="sd-tab-set-79" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-163">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-165" name="sd-tab-set-80" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-165">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-166" name="sd-tab-set-80" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-166">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-164" name="sd-tab-set-79" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-164">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-167" name="sd-tab-set-81" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-167">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-168" name="sd-tab-set-81" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-168">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="id2">
<h4>AMD gfx generic targets<a class="headerlink" href="#id2" title="Link to this heading">#</a></h4>
<p>The integer type atomic operations that are supported by different gfx generic
targets listed in the following table.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-169" name="sd-tab-set-82" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="hw-atomics" for="sd-tab-item-169">
HW atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-171" name="sd-tab-set-83" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="nopcie" for="sd-tab-item-171">
No PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-173" name="sd-tab-set-84" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-173">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-176" name="sd-tab-set-85" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-176">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-178" name="sd-tab-set-86" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-178">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-179" name="sd-tab-set-86" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-179">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-177" name="sd-tab-set-85" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-177">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-180" name="sd-tab-set-87" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-180">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-181" name="sd-tab-set-87" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-181">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-174" name="sd-tab-set-84" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-174">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-182" name="sd-tab-set-88" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-182">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-184" name="sd-tab-set-89" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-184">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-185" name="sd-tab-set-89" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-185">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-183" name="sd-tab-set-88" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-183">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-186" name="sd-tab-set-90" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-186">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-187" name="sd-tab-set-90" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-187">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-175" name="sd-tab-set-84" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-175">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-188" name="sd-tab-set-91" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-188">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-190" name="sd-tab-set-92" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-190">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-191" name="sd-tab-set-92" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-191">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-189" name="sd-tab-set-91" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-189">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-192" name="sd-tab-set-93" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-192">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-193" name="sd-tab-set-93" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-193">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-172" name="sd-tab-set-83" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pcie" for="sd-tab-item-172">
PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-194" name="sd-tab-set-94" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-194">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-197" name="sd-tab-set-95" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-197">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-199" name="sd-tab-set-96" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-199">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-200" name="sd-tab-set-96" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-200">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-198" name="sd-tab-set-95" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-198">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-201" name="sd-tab-set-97" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-201">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-202" name="sd-tab-set-97" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-202">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-195" name="sd-tab-set-94" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-195">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-203" name="sd-tab-set-98" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-203">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-205" name="sd-tab-set-99" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-205">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-206" name="sd-tab-set-99" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-206">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-204" name="sd-tab-set-98" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-204">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-207" name="sd-tab-set-100" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-207">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-208" name="sd-tab-set-100" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-208">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-196" name="sd-tab-set-94" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-196">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-209" name="sd-tab-set-101" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-209">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-211" name="sd-tab-set-102" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-211">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-212" name="sd-tab-set-102" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-212">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-210" name="sd-tab-set-101" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-210">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-213" name="sd-tab-set-103" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-213">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-214" name="sd-tab-set-103" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-214">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-170" name="sd-tab-set-82" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="cas-atomics" for="sd-tab-item-170">
CAS emulation</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-215" name="sd-tab-set-104" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="nopcie" for="sd-tab-item-215">
No PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-217" name="sd-tab-set-105" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-217">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-220" name="sd-tab-set-106" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-220">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-222" name="sd-tab-set-107" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-222">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-223" name="sd-tab-set-107" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-223">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-221" name="sd-tab-set-106" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-221">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-224" name="sd-tab-set-108" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-224">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-225" name="sd-tab-set-108" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-225">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-218" name="sd-tab-set-105" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-218">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-226" name="sd-tab-set-109" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-226">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-228" name="sd-tab-set-110" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-228">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-229" name="sd-tab-set-110" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-229">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-227" name="sd-tab-set-109" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-227">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-230" name="sd-tab-set-111" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-230">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-231" name="sd-tab-set-111" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-231">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-219" name="sd-tab-set-105" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-219">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-232" name="sd-tab-set-112" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-232">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-234" name="sd-tab-set-113" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-234">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-235" name="sd-tab-set-113" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-235">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-233" name="sd-tab-set-112" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-233">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-236" name="sd-tab-set-114" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-236">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-237" name="sd-tab-set-114" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-237">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-216" name="sd-tab-set-104" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pcie" for="sd-tab-item-216">
PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-238" name="sd-tab-set-115" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-238">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-241" name="sd-tab-set-116" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-241">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-243" name="sd-tab-set-117" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-243">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-244" name="sd-tab-set-117" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-244">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-242" name="sd-tab-set-116" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-242">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-245" name="sd-tab-set-118" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-245">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-246" name="sd-tab-set-118" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-246">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-239" name="sd-tab-set-115" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-239">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-247" name="sd-tab-set-119" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-247">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-249" name="sd-tab-set-120" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-249">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-250" name="sd-tab-set-120" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-250">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-248" name="sd-tab-set-119" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-248">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-251" name="sd-tab-set-121" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-251">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-252" name="sd-tab-set-121" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-252">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-240" name="sd-tab-set-115" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-240">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-253" name="sd-tab-set-122" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-253">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-255" name="sd-tab-set-123" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-255">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-256" name="sd-tab-set-123" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-256">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-254" name="sd-tab-set-122" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-254">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-257" name="sd-tab-set-124" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-257">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-258" name="sd-tab-set-124" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-258">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicSub</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicInc</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicDec</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="bitwise-atomics-operations">
<h3>Bitwise atomics operations<a class="headerlink" href="#bitwise-atomics-operations" title="Link to this heading">#</a></h3>
<p>The bitwise atomic operations that are supported by different hardware.</p>
<ul class="simple">
<li><p>32 bit bitwise</p>
<ul>
<li><p>Exchange</p></li>
<li><p>Compare-and-Swap (CAS)</p></li>
<li><p>AND</p></li>
<li><p>OR</p></li>
<li><p>XOR</p></li>
</ul>
</li>
<li><p>64 bit bitwise</p>
<ul>
<li><p>Exchange</p></li>
<li><p>CAS</p></li>
<li><p>AND</p></li>
<li><p>OR</p></li>
<li><p>XOR</p></li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>128-bit bitwise Exchange and CAS are not supported on AMD GPUs</p>
</div>
<section id="id3">
<h4>AMD Instinct accelerators<a class="headerlink" href="#id3" title="Link to this heading">#</a></h4>
<p>The bitwise atomic operations that are supported by different AMD Instinct
accelerators listed in the following table.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-259" name="sd-tab-set-125" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="hw-atomics" for="sd-tab-item-259">
HW atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-261" name="sd-tab-set-126" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="nopcie" for="sd-tab-item-261">
No PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-263" name="sd-tab-set-127" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-263">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-266" name="sd-tab-set-128" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-266">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-268" name="sd-tab-set-129" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-268">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-269" name="sd-tab-set-129" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-269">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-267" name="sd-tab-set-128" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-267">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-270" name="sd-tab-set-130" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-270">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-271" name="sd-tab-set-130" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-271">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-264" name="sd-tab-set-127" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-264">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-272" name="sd-tab-set-131" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-272">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-274" name="sd-tab-set-132" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-274">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-275" name="sd-tab-set-132" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-275">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-273" name="sd-tab-set-131" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-273">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-276" name="sd-tab-set-133" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-276">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-277" name="sd-tab-set-133" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-277">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-265" name="sd-tab-set-127" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-265">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-278" name="sd-tab-set-134" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-278">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-280" name="sd-tab-set-135" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-280">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-281" name="sd-tab-set-135" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-281">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-279" name="sd-tab-set-134" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-279">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-282" name="sd-tab-set-136" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-282">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-283" name="sd-tab-set-136" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-283">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-262" name="sd-tab-set-126" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pcie" for="sd-tab-item-262">
PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-284" name="sd-tab-set-137" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-284">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-287" name="sd-tab-set-138" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-287">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-289" name="sd-tab-set-139" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-289">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-290" name="sd-tab-set-139" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-290">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-288" name="sd-tab-set-138" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-288">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-291" name="sd-tab-set-140" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-291">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-292" name="sd-tab-set-140" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-292">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-285" name="sd-tab-set-137" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-285">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-293" name="sd-tab-set-141" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-293">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-295" name="sd-tab-set-142" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-295">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-296" name="sd-tab-set-142" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-296">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-294" name="sd-tab-set-141" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-294">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-297" name="sd-tab-set-143" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-297">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-298" name="sd-tab-set-143" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-298">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-286" name="sd-tab-set-137" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-286">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-299" name="sd-tab-set-144" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-299">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-301" name="sd-tab-set-145" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-301">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-302" name="sd-tab-set-145" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-302">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-300" name="sd-tab-set-144" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-300">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-303" name="sd-tab-set-146" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-303">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-304" name="sd-tab-set-146" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-304">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-260" name="sd-tab-set-125" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="cas-atomics" for="sd-tab-item-260">
CAS emulation</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-305" name="sd-tab-set-147" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="nopcie" for="sd-tab-item-305">
No PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-307" name="sd-tab-set-148" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-307">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-310" name="sd-tab-set-149" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-310">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-312" name="sd-tab-set-150" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-312">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-313" name="sd-tab-set-150" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-313">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-311" name="sd-tab-set-149" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-311">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-314" name="sd-tab-set-151" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-314">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-315" name="sd-tab-set-151" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-315">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-308" name="sd-tab-set-148" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-308">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-316" name="sd-tab-set-152" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-316">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-318" name="sd-tab-set-153" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-318">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-319" name="sd-tab-set-153" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-319">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-317" name="sd-tab-set-152" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-317">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-320" name="sd-tab-set-154" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-320">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-321" name="sd-tab-set-154" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-321">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-309" name="sd-tab-set-148" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-309">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-322" name="sd-tab-set-155" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-322">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-324" name="sd-tab-set-156" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-324">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-325" name="sd-tab-set-156" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-325">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-323" name="sd-tab-set-155" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-323">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-326" name="sd-tab-set-157" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-326">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-327" name="sd-tab-set-157" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-327">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-306" name="sd-tab-set-147" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pcie" for="sd-tab-item-306">
PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-328" name="sd-tab-set-158" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-328">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-331" name="sd-tab-set-159" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-331">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-333" name="sd-tab-set-160" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-333">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-334" name="sd-tab-set-160" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-334">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-332" name="sd-tab-set-159" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-332">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-335" name="sd-tab-set-161" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-335">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-336" name="sd-tab-set-161" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-336">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-329" name="sd-tab-set-158" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-329">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-337" name="sd-tab-set-162" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-337">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-339" name="sd-tab-set-163" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-339">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-340" name="sd-tab-set-163" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-340">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-338" name="sd-tab-set-162" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-338">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-341" name="sd-tab-set-164" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-341">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-342" name="sd-tab-set-164" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-342">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-330" name="sd-tab-set-158" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-330">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-343" name="sd-tab-set-165" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-343">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-345" name="sd-tab-set-166" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-345">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-346" name="sd-tab-set-166" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-346">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-344" name="sd-tab-set-165" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-344">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-347" name="sd-tab-set-167" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-347">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-348" name="sd-tab-set-167" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-348">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="id4">
<h4>AMD gfx generic targets<a class="headerlink" href="#id4" title="Link to this heading">#</a></h4>
<p>The bitwise atomic operations that are supported by different AMD gfx generic
targets listed in the following table.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-349" name="sd-tab-set-168" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="hw-atomics" for="sd-tab-item-349">
HW atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-351" name="sd-tab-set-169" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="nopcie" for="sd-tab-item-351">
No PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-353" name="sd-tab-set-170" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-353">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-356" name="sd-tab-set-171" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-356">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-358" name="sd-tab-set-172" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-358">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-359" name="sd-tab-set-172" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-359">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-357" name="sd-tab-set-171" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-357">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-360" name="sd-tab-set-173" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-360">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-361" name="sd-tab-set-173" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-361">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-354" name="sd-tab-set-170" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-354">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-362" name="sd-tab-set-174" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-362">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-364" name="sd-tab-set-175" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-364">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-365" name="sd-tab-set-175" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-365">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-363" name="sd-tab-set-174" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-363">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-366" name="sd-tab-set-176" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-366">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-367" name="sd-tab-set-176" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-367">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-355" name="sd-tab-set-170" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-355">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-368" name="sd-tab-set-177" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-368">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-370" name="sd-tab-set-178" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-370">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-371" name="sd-tab-set-178" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-371">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-369" name="sd-tab-set-177" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-369">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-372" name="sd-tab-set-179" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-372">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-373" name="sd-tab-set-179" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-373">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-352" name="sd-tab-set-169" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pcie" for="sd-tab-item-352">
PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-374" name="sd-tab-set-180" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-374">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-377" name="sd-tab-set-181" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-377">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-379" name="sd-tab-set-182" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-379">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-380" name="sd-tab-set-182" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-380">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-378" name="sd-tab-set-181" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-378">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-381" name="sd-tab-set-183" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-381">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-382" name="sd-tab-set-183" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-382">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-375" name="sd-tab-set-180" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-375">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-383" name="sd-tab-set-184" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-383">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-385" name="sd-tab-set-185" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-385">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-386" name="sd-tab-set-185" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-386">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-384" name="sd-tab-set-184" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-384">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-387" name="sd-tab-set-186" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-387">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-388" name="sd-tab-set-186" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-388">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-376" name="sd-tab-set-180" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-376">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-389" name="sd-tab-set-187" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-389">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-391" name="sd-tab-set-188" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-391">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-392" name="sd-tab-set-188" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-392">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-390" name="sd-tab-set-187" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-390">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-393" name="sd-tab-set-189" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-393">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-394" name="sd-tab-set-189" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-394">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-350" name="sd-tab-set-168" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="cas-atomics" for="sd-tab-item-350">
CAS emulation</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-395" name="sd-tab-set-190" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="nopcie" for="sd-tab-item-395">
No PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-397" name="sd-tab-set-191" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-397">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-400" name="sd-tab-set-192" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-400">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-402" name="sd-tab-set-193" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-402">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-403" name="sd-tab-set-193" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-403">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-401" name="sd-tab-set-192" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-401">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-404" name="sd-tab-set-194" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-404">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-405" name="sd-tab-set-194" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-405">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-398" name="sd-tab-set-191" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-398">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-406" name="sd-tab-set-195" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-406">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-408" name="sd-tab-set-196" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-408">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-409" name="sd-tab-set-196" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-409">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-407" name="sd-tab-set-195" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-407">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-410" name="sd-tab-set-197" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-410">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-411" name="sd-tab-set-197" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-411">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-399" name="sd-tab-set-191" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-399">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-412" name="sd-tab-set-198" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-412">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-414" name="sd-tab-set-199" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-414">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-415" name="sd-tab-set-199" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-415">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-413" name="sd-tab-set-198" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-413">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-416" name="sd-tab-set-200" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-416">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-417" name="sd-tab-set-200" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-417">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-396" name="sd-tab-set-190" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pcie" for="sd-tab-item-396">
PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-418" name="sd-tab-set-201" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-418">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-421" name="sd-tab-set-202" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-421">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-423" name="sd-tab-set-203" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-423">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-424" name="sd-tab-set-203" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-424">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-422" name="sd-tab-set-202" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-422">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-425" name="sd-tab-set-204" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-425">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-426" name="sd-tab-set-204" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-426">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-419" name="sd-tab-set-201" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-419">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-427" name="sd-tab-set-205" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-427">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-429" name="sd-tab-set-206" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-429">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-430" name="sd-tab-set-206" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-430">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-428" name="sd-tab-set-205" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-428">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-431" name="sd-tab-set-207" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-431">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-432" name="sd-tab-set-207" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-432">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-420" name="sd-tab-set-201" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-420">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-433" name="sd-tab-set-208" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-433">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-435" name="sd-tab-set-209" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-435">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-436" name="sd-tab-set-209" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-436">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-434" name="sd-tab-set-208" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-434">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-437" name="sd-tab-set-210" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-437">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-438" name="sd-tab-set-210" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-438">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atoimcExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicExch</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicCAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicAnd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit atomicOr</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit atomicXor</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="float-atomics-operations">
<h3>Float atomics operations<a class="headerlink" href="#float-atomics-operations" title="Link to this heading">#</a></h3>
<p>The float types atomic operations that are supported by different hardware.</p>
<ul class="simple">
<li><p>32-bit IEEE 754 floating point (‘single precision’, FP32)</p>
<ul>
<li><p>Add</p></li>
<li><p>Min</p></li>
<li><p>Max</p></li>
</ul>
</li>
<li><p>64-bit IEEE 754 floating point (‘double precision’, FP64)</p>
<ul>
<li><p>Add</p></li>
<li><p>Min</p></li>
<li><p>Max</p></li>
</ul>
</li>
<li><p>16-bit IEEE 754 floating point (‘half precision”, FP16)</p>
<ul>
<li><p>Add</p></li>
</ul>
</li>
<li><p>2xPacked 16-bit IEEE 754 floating point (‘half precision’, FP16)</p>
<ul>
<li><p>Add</p></li>
</ul>
</li>
<li><p>BrainFloat-16 floating point (BF16)</p>
<ul>
<li><p>Add</p></li>
</ul>
</li>
<li><p>2xPacked BrainFloat-16 floating point (BF16)</p>
<ul>
<li><p>Add</p></li>
</ul>
</li>
</ul>
<section id="id5">
<h4>AMD Instinct accelerators<a class="headerlink" href="#id5" title="Link to this heading">#</a></h4>
<p>The float type atomic operations that are supported by different AMD Instinct
accelerators listed in the following table.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-439" name="sd-tab-set-211" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="hw-atomics" for="sd-tab-item-439">
HW atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-441" name="sd-tab-set-212" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="nopcie" for="sd-tab-item-441">
No PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-443" name="sd-tab-set-213" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-443">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-446" name="sd-tab-set-214" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-446">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-448" name="sd-tab-set-215" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-448">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-449" name="sd-tab-set-215" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-449">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-447" name="sd-tab-set-214" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-447">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-450" name="sd-tab-set-216" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-450">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-451" name="sd-tab-set-216" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-451">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-444" name="sd-tab-set-213" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-444">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-452" name="sd-tab-set-217" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-452">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-454" name="sd-tab-set-218" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-454">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-455" name="sd-tab-set-218" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-455">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-453" name="sd-tab-set-217" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-453">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-456" name="sd-tab-set-219" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-456">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-457" name="sd-tab-set-219" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-457">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-445" name="sd-tab-set-213" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-445">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-458" name="sd-tab-set-220" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-458">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-460" name="sd-tab-set-221" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-460">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-461" name="sd-tab-set-221" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-461">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-459" name="sd-tab-set-220" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-459">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-462" name="sd-tab-set-222" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-462">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-463" name="sd-tab-set-222" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-463">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-442" name="sd-tab-set-212" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pcie" for="sd-tab-item-442">
PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-464" name="sd-tab-set-223" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-464">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-467" name="sd-tab-set-224" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-467">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-469" name="sd-tab-set-225" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-469">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-470" name="sd-tab-set-225" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-470">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-468" name="sd-tab-set-224" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-468">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-471" name="sd-tab-set-226" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-471">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-472" name="sd-tab-set-226" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-472">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-465" name="sd-tab-set-223" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-465">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-473" name="sd-tab-set-227" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-473">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-475" name="sd-tab-set-228" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-475">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-476" name="sd-tab-set-228" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-476">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-474" name="sd-tab-set-227" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-474">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-477" name="sd-tab-set-229" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-477">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-478" name="sd-tab-set-229" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-478">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-466" name="sd-tab-set-223" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-466">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-479" name="sd-tab-set-230" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-479">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-481" name="sd-tab-set-231" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-481">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-482" name="sd-tab-set-231" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-482">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ NoReturn</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-480" name="sd-tab-set-230" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-480">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-483" name="sd-tab-set-232" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-483">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-484" name="sd-tab-set-232" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-484">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-440" name="sd-tab-set-211" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="cas-atomics" for="sd-tab-item-440">
CAS emulation</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-485" name="sd-tab-set-233" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="nopcie" for="sd-tab-item-485">
No PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-487" name="sd-tab-set-234" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-487">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-490" name="sd-tab-set-235" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-490">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-492" name="sd-tab-set-236" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-492">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-493" name="sd-tab-set-236" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-493">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-491" name="sd-tab-set-235" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-491">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-494" name="sd-tab-set-237" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-494">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-495" name="sd-tab-set-237" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-495">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-488" name="sd-tab-set-234" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-488">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-496" name="sd-tab-set-238" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-496">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-498" name="sd-tab-set-239" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-498">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-499" name="sd-tab-set-239" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-499">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-497" name="sd-tab-set-238" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-497">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-500" name="sd-tab-set-240" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-500">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-501" name="sd-tab-set-240" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-501">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-489" name="sd-tab-set-234" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-489">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-502" name="sd-tab-set-241" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-502">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-504" name="sd-tab-set-242" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-504">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-505" name="sd-tab-set-242" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-505">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-503" name="sd-tab-set-241" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-503">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-506" name="sd-tab-set-243" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-506">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-507" name="sd-tab-set-243" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-507">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-486" name="sd-tab-set-233" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pcie" for="sd-tab-item-486">
PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-508" name="sd-tab-set-244" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-508">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-511" name="sd-tab-set-245" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-511">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-513" name="sd-tab-set-246" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-513">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-514" name="sd-tab-set-246" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-514">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-512" name="sd-tab-set-245" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-512">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-515" name="sd-tab-set-247" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-515">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-516" name="sd-tab-set-247" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-516">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-509" name="sd-tab-set-244" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-509">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-517" name="sd-tab-set-248" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-517">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-519" name="sd-tab-set-249" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-519">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-520" name="sd-tab-set-249" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-520">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-518" name="sd-tab-set-248" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-518">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-521" name="sd-tab-set-250" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-521">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-522" name="sd-tab-set-250" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-522">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-510" name="sd-tab-set-244" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-510">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-523" name="sd-tab-set-251" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-523">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-525" name="sd-tab-set-252" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-525">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-526" name="sd-tab-set-252" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-526">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-524" name="sd-tab-set-251" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-524">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-527" name="sd-tab-set-253" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-527">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-528" name="sd-tab-set-253" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-528">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI100</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 PCIe</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI200 A+A</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300X</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">MI300A</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="id6">
<h4>AMD gfx generic targets<a class="headerlink" href="#id6" title="Link to this heading">#</a></h4>
<p>The float types atomic operations that are supported by different AMD gfx
generic targets listed in the following table.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-529" name="sd-tab-set-254" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="hw-atomics" for="sd-tab-item-529">
HW atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-531" name="sd-tab-set-255" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="nopcie" for="sd-tab-item-531">
No PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-533" name="sd-tab-set-256" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-533">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-536" name="sd-tab-set-257" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-536">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-538" name="sd-tab-set-258" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-538">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-539" name="sd-tab-set-258" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-539">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-537" name="sd-tab-set-257" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-537">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-540" name="sd-tab-set-259" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-540">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-541" name="sd-tab-set-259" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-541">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-534" name="sd-tab-set-256" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-534">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-542" name="sd-tab-set-260" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-542">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-544" name="sd-tab-set-261" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-544">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-545" name="sd-tab-set-261" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-545">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-543" name="sd-tab-set-260" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-543">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-546" name="sd-tab-set-262" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-546">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-547" name="sd-tab-set-262" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-547">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-535" name="sd-tab-set-256" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-535">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-548" name="sd-tab-set-263" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-548">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-550" name="sd-tab-set-264" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-550">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">n/a</div>
</div>
</td>
<td><div class="line-block">
<div class="line">n/a</div>
</div>
</td>
<td><div class="line-block">
<div class="line">n/a</div>
</div>
</td>
<td><div class="line-block">
<div class="line">n/a</div>
</div>
</td>
<td><div class="line-block">
<div class="line">n/a</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-551" name="sd-tab-set-264" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-551">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-549" name="sd-tab-set-263" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-549">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-552" name="sd-tab-set-265" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-552">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-553" name="sd-tab-set-265" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-553">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-532" name="sd-tab-set-255" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pcie" for="sd-tab-item-532">
PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-554" name="sd-tab-set-266" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-554">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-557" name="sd-tab-set-267" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-557">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-559" name="sd-tab-set-268" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-559">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-560" name="sd-tab-set-268" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-560">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-558" name="sd-tab-set-267" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-558">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-561" name="sd-tab-set-269" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-561">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-562" name="sd-tab-set-269" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-562">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-555" name="sd-tab-set-266" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-555">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-563" name="sd-tab-set-270" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-563">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-565" name="sd-tab-set-271" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-565">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-566" name="sd-tab-set-271" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-566">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-564" name="sd-tab-set-270" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-564">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-567" name="sd-tab-set-272" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-567">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-568" name="sd-tab-set-272" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-568">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-556" name="sd-tab-set-266" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-556">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-569" name="sd-tab-set-273" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-569">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-571" name="sd-tab-set-274" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-571">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-572" name="sd-tab-set-274" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-572">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-570" name="sd-tab-set-273" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-570">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-573" name="sd-tab-set-275" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-573">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ Native</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-574" name="sd-tab-set-275" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-574">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-530" name="sd-tab-set-254" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="cas-atomics" for="sd-tab-item-530">
CAS emulation</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-575" name="sd-tab-set-276" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="nopcie" for="sd-tab-item-575">
No PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-577" name="sd-tab-set-277" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-577">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-580" name="sd-tab-set-278" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-580">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-582" name="sd-tab-set-279" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-582">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-583" name="sd-tab-set-279" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-583">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-581" name="sd-tab-set-278" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-581">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-584" name="sd-tab-set-280" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-584">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-585" name="sd-tab-set-280" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-585">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-578" name="sd-tab-set-277" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-578">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-586" name="sd-tab-set-281" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-586">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-588" name="sd-tab-set-282" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-588">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-589" name="sd-tab-set-282" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-589">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-587" name="sd-tab-set-281" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-587">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-590" name="sd-tab-set-283" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-590">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-591" name="sd-tab-set-283" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-591">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-579" name="sd-tab-set-277" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-579">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-592" name="sd-tab-set-284" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-592">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-594" name="sd-tab-set-285" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-594">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-595" name="sd-tab-set-285" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-595">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-593" name="sd-tab-set-284" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-593">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-596" name="sd-tab-set-286" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-596">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-597" name="sd-tab-set-286" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-597">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">❌ NOP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">⚠️ Scope Downgrade - CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-576" name="sd-tab-set-276" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pcie" for="sd-tab-item-576">
PCIe atomics</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-598" name="sd-tab-set-287" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device-dram" for="sd-tab-item-598">
Device DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-601" name="sd-tab-set-288" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-601">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-603" name="sd-tab-set-289" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-603">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-604" name="sd-tab-set-289" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-604">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-602" name="sd-tab-set-288" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-602">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-605" name="sd-tab-set-290" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-605">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-606" name="sd-tab-set-290" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-606">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-599" name="sd-tab-set-287" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="migratable-host-dram" for="sd-tab-item-599">
Migratable Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-607" name="sd-tab-set-291" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-607">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-609" name="sd-tab-set-292" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-609">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-610" name="sd-tab-set-292" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-610">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-608" name="sd-tab-set-291" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-608">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-611" name="sd-tab-set-293" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-611">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-612" name="sd-tab-set-293" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-612">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-600" name="sd-tab-set-287" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="pinned-host-dram" for="sd-tab-item-600">
Pinned Host DRAM</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-613" name="sd-tab-set-294" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="coarse-grained" for="sd-tab-item-613">
Coarse-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-615" name="sd-tab-set-295" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-615">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-616" name="sd-tab-set-295" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-616">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<input id="sd-tab-item-614" name="sd-tab-set-294" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="fine-grained" for="sd-tab-item-614">
Fine-grained</label><div class="sd-tab-content docutils">
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-617" name="sd-tab-set-296" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="device" for="sd-tab-item-617">
Device</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-618" name="sd-tab-set-296" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="system" for="sd-tab-item-618">
System</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">﻿Atomic</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx9 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx10 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 dGPU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx11 APU</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">gfx12 dGPU</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">32 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">32 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">64 bit float atomicMin</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">64 bit float atomicMax</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">16bx2 half2 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">16bx2 bfloat162 atomicAdd</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
<td><div class="line-block">
<div class="line">✅ CAS</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
</section>
<span id="document-reference/precision-support"></span><section id="data-types-and-precision-support">
<h2>Data types and precision support<a class="headerlink" href="#data-types-and-precision-support" title="Link to this heading">#</a></h2>
<p>This topic lists the data types support on AMD GPUs, ROCm libraries along
with corresponding <a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/index.html" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-doc">HIP</span></a> data types.</p>
<section id="integral-types">
<h3>Integral types<a class="headerlink" href="#integral-types" title="Link to this heading">#</a></h3>
<p>The signed and unsigned integral types supported by ROCm are listed in
the following table.</p>
<div class="pst-scrollable-table-container"><table class="table">
<colgroup>
<col style="width: 15.0%"/>
<col style="width: 35.0%"/>
<col style="width: 50.0%"/>
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Type name</p></th>
<th class="head"><p>HIP type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>int8</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int8_t</span></code>, <code class="docutils literal notranslate"><span class="pre">uint8_t</span></code></p></td>
<td><p>A signed or unsigned 8-bit integer</p></td>
</tr>
<tr class="row-odd"><td><p>int16</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int16_t</span></code>, <code class="docutils literal notranslate"><span class="pre">uint16_t</span></code></p></td>
<td><p>A signed or unsigned 16-bit integer</p></td>
</tr>
<tr class="row-even"><td><p>int32</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int32_t</span></code>, <code class="docutils literal notranslate"><span class="pre">uint32_t</span></code></p></td>
<td><p>A signed or unsigned 32-bit integer</p></td>
</tr>
<tr class="row-odd"><td><p>int64</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int64_t</span></code>, <code class="docutils literal notranslate"><span class="pre">uint64_t</span></code></p></td>
<td><p>A signed or unsigned 64-bit integer</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="floating-point-types">
<span id="precision-support-floating-point-types"></span><h3>Floating-point types<a class="headerlink" href="#floating-point-types" title="Link to this heading">#</a></h3>
<p>The floating-point types supported by ROCm are listed in the following table.</p>
<img alt="Supported floating-point types" src="_images/floating-point-data-types.png"/>
<div class="pst-scrollable-table-container"><table class="table">
<colgroup>
<col style="width: 15.0%"/>
<col style="width: 25.0%"/>
<col style="width: 60.0%"/>
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Type name</p></th>
<th class="head"><p>HIP type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>float8 (E4M3)</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">__hip_fp8_e4m3_fnuz</span></code>,</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">__hip_fp8_e4m3</span></code></div>
</div>
</td>
<td><p>An 8-bit floating-point number with <strong>S1E4M3</strong> bit layout, as described in <a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/reference/low_fp_types.html" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-doc">low precision floating point types page</span></a>.
The FNUZ variant has expanded range with no infinity or signed zero (NaN represented as negative zero),
while the OCP variant follows the Open Compute Project specification.</p></td>
</tr>
<tr class="row-odd"><td><p>float8 (E5M2)</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">__hip_fp8_e5m2_fnuz</span></code>,</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">__hip_fp8_e5m2</span></code></div>
</div>
</td>
<td><p>An 8-bit floating-point number with <strong>S1E5M2</strong> bit layout, as described in <a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/reference/low_fp_types.html" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-doc">low precision floating point types page</span></a>.
The FNUZ variant has expanded range with no infinity or signed zero (NaN represented as negative zero),
while the OCP variant follows the Open Compute Project specification.</p></td>
</tr>
<tr class="row-even"><td><p>float16</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">half</span></code></p></td>
<td><p>A 16-bit floating-point number that conforms to the IEEE 754-2008
half-precision storage format.</p></td>
</tr>
<tr class="row-odd"><td><p>bfloat16</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bfloat16</span></code></p></td>
<td><p>A shortened 16-bit version of the IEEE 754 single-precision storage
format.</p></td>
</tr>
<tr class="row-even"><td><p>tensorfloat32</p></td>
<td><p>Not available</p></td>
<td><p>A floating-point number that occupies 32 bits or less of storage,
providing improved range compared to half (16-bit) format, at
(potentially) greater throughput than single-precision (32-bit) formats.</p></td>
</tr>
<tr class="row-odd"><td><p>float32</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">float</span></code></p></td>
<td><p>A 32-bit floating-point number that conforms to the IEEE 754
single-precision storage format.</p></td>
</tr>
<tr class="row-even"><td><p>float64</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">double</span></code></p></td>
<td><p>A 64-bit floating-point number that conforms to the IEEE 754
double-precision storage format.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>The float8 and tensorfloat32 types are internal types used in calculations
in Matrix Cores and can be stored in any type of the same size.</p></li>
<li><p>CNDA3 natively supports FP8 FNUZ (E4M3 and E5M2), which differs from the customised
FP8 format used in NVIDIA’s H100
(<a class="reference external" href="https://arxiv.org/abs/2209.05433">FP8 Formats for Deep Learning</a>).</p></li>
<li><p>In some AMD documents and articles, float8 (E5M2) is referred to as bfloat8.</p></li>
<li><p>The <a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/reference/low_fp_types.html" title="(in HIP Documentation v6.4.43484)"><span class="xref std std-doc">low precision floating point types page</span></a>
describes how to use these types in HIP with examples.</p></li>
</ul>
</div>
</section>
<section id="level-of-support-definitions">
<h3>Level of support definitions<a class="headerlink" href="#level-of-support-definitions" title="Link to this heading">#</a></h3>
<p>In the following sections, icons represent the level of support. These icons,
described in the following table, are also used in the library data type support
pages.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Icon</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>NA</p></td>
<td><p>Not applicable</p></td>
</tr>
<tr class="row-odd"><td><p>❌</p></td>
<td><p>Not supported</p></td>
</tr>
<tr class="row-even"><td><p>⚠️</p></td>
<td><p>Partial support</p></td>
</tr>
<tr class="row-odd"><td><p>✅</p></td>
<td><p>Full support</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Full support means that the type is supported natively or with hardware
emulation.</p></li>
<li><p>Native support means that the operations for that type are implemented in
hardware. Types that are not natively supported are emulated with the
available hardware. The performance of non-natively supported types can
differ from the full instruction throughput rate. For example, 16-bit
integer operations can be performed on the 32-bit integer ALUs at full rate;
however, 64-bit integer operations might need several instructions on the
32-bit integer ALUs.</p></li>
<li><p>Any type can be emulated by software, but this page does not cover such
cases.</p></li>
</ul>
</div>
</section>
<section id="data-type-support-by-hardware-architecture">
<h3>Data type support by hardware architecture<a class="headerlink" href="#data-type-support-by-hardware-architecture" title="Link to this heading">#</a></h3>
<p>AMD’s GPU lineup spans multiple architecture generations:</p>
<ul class="simple">
<li><p>CDNA1 architecture: includes models such as MI100</p></li>
<li><p>CDNA2 architecture: includes models such as MI210, MI250, and MI250X</p></li>
<li><p>CDNA3 architecture: includes models such as MI300A, MI300X, and MI325X</p></li>
<li><p>RDNA3 architecture: includes models such as RX 7900XT and RX 7900XTX</p></li>
<li><p>RDNA4 architecture: includes models such as RX 9070 and RX 9070XT</p></li>
</ul>
<section id="hip-c-type-implementation-support">
<h4>HIP C++ type implementation support<a class="headerlink" href="#hip-c-type-implementation-support" title="Link to this heading">#</a></h4>
<p>The HIP C++ types available on different hardware platforms are listed in the
following table.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>HIP C++ Type</p></th>
<th class="head"><p>CDNA1</p></th>
<th class="head"><p>CDNA2</p></th>
<th class="head"><p>CDNA3</p></th>
<th class="head"><p>RDNA3</p></th>
<th class="head"><p>RDNA4</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">int8_t</span></code>, <code class="docutils literal notranslate"><span class="pre">uint8_t</span></code></p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">int16_t</span></code>, <code class="docutils literal notranslate"><span class="pre">uint16_t</span></code></p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">int32_t</span></code>, <code class="docutils literal notranslate"><span class="pre">uint32_t</span></code></p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">int64_t</span></code>, <code class="docutils literal notranslate"><span class="pre">uint64_t</span></code></p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">__hip_fp8_e4m3_fnuz</span></code></p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">__hip_fp8_e5m2_fnuz</span></code></p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">__hip_fp8_e4m3</span></code></p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">__hip_fp8_e5m2</span></code></p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">half</span></code></p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">bfloat16</span></code></p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">float</span></code></p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">double</span></code></p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Library support for specific data types is contingent upon hardware support.
Even if a ROCm library indicates support for a particular data type, that type
will only be fully functional if the underlying hardware architecture (as shown
in the table above) also supports it. For example, fp8 types are only available
on architectures shown with a checkmark in the relevant rows.</p>
</div>
</section>
<section id="compute-units-support">
<h4>Compute units support<a class="headerlink" href="#compute-units-support" title="Link to this heading">#</a></h4>
<p>The following table lists data type support for compute units.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-619" name="sd-tab-set-297" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="integral-type" for="sd-tab-item-619">
Integral types</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Type name</p></th>
<th class="head"><p>int8</p></th>
<th class="head"><p>int16</p></th>
<th class="head"><p>int32</p></th>
<th class="head"><p>int64</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>CDNA1</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p>CDNA2</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p>CDNA3</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p>RDNA3</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p>RDNA4</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-620" name="sd-tab-set-297" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="floating-point-type" for="sd-tab-item-620">
Floating-point types</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Type name</p></th>
<th class="head"><p>float8 (E4M3)</p></th>
<th class="head"><p>float8 (E5M2)</p></th>
<th class="head"><p>float16</p></th>
<th class="head"><p>bfloat16</p></th>
<th class="head"><p>tensorfloat32</p></th>
<th class="head"><p>float32</p></th>
<th class="head"><p>float64</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>CDNA1</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p>CDNA2</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p>CDNA3</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p>RDNA3</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p>RDNA4</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section id="matrix-core-support">
<h4>Matrix core support<a class="headerlink" href="#matrix-core-support" title="Link to this heading">#</a></h4>
<p>The following table lists data type support for AMD GPU matrix cores.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-621" name="sd-tab-set-298" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="integral-type" for="sd-tab-item-621">
Integral types</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Type name</p></th>
<th class="head"><p>int8</p></th>
<th class="head"><p>int16</p></th>
<th class="head"><p>int32</p></th>
<th class="head"><p>int64</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>CDNA1</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-odd"><td><p>CDNA2</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-even"><td><p>CDNA3</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-odd"><td><p>RDNA3</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-even"><td><p>RDNA4</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-622" name="sd-tab-set-298" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="floating-point-type" for="sd-tab-item-622">
Floating-point types</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Type name</p></th>
<th class="head"><p>float8 (E4M3)</p></th>
<th class="head"><p>float8 (E5M2)</p></th>
<th class="head"><p>float16</p></th>
<th class="head"><p>bfloat16</p></th>
<th class="head"><p>tensorfloat32</p></th>
<th class="head"><p>float32</p></th>
<th class="head"><p>float64</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>CDNA1</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-odd"><td><p>CDNA2</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p>CDNA3</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p>RDNA3</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-even"><td><p>RDNA4</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section id="atomic-operations-support">
<h4>Atomic operations support<a class="headerlink" href="#atomic-operations-support" title="Link to this heading">#</a></h4>
<p>The following table lists which data types are supported for atomic
operations on AMD GPUs. The atomics operation type behavior is affected by the
memory locations, memory granularity, or scope of operations. For detailed
various support of atomic read-modify-write (atomicRMW) operations collected on
the <a class="reference internal" href="#hw-atomics-operation-support"><span class="std std-ref">Hardware atomics operation support</span></a>
page.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-623" name="sd-tab-set-299" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="integral-type" for="sd-tab-item-623">
Integral types</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Type name</p></th>
<th class="head"><p>int8</p></th>
<th class="head"><p>int16</p></th>
<th class="head"><p>int32</p></th>
<th class="head"><p>int64</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>CDNA1</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p>CDNA2</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p>CDNA3</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p>RDNA3</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p>RDNA4</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-624" name="sd-tab-set-299" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="floating-point-type" for="sd-tab-item-624">
Floating-point types</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Type name</p></th>
<th class="head"><p>float8 (E4M3)</p></th>
<th class="head"><p>float8 (E5M2)</p></th>
<th class="head"><p>2 x float16</p></th>
<th class="head"><p>2 x bfloat16</p></th>
<th class="head"><p>tensorfloat32</p></th>
<th class="head"><p>float32</p></th>
<th class="head"><p>float64</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>CDNA1</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-odd"><td><p>CDNA2</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p>CDNA3</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p>RDNA3</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-even"><td><p>RDNA4</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can emulate atomic operations using software for cases that are not
natively supported. Software-emulated atomic operations have a high negative
performance impact when they frequently access the same memory address.</p>
</div>
</section>
</section>
<section id="data-type-support-in-rocm-libraries">
<h3>Data type support in ROCm libraries<a class="headerlink" href="#data-type-support-in-rocm-libraries" title="Link to this heading">#</a></h3>
<p>ROCm library support for int8, float8 (E4M3), float8 (E5M2), int16, float16,
bfloat16, int32, tensorfloat32, float32, int64, and float64 is listed in the
following tables.</p>
<section id="libraries-input-output-type-support">
<h4>Libraries input/output type support<a class="headerlink" href="#libraries-input-output-type-support" title="Link to this heading">#</a></h4>
<p>The following tables list ROCm library support for specific input and output
data types. Refer to the corresponding library data type support page for a
detailed description.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-625" name="sd-tab-set-300" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="integral-type" for="sd-tab-item-625">
Integral types</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Library input/output data type name</p></th>
<th class="head"><p>int8</p></th>
<th class="head"><p>int16</p></th>
<th class="head"><p>int32</p></th>
<th class="head"><p>int64</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/composable_kernel/en/latest/reference/Composable_Kernel_supported_scalar_types.html" title="(in Composable Kernel Documentation v1.1.0)"><span class="xref std std-doc">Composable Kernel</span></a></p></td>
<td><p>✅/✅</p></td>
<td><p>❌/❌</p></td>
<td><p>✅/✅</p></td>
<td><p>❌/❌</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipCUB/en/latest/api-reference/data-type-support.html" title="(in hipCUB Documentation v3.4.0)"><span class="xref std std-doc">hipCUB</span></a></p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipRAND/en/latest/api-reference/data-type-support.html" title="(in hipRAND Documentation v2.12.0)"><span class="xref std std-doc">hipRAND</span></a></p></td>
<td><p>NA/✅</p></td>
<td><p>NA/✅</p></td>
<td><p>NA/✅</p></td>
<td><p>NA/✅</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipSOLVER/en/latest/reference/precision.html" title="(in hipSOLVER Documentation v2.4.0)"><span class="xref std std-doc">hipSOLVER</span></a></p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipSPARSELt/en/latest/reference/data-type-support.html" title="(in hipSPARSELt Documentation v0.2.3)"><span class="xref std std-doc">hipSPARSELt</span></a></p></td>
<td><p>✅/✅</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipTensor/en/latest/api-reference/api-reference.html" title="(in hipTensor Documentation v1.5.0)"><span class="xref std std-doc">hipTensor</span></a></p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/AMDMIGraphX/en/latest/reference/cpp.html" title="(in MIGraphX v2.12.0)"><span class="xref std std-doc">MIGraphX</span></a></p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/MIOpen/en/latest/reference/datatypes.html" title="(in MIOpen Documentation v3.4.0)"><span class="xref std std-doc">MIOpen</span></a></p></td>
<td><p>⚠️/⚠️</p></td>
<td><p>❌/❌</p></td>
<td><p>⚠️/⚠️</p></td>
<td><p>❌/❌</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rccl/en/latest/api-reference/library-specification.html" title="(in RCCL Documentation v2.22.3)"><span class="xref std std-doc">RCCL</span></a></p></td>
<td><p>✅/✅</p></td>
<td><p>❌/❌</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocFFT/en/latest/reference/api.html" title="(in rocFFT Documentation v1.0.32)"><span class="xref std std-doc">rocFFT</span></a></p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocPRIM/en/latest/reference/data-type-support.html" title="(in rocPRIM Documentation v3.4.1)"><span class="xref std std-doc">rocPRIM</span></a></p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocRAND/en/latest/api-reference/data-type-support.html" title="(in rocRAND Documentation v3.3.0)"><span class="xref std std-doc">rocRAND</span></a></p></td>
<td><p>NA/✅</p></td>
<td><p>NA/✅</p></td>
<td><p>NA/✅</p></td>
<td><p>NA/✅</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocSOLVER/en/latest/reference/precision.html" title="(in rocSOLVER Documentation v3.28.2)"><span class="xref std std-doc">rocSOLVER</span></a></p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocThrust/en/latest/data-type-support.html" title="(in rocThrust Documentation v3.3.0)"><span class="xref std std-doc">rocThrust</span></a></p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocWMMA/en/latest/api-reference/api-reference-guide.html" title="(in rocWMMA Documentation v1.7.0)"><span class="xref std std-doc">rocWMMA</span></a></p></td>
<td><p>✅/✅</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/✅</p></td>
<td><p>❌/❌</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-626" name="sd-tab-set-300" type="radio"/>
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="floating-point-type" for="sd-tab-item-626">
Floating-point types</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Library input/output data type name</p></th>
<th class="head"><p>float8 (E4M3)</p></th>
<th class="head"><p>float8 (E5M2)</p></th>
<th class="head"><p>float16</p></th>
<th class="head"><p>bfloat16</p></th>
<th class="head"><p>tensorfloat32</p></th>
<th class="head"><p>float32</p></th>
<th class="head"><p>float64</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/composable_kernel/en/latest/reference/Composable_Kernel_supported_scalar_types.html" title="(in Composable Kernel Documentation v1.1.0)"><span class="xref std std-doc">Composable Kernel</span></a></p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>❌/❌</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipCUB/en/latest/api-reference/data-type-support.html" title="(in hipCUB Documentation v3.4.0)"><span class="xref std std-doc">hipCUB</span></a></p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>❌/❌</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipRAND/en/latest/api-reference/data-type-support.html" title="(in hipRAND Documentation v2.12.0)"><span class="xref std std-doc">hipRAND</span></a></p></td>
<td><p>NA/❌</p></td>
<td><p>NA/❌</p></td>
<td><p>NA/✅</p></td>
<td><p>NA/❌</p></td>
<td><p>NA/❌</p></td>
<td><p>NA/✅</p></td>
<td><p>NA/✅</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipSOLVER/en/latest/reference/precision.html" title="(in hipSOLVER Documentation v2.4.0)"><span class="xref std std-doc">hipSOLVER</span></a></p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipSPARSELt/en/latest/reference/data-type-support.html" title="(in hipSPARSELt Documentation v0.2.3)"><span class="xref std std-doc">hipSPARSELt</span></a></p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipTensor/en/latest/api-reference/api-reference.html" title="(in hipTensor Documentation v1.5.0)"><span class="xref std std-doc">hipTensor</span></a></p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>❌/❌</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/AMDMIGraphX/en/latest/reference/cpp.html" title="(in MIGraphX v2.12.0)"><span class="xref std std-doc">MIGraphX</span></a></p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/MIOpen/en/latest/reference/datatypes.html" title="(in MIOpen Documentation v3.4.0)"><span class="xref std std-doc">MIOpen</span></a></p></td>
<td><p>⚠️/⚠️</p></td>
<td><p>⚠️/⚠️</p></td>
<td><p>✅/✅</p></td>
<td><p>⚠️/⚠️</p></td>
<td><p>❌/❌</p></td>
<td><p>✅/✅</p></td>
<td><p>⚠️/⚠️</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rccl/en/latest/api-reference/library-specification.html" title="(in RCCL Documentation v2.22.3)"><span class="xref std std-doc">RCCL</span></a></p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>❌/❌</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocFFT/en/latest/reference/api.html" title="(in rocFFT Documentation v1.0.32)"><span class="xref std std-doc">rocFFT</span></a></p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>✅/✅</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocPRIM/en/latest/reference/data-type-support.html" title="(in rocPRIM Documentation v3.4.1)"><span class="xref std std-doc">rocPRIM</span></a></p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>❌/❌</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocRAND/en/latest/api-reference/data-type-support.html" title="(in rocRAND Documentation v3.3.0)"><span class="xref std std-doc">rocRAND</span></a></p></td>
<td><p>NA/❌</p></td>
<td><p>NA/❌</p></td>
<td><p>NA/✅</p></td>
<td><p>NA/❌</p></td>
<td><p>NA/❌</p></td>
<td><p>NA/✅</p></td>
<td><p>NA/✅</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocSOLVER/en/latest/reference/precision.html" title="(in rocSOLVER Documentation v3.28.2)"><span class="xref std std-doc">rocSOLVER</span></a></p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocThrust/en/latest/data-type-support.html" title="(in rocThrust Documentation v3.3.0)"><span class="xref std std-doc">rocThrust</span></a></p></td>
<td><p>❌/❌</p></td>
<td><p>❌/❌</p></td>
<td><p>⚠️/⚠️</p></td>
<td><p>⚠️/⚠️</p></td>
<td><p>❌/❌</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocWMMA/en/latest/api-reference/api-reference-guide.html" title="(in rocWMMA Documentation v1.7.0)"><span class="xref std std-doc">rocWMMA</span></a></p></td>
<td><p>✅/❌</p></td>
<td><p>✅/❌</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
<td><p>✅/✅</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As random number generation libraries, rocRAND and hipRAND only specify output
data types for the random values they generate, with no need for input data
types.</p>
</div>
</section>
<section id="hipdatatype-enumeration">
<h4>hipDataType enumeration<a class="headerlink" href="#hipdatatype-enumeration" title="Link to this heading">#</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">hipDataType</span></code> enumeration defines data precision types and is primarily
used when the data reference itself does not include type information, such as
in <code class="docutils literal notranslate"><span class="pre">void*</span></code> pointers. This enumeration is mainly utilized in BLAS libraries.
The HIP type equivalents of the <code class="docutils literal notranslate"><span class="pre">hipDataType</span></code> enumeration are listed in the
following table with descriptions and values.</p>
<div class="pst-scrollable-table-container"><table class="table">
<colgroup>
<col style="width: 25.0%"/>
<col style="width: 25.0%"/>
<col style="width: 10.0%"/>
<col style="width: 40.0%"/>
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>hipDataType</p></th>
<th class="head"><p>HIP type</p></th>
<th class="head"><p>Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">HIP_R_8I</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int8_t</span></code></p></td>
<td><p>3</p></td>
<td><p>8-bit real signed integer.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">HIP_R_8U</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">uint8_t</span></code></p></td>
<td><p>8</p></td>
<td><p>8-bit real unsigned integer.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">HIP_R_16I</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int16_t</span></code></p></td>
<td><p>20</p></td>
<td><p>16-bit real signed integer.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">HIP_R_16U</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">uint16_t</span></code></p></td>
<td><p>22</p></td>
<td><p>16-bit real unsigned integer.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">HIP_R_32I</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int32_t</span></code></p></td>
<td><p>10</p></td>
<td><p>32-bit real signed integer.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">HIP_R_32U</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">uint32_t</span></code></p></td>
<td><p>12</p></td>
<td><p>32-bit real unsigned integer.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">HIP_R_32F</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">float</span></code></p></td>
<td><p>0</p></td>
<td><p>32-bit real single precision floating-point.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">HIP_R_64F</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">double</span></code></p></td>
<td><p>1</p></td>
<td><p>64-bit real double precision floating-point.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">HIP_R_16F</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">half</span></code></p></td>
<td><p>2</p></td>
<td><p>16-bit real half precision floating-point.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">HIP_R_16BF</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bfloat16</span></code></p></td>
<td><p>14</p></td>
<td><p>16-bit real bfloat16 precision floating-point.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">HIP_R_8F_E4M3</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">__hip_fp8_e4m3</span></code></p></td>
<td><p>28</p></td>
<td><p>8-bit real float8 precision floating-point (OCP version).</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">HIP_R_8F_E5M2</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">__hip_fp8_e5m2</span></code></p></td>
<td><p>29</p></td>
<td><p>8-bit real bfloat8 precision floating-point (OCP version).</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">HIP_R_8F_E4M3_FNUZ</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">__hip_fp8_e4m3_fnuz</span></code></p></td>
<td><p>1000</p></td>
<td><p>8-bit real float8 precision floating-point (FNUZ version).</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">HIP_R_8F_E5M2_FNUZ</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">__hip_fp8_e5m2_fnuz</span></code></p></td>
<td><p>1001</p></td>
<td><p>8-bit real bfloat8 precision floating-point (FNUZ version).</p></td>
</tr>
</tbody>
</table>
</div>
<p>The full list of the <code class="docutils literal notranslate"><span class="pre">hipDataType</span></code> enumeration listed in <a class="reference external" href="https://github.com/ROCm/hip/blob/amd-staging/include/hip/library_types.h">library_types.h</a> .</p>
</section>
</section>
</section>
<span id="document-reference/graph-safe-support"></span><section id="graph-safe-support-for-rocm-libraries">
<h2>Graph-safe support for ROCm libraries<a class="headerlink" href="#graph-safe-support-for-rocm-libraries" title="Link to this heading">#</a></h2>
<p>HIP graph-safe libraries operate safely in HIP execution graphs.
<a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/how-to/hip_runtime_api/hipgraph.html#how-to-hip-graph" title="(in HIP Documentation v6.4.43484)"><span>HIP graphs</span></a> are an alternative way of executing tasks on a GPU
that can provide performance benefits over launching kernels using the standard
method via streams.</p>
<p>Functions and routines from graph-safe libraries shouldn’t result in issues like
race conditions, deadlocks, or unintended dependencies.</p>
<p>The following table shows whether a ROCm library is graph-safe.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>ROCm library</p></th>
<th class="head"><p>Graph safe support</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ROCm/composable_kernel">Composable Kernel</a></p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/ROCm/hipBLAS">hipBLAS</a></p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ROCm/hipBLASLt">hipBLASLt</a></p></td>
<td><p>⚠️</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/ROCm/hipCUB">hipCUB</a></p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ROCm/hipFFT">hipFFT</a></p></td>
<td><p>✅ (see <a class="reference external" href="https://rocm.docs.amd.com/projects/hipFFT/en/latest/reference/fft-api-usage.html#hip-graph-support-for-hipfft" title="(in hipFFT Documentation v1.0.18)"><span class="xref std std-ref">details</span></a>)</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/ROCm/hipRAND">hipRAND</a></p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ROCm/hipSOLVER">hipSOLVER</a></p></td>
<td><p>⚠️ (experimental)</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/ROCm/hipSPARSE">hipSPARSE</a></p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ROCm/hipSPARSELt">hipSPARSELt</a></p></td>
<td><p>⚠️ (experimental)</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/ROCm/hipTensor">hipTensor</a></p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ROCm/MIOpen">MIOpen</a></p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/ROCm/rccl">RCCL</a></p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ROCm/rocAL">rocAL</a></p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/ROCm/rocALUTION">rocALUTION</a></p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ROCm/rocBLAS">rocBLAS</a></p></td>
<td><p>✅ (see <a class="reference external" href="https://rocm.docs.amd.com/projects/rocBLAS/en/latest/reference/beta-features.html" title="(in rocBLAS Documentation v4.4.1)"><span class="xref std std-doc">details</span></a>)</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/ROCm/rocDecode">rocDecode</a></p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ROCm/rocFFT">rocFFT</a></p></td>
<td><p>✅ (see <a class="reference external" href="https://rocm.docs.amd.com/projects/rocFFT/en/latest/reference/api.html#hip-graph-support-for-rocfft" title="(in rocFFT Documentation v1.0.32)"><span class="xref std std-ref">details</span></a>)</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/ROCm/rocHPCG">rocHPCG</a></p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ROCm/rocJPEG">rocJPEG</a></p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/ROCm/rocPRIM">rocPRIM</a></p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ROCm/rocRAND">rocRAND</a></p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/ROCm/rocSOLVER">rocSOLVER</a></p></td>
<td><p>⚠️ (experimental)</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ROCm/rocSPARSE">rocSPARSE</a></p></td>
<td><p>⚠️ (experimental)</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/ROCm/rocThrust">rocThrust</a></p></td>
<td><p>❌ (see <a class="reference external" href="https://rocm.docs.amd.com/projects/rocThrust/en/latest/hipgraph-support.html" title="(in rocThrust Documentation v3.3.0)"><span class="xref std std-doc">details</span></a>)</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ROCm/rocWMMA">rocWMMA</a></p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/ROCm/rpp">RPP</a></p></td>
<td><p>⚠️</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ROCm/Tensile">Tensile</a></p></td>
<td><p>✅</p></td>
</tr>
</tbody>
</table>
</div>
<p>✅: full support</p>
<p>⚠️: partial support</p>
<p>❌: not supported</p>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-contribute/contributing"></span><head>
<meta charset="utf-8"/>
<meta content="Contributing to ROCm" name="description"/>
<meta content="ROCm, contributing, contribute, maintainer, contributor" name="keywords"/>
</head>
<section class="tex2jax_ignore mathjax_ignore" id="contributing-to-the-rocm-documentation">
<h2>Contributing to the ROCm documentation<a class="headerlink" href="#contributing-to-the-rocm-documentation" title="Link to this heading">#</a></h2>
<p>The ROCm documentation, like all of ROCm, is open source and available on GitHub. You can contribute to the ROCm documentation by forking the appropriate repository, making your changes, and opening a pull request.</p>
<p>To provide feedback on the ROCm documentation, including submitting an issue or suggesting a feature, see <a class="reference internal" href="#document-contribute/feedback"><span class="std std-doc">Providing feedback about the ROCm documentation</span></a>.</p>
<section id="the-rocm-repositories">
<h3>The ROCm repositories<a class="headerlink" href="#the-rocm-repositories" title="Link to this heading">#</a></h3>
<p>The repositories for ROCm and all ROCm components are available on GitHub.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Module</p></th>
<th class="head"><p>Documentation location</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ROCm framework</p></td>
<td><p><a class="github reference external" href="https://github.com/ROCm/ROCm/tree/develop/docs">ROCm/ROCm</a></p></td>
</tr>
<tr class="row-odd"><td><p>ROCm installation for Linux</p></td>
<td><p><a class="github reference external" href="https://github.com/ROCm/rocm-install-on-linux/tree/develop/docs">ROCm/rocm-install-on-linux</a></p></td>
</tr>
<tr class="row-even"><td><p>ROCm HIP SDK installation for Windows</p></td>
<td><p><a class="github reference external" href="https://github.com/ROCm/rocm-install-on-windows/tree/develop/docs">ROCm/rocm-install-on-windows</a></p></td>
</tr>
</tbody>
</table>
</div>
<p>Individual components have their own repositories with their own documentation in their own <code class="docutils literal notranslate"><span class="pre">docs</span></code> folders.</p>
<p>The sub-folders within the <code class="docutils literal notranslate"><span class="pre">docs</span></code> folders across ROCm are typically structured as follows:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Sub-folder name</p></th>
<th class="head"><p>Documentation type</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">install</span></code></p></td>
<td><p>Installation instructions, build instructions, and prerequisites</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">conceptual</span></code></p></td>
<td><p>Important concepts</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">how-to</span></code></p></td>
<td><p>How to implement specific use cases</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">tutorials</span></code></p></td>
<td><p>Tutorials</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">reference</span></code></p></td>
<td><p>API references and other reference resources</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="editing-and-adding-to-the-documentation">
<h3>Editing and adding to the documentation<a class="headerlink" href="#editing-and-adding-to-the-documentation" title="Link to this heading">#</a></h3>
<p>ROCm documentation follows the <a class="reference external" href="https://developers.google.com/style/highlights">Google developer documentation style guide</a>.</p>
<p>Most topics in the ROCm documentation are written in <a class="reference external" href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/index.html">reStructuredText (rst)</a>, with some topics written in Markdown. Only use reStructuredText when adding new topics. Only use Markdown if the topic you are editing is already in Markdown.</p>
<p>To edit or add to the documentation:</p>
<ol class="arabic">
<li><p>Fork the repository you want to add to or edit.</p></li>
<li><p>Clone your fork locally.</p></li>
<li><p>Create a new local branch cut from the <code class="docutils literal notranslate"><span class="pre">develop</span></code> branch of the repository.</p></li>
<li><p>Make your changes to the documentation.</p></li>
<li><p>Optionally, build the documentation locally before creating a pull request by running the following commands from within the <code class="docutils literal notranslate"><span class="pre">docs</span></code> folder:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w"> </span>pip3<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>sphinx/requirements.txt<span class="w">  </span><span class="c1"># You only need to run this command once</span>
<span class="w"> </span>python3<span class="w"> </span>-m<span class="w"> </span>sphinx<span class="w"> </span>-T<span class="w"> </span>-E<span class="w"> </span>-b<span class="w"> </span>html<span class="w"> </span>-d<span class="w"> </span>_build/doctrees<span class="w"> </span>-D<span class="w"> </span><span class="nv">language</span><span class="o">=</span>en<span class="w"> </span>.<span class="w"> </span>_build/html
</pre></div>
</div>
<p>The output files will be located in the <code class="docutils literal notranslate"><span class="pre">docs/_build</span></code> folder. Open <code class="docutils literal notranslate"><span class="pre">docs/_build/html/index.html</span></code> to view the documentation.</p>
<p>For more information on ROCm build tools, see <a class="reference internal" href="#document-contribute/toolchain"><span class="std std-doc">Documentation toolchain</span></a>.</p>
</li>
<li><p>Push your changes. A GitHub link will be returned in the output of the <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">push</span></code> command. Open this link in a browser to create the pull request.</p>
<p>The documentation is built as part of the checks on pull request, along with spell checking and linting. Scroll to the bottom of your pull request to view all the checks.</p>
<p>Verify that the linting and spell checking have passed, and that the documentation was built successfully. New words or acronyms can be added to the <a class="reference external" href="https://github.com/ROCm/rocm-docs-core/blob/develop/.wordlist.txt">wordlist file</a>. The wordlist is subject to approval by the ROCm documentation team.</p>
<p>The Read The Docs build of your pull request can be accessed by clicking on the Details link next to the Read The Docs build check. Verify that your changes are in the build and look as expected.</p>
<p><img alt='The GitHub checks are collapsed by default and can be accessed by clicking on "Show All Checks".' src="_images/GitHubCheck-Highlight.png"/></p>
<p><img alt="The Read The Docs Build is accessed from the Details link in the Read The Docs check." src="_images/GitHub-ReadThe-Docs-Highlight.png"/></p>
<p>Your pull request will be reviewed by a member of the ROCm documentation team.</p>
</li>
</ol>
<p>See the <a class="reference external" href="https://docs.github.com/en">GitHub documentation</a> for information on how to fork and clone a repository, and how to create and push a local branch.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>By creating a pull request (PR), you agree to allow your contribution to be licensed under the terms of the
LICENSE.txt file in the corresponding repository. Different repositories can use different licenses.</p>
</div>
</section>
<div class="toctree-wrapper compound">
<span id="document-contribute/toolchain"></span><head>
<meta charset="utf-8"/>
<meta content="ROCm documentation toolchain" name="description"/>
<meta content="documentation, toolchain, Sphinx, Doxygen, MyST, AMD, ROCm" name="keywords"/>
</head>
<section class="tex2jax_ignore mathjax_ignore" id="rocm-documentation-toolchain">
<h3>ROCm documentation toolchain<a class="headerlink" href="#rocm-documentation-toolchain" title="Link to this heading">#</a></h3>
<p>The ROCm documentation relies on several open source toolchains and sites.</p>
<section id="rocm-docs-core">
<h4>rocm-docs-core<a class="headerlink" href="#rocm-docs-core" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://github.com/ROCm/rocm-docs-core">rocm-docs-core</a> is an AMD-maintained
project that applies customizations for the ROCm documentation. This project is the tool most ROCm repositories use as part of their documentation build pipeline. It is available as a <a class="reference external" href="https://pypi.org/project/rocm-docs-core/">pip package on PyPI</a>.</p>
<p>See the user and developer guides for rocm-docs-core at
<a class="reference external" href="https://rocm.docs.amd.com/projects/rocm-docs-core/en/latest/index.html" title="(in ROCm Docs Core v1.22.0)"><span class="xref std std-doc">rocm-docs-core documentation</span></a>.</p>
</section>
<section id="sphinx">
<h4>Sphinx<a class="headerlink" href="#sphinx" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://www.sphinx-doc.org/en/master/">Sphinx</a> is a documentation generator originally used for Python. It is now widely used in the open source community.</p>
<section id="sphinx-external-toc">
<h5>Sphinx External ToC<a class="headerlink" href="#sphinx-external-toc" title="Link to this heading">#</a></h5>
<p><a class="reference external" href="https://sphinx-external-toc.readthedocs.io/en/latest/intro.html">Sphinx External ToC</a> is a Sphinx extension used for ROCm documentation navigation. This tool generates a navigation menu on the left
based on a YAML file (<code class="docutils literal notranslate"><span class="pre">_toc.yml.in</span></code>) that contains the table of contents.</p>
</section>
<section id="sphinx-book-theme">
<h5>Sphinx-book-theme<a class="headerlink" href="#sphinx-book-theme" title="Link to this heading">#</a></h5>
<p><a class="reference external" href="https://sphinx-book-theme.readthedocs.io/en/latest/">Sphinx-book-theme</a> is a Sphinx theme that defines the base appearance for ROCm documentation. ROCm documentation applies some customization, such as a custom header and footer, on top of the Sphinx Book Theme.</p>
</section>
<section id="sphinx-design">
<h5>Sphinx Design<a class="headerlink" href="#sphinx-design" title="Link to this heading">#</a></h5>
<p><a class="reference external" href="https://sphinx-design.readthedocs.io/en/latest/index.html">Sphinx design</a> is a Sphinx extension that adds design functionality. ROCm documentation uses Sphinx Design for grids, cards, and synchronized tabs.</p>
</section>
</section>
<section id="doxygen">
<h4>Doxygen<a class="headerlink" href="#doxygen" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://www.doxygen.nl/">Doxygen</a> is a documentation generator that extracts information from in-code comments. It is used for API documentation.</p>
</section>
<section id="breathe">
<h4>Breathe<a class="headerlink" href="#breathe" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://www.breathe-doc.org/">Breathe</a> is a Sphinx plugin for integrating Doxygen content.</p>
</section>
<section id="read-the-docs">
<h4>Read the Docs<a class="headerlink" href="#read-the-docs" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://docs.readthedocs.io/en/stable/">Read the Docs</a> is the service that builds and hosts the HTML version of the ROCm documentation.</p>
</section>
</section>
<span id="document-contribute/building"></span><head>
<meta charset="utf-8"/>
<meta content="Building ROCm documentation" name="description"/>
<meta content="documentation, Visual Studio Code, GitHub, command line,
  AMD, ROCm" name="keywords"/>
</head>
<section class="tex2jax_ignore mathjax_ignore" id="building-documentation">
<h3>Building documentation<a class="headerlink" href="#building-documentation" title="Link to this heading">#</a></h3>
<section id="github">
<h4>GitHub<a class="headerlink" href="#github" title="Link to this heading">#</a></h4>
<p>If you open a pull request and scroll down to the summary panel,
there is a commit status section. Next to the line
<code class="docutils literal notranslate"><span class="pre">docs/readthedocs.com:advanced-micro-devices-demo</span></code>, there is a <code class="docutils literal notranslate"><span class="pre">Details</span></code> link.
If you click this, it takes you to the Read the Docs build for your pull request.</p>
<p><img alt="GitHub PR commit status" src="_images/commit-status.png"/></p>
<p>If you don’t see this line, click <code class="docutils literal notranslate"><span class="pre">Show</span> <span class="pre">all</span> <span class="pre">checks</span></code> to get an itemized view.</p>
</section>
<section id="command-line">
<h4>Command line<a class="headerlink" href="#command-line" title="Link to this heading">#</a></h4>
<p>You can build our documentation via the command line using Python.</p>
<p>See the <code class="docutils literal notranslate"><span class="pre">build.tools.python</span></code> setting in the <a class="reference external" href="https://github.com/ROCm/ROCm/blob/develop/.readthedocs.yaml">Read the Docs configuration file</a> for the Python version used by Read the Docs to build documentation.</p>
<p>See the <a class="reference external" href="https://github.com/ROCm/ROCm/blob/develop/docs/sphinx/requirements.txt">Python requirements file</a> for Python packages needed to build the documentation.</p>
<p>Use the Python Virtual Environment (<code class="docutils literal notranslate"><span class="pre">venv</span></code>) and run the following commands from the project root:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-mvenv<span class="w"> </span>.venv

.venv/bin/python<span class="w">     </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>docs/sphinx/requirements.txt
.venv/bin/python<span class="w">     </span>-m<span class="w"> </span>sphinx<span class="w"> </span>-T<span class="w"> </span>-E<span class="w"> </span>-b<span class="w"> </span>html<span class="w"> </span>-d<span class="w"> </span>_build/doctrees<span class="w"> </span>-D<span class="w"> </span><span class="nv">language</span><span class="o">=</span>en<span class="w"> </span>docs<span class="w"> </span>_build/html
</pre></div>
</div>
<p>Navigate to <code class="docutils literal notranslate"><span class="pre">_build/html/index.html</span></code> and open this file in a web browser.</p>
</section>
<section id="visual-studio-code">
<h4>Visual Studio Code<a class="headerlink" href="#visual-studio-code" title="Link to this heading">#</a></h4>
<p>With the help of a few extensions, you can create a productive environment to author and test
documentation locally using Visual Studio (VS) Code. Follow these steps to configure VS Code:</p>
<ol class="arabic">
<li><p>Install the required extensions:</p>
<ul class="simple">
<li><p>Python: <code class="docutils literal notranslate"><span class="pre">(ms-python.python)</span></code></p></li>
<li><p>Live Server: <code class="docutils literal notranslate"><span class="pre">(ritwickdey.LiveServer)</span></code></p></li>
</ul>
</li>
<li><p>Add the following entries to <code class="docutils literal notranslate"><span class="pre">.vscode/settings.json</span></code>.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="p">{</span>
<span class="w">    </span><span class="nt">"liveServer.settings.root"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/.vscode/build/html"</span><span class="p">,</span>
<span class="w">    </span><span class="nt">"liveServer.settings.wait"</span><span class="p">:</span><span class="w"> </span><span class="mi">1000</span><span class="p">,</span>
<span class="w">    </span><span class="nt">"python.terminal.activateEnvInCurrentTerminal"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">liveServer.settings.root</span></code>: Sets the root of the output website for live previews. Must be changed
alongside the <code class="docutils literal notranslate"><span class="pre">tasks.json</span></code> command.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">liveServer.settings.wait</span></code>: Tells the live server to wait with the update in order to give Sphinx time to
regenerate the site contents and not refresh before the build is complete.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">python.terminal.activateEnvInCurrentTerminal</span></code>: Activates the automatic virtual environment, so you
can build the site from the integrated terminal.</p></li>
</ul>
</li>
<li><p>Add the following tasks to <code class="docutils literal notranslate"><span class="pre">.vscode/tasks.json</span></code>.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="p">{</span>
<span class="w">    </span><span class="nt">"version"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2.0.0"</span><span class="p">,</span>
<span class="w">    </span><span class="nt">"tasks"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">        </span><span class="nt">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Build Docs"</span><span class="p">,</span>
<span class="w">        </span><span class="nt">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"process"</span><span class="p">,</span>
<span class="w">        </span><span class="nt">"windows"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">          </span><span class="nt">"command"</span><span class="p">:</span><span class="w"> </span><span class="s2">"${workspaceFolder}/.venv/Scripts/python.exe"</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">"command"</span><span class="p">:</span><span class="w"> </span><span class="s2">"${workspaceFolder}/.venv/bin/python3"</span><span class="p">,</span>
<span class="w">        </span><span class="nt">"args"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">          </span><span class="s2">"-m"</span><span class="p">,</span>
<span class="w">          </span><span class="s2">"sphinx"</span><span class="p">,</span>
<span class="w">          </span><span class="s2">"-j"</span><span class="p">,</span>
<span class="w">          </span><span class="s2">"auto"</span><span class="p">,</span>
<span class="w">          </span><span class="s2">"-T"</span><span class="p">,</span>
<span class="w">          </span><span class="s2">"-b"</span><span class="p">,</span>
<span class="w">          </span><span class="s2">"html"</span><span class="p">,</span>
<span class="w">          </span><span class="s2">"-d"</span><span class="p">,</span>
<span class="w">          </span><span class="s2">"${workspaceFolder}/.vscode/build/doctrees"</span><span class="p">,</span>
<span class="w">          </span><span class="s2">"-D"</span><span class="p">,</span>
<span class="w">          </span><span class="s2">"language=en"</span><span class="p">,</span>
<span class="w">          </span><span class="s2">"${workspaceFolder}/docs"</span><span class="p">,</span>
<span class="w">          </span><span class="s2">"${workspaceFolder}/.vscode/build/html"</span>
<span class="w">        </span><span class="p">],</span>
<span class="w">        </span><span class="nt">"problemMatcher"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">          </span><span class="p">{</span>
<span class="w">            </span><span class="nt">"owner"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sphinx"</span><span class="p">,</span>
<span class="w">            </span><span class="nt">"fileLocation"</span><span class="p">:</span><span class="w"> </span><span class="s2">"absolute"</span><span class="p">,</span>
<span class="w">            </span><span class="nt">"pattern"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">              </span><span class="nt">"regexp"</span><span class="p">:</span><span class="w"> </span><span class="s2">"^(?:.*\\.{3}\\s+)?(\\/[^:]*|[a-zA-Z]:\\\\[^:]*):(\\d+):\\s+(WARNING|ERROR):\\s+(.*)$"</span><span class="p">,</span>
<span class="w">              </span><span class="nt">"file"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">              </span><span class="nt">"line"</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">              </span><span class="nt">"severity"</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span>
<span class="w">              </span><span class="nt">"message"</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">          </span><span class="p">},</span>
<span class="w">          </span><span class="p">{</span>
<span class="w">          </span><span class="nt">"owner"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sphinx"</span><span class="p">,</span>
<span class="w">            </span><span class="nt">"fileLocation"</span><span class="p">:</span><span class="w"> </span><span class="s2">"absolute"</span><span class="p">,</span>
<span class="w">            </span><span class="nt">"pattern"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">              </span><span class="nt">"regexp"</span><span class="p">:</span><span class="w"> </span><span class="s2">"^(?:.*\\.{3}\\s+)?(\\/[^:]*|[a-zA-Z]:\\\\[^:]*):{1,2}\\s+(WARNING|ERROR):\\s+(.*)$"</span><span class="p">,</span>
<span class="w">              </span><span class="nt">"file"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">              </span><span class="nt">"severity"</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">              </span><span class="nt">"message"</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">          </span><span class="p">}</span>
<span class="w">        </span><span class="p">],</span>
<span class="w">        </span><span class="nt">"group"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">          </span><span class="nt">"kind"</span><span class="p">:</span><span class="w"> </span><span class="s2">"build"</span><span class="p">,</span>
<span class="w">          </span><span class="nt">"isDefault"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">]</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
<blockquote>
<div><p>Implementation detail: two problem matchers were needed to be defined,
because VS Code doesn’t tolerate some problem information being potentially
absent. While a single regex could match all types of errors, if a capture
group remains empty (the line number doesn’t show up in all warning/error
messages) but the <code class="docutils literal notranslate"><span class="pre">pattern</span></code> references said empty capture group, VS Code
discards the message completely.</p>
</div></blockquote>
</li>
<li><p>Configure the Python virtual environment (<code class="docutils literal notranslate"><span class="pre">venv</span></code>).</p>
<p>From the Command Palette, run <code class="docutils literal notranslate"><span class="pre">Python:</span> <span class="pre">Create</span> <span class="pre">Environment</span></code>. Select <code class="docutils literal notranslate"><span class="pre">venv</span></code> environment and
<code class="docutils literal notranslate"><span class="pre">docs/sphinx/requirements.txt</span></code>.</p>
</li>
<li><p>Build the docs.</p>
<p>Launch the default build task using one of the following options:</p>
<ul class="simple">
<li><p>A hotkey (the default is <code class="docutils literal notranslate"><span class="pre">Ctrl+Shift+B</span></code>)</p></li>
<li><p>Issuing the <code class="docutils literal notranslate"><span class="pre">Tasks:</span> <span class="pre">Run</span> <span class="pre">Build</span> <span class="pre">Task</span></code> from the Command Palette</p></li>
</ul>
</li>
<li><p>Open the live preview.</p>
<p>Navigate to the site output within VS Code: right-click on <code class="docutils literal notranslate"><span class="pre">.vscode/build/html/index.html</span></code> and
select <code class="docutils literal notranslate"><span class="pre">Open</span> <span class="pre">with</span> <span class="pre">Live</span> <span class="pre">Server</span></code>. The contents should update on every rebuild without having to
refresh the browser.</p>
</li>
</ol>
</section>
</section>
</div>
</section>
<span id="document-contribute/feedback"></span><head>
<meta charset="utf-8"/>
<meta content="Providing feedback for ROCm documentation" name="description"/>
<meta content="documentation, pull request, GitHub, AMD, ROCm" name="keywords"/>
</head>
<section class="tex2jax_ignore mathjax_ignore" id="providing-feedback-about-the-rocm-documentation">
<h2>Providing feedback about the ROCm documentation<a class="headerlink" href="#providing-feedback-about-the-rocm-documentation" title="Link to this heading">#</a></h2>
<p>Feedback about the ROCm documentation is welcome. You can provide feedback about the ROCm documentation either through GitHub Discussions or GitHub Issues.</p>
<section id="participating-in-discussions-through-github-discussions">
<h3>Participating in discussions through GitHub Discussions<a class="headerlink" href="#participating-in-discussions-through-github-discussions" title="Link to this heading">#</a></h3>
<p>You can ask questions, view announcements, suggest new features, and communicate with other members of the community through <a class="reference external" href="https://github.com/ROCm/ROCm/discussions">GitHub Discussions</a>.</p>
</section>
<section id="submitting-issues-through-github-issues">
<h3>Submitting issues through GitHub Issues<a class="headerlink" href="#submitting-issues-through-github-issues" title="Link to this heading">#</a></h3>
<p>You can submit issues through <a class="reference external" href="https://github.com/ROCm/ROCm/issues">GitHub Issues</a>.</p>
<p>When creating a new issue, follow the following guidelines:</p>
<ol class="arabic simple">
<li><p>Always do a search to see if the same issue already exists. If the issue already exists, upvote it, and comment or post to provide any additional details you might have.</p></li>
<li><p>If you find an issue that is similar to your issue, log your issue, then add a comment that includes a link to the similar issue, as well as its issue number.</p></li>
<li><p>Always provide as much information as possible. This helps reduce the time required to reproduce the issue.</p></li>
</ol>
<p>After creating your issue, make sure to check it regularly for any requests for additional information.</p>
<p>For information about contributing content to the ROCm documentation, see <a class="reference internal" href="#document-contribute/contributing"><span class="std std-doc">Contributing to the ROCm documentation</span></a>.</p>
</section>
</section>
<span id="document-about/license"></span><head>
<meta charset="utf-8"/>
<meta content="ROCm licensing terms" name="description"/>
<meta content="license, licensing terms" name="keywords"/>
</head>
<section class="tex2jax_ignore mathjax_ignore" id="rocm-license">
<h2>ROCm license<a class="headerlink" href="#rocm-license" title="Link to this heading">#</a></h2>
<p>MIT License</p>
<p>Copyright © 2023 - 2025 Advanced Micro Devices, Inc. All rights reserved.</p>
<p>Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the “Software”), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:</p>
<p>The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.</p>
<p>THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The preceding license applies to the <a class="reference external" href="https://github.com/ROCm/ROCm">ROCm repository</a>, which
primarily contains documentation. For licenses related to other ROCm components, refer to the
following section.</p>
</div>
<section id="rocm-component-licenses">
<h3>ROCm component licenses<a class="headerlink" href="#rocm-component-licenses" title="Link to this heading">#</a></h3>
<p>ROCm is released by Advanced Micro Devices, Inc. (AMD) and is licensed per component separately.
The following table is a list of ROCm components with links to their respective license
terms. These components may include third party components subject to
additional licenses. Please review individual repositories for more information.</p>
<!-- spellcheck-disable -->
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Component</p></th>
<th class="head text-left"><p>License</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/clr">AMD Compute Language Runtime (CLR)</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/clr/blob/amd-staging/LICENSE.txt">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/amdsmi">AMD SMI</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/amdsmi/blob/amd-staging/LICENSE">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/aomp/">aomp</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/aomp/blob/aomp-dev/LICENSE">Apache 2.0</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/aomp-extras/">aomp-extras</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/aomp-extras/blob/aomp-dev/LICENSE">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/llvm-project/tree/amd-staging/amd/comgr">Code Object Manager (Comgr)</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/llvm-project/blob/amd-staging/amd/comgr/LICENSE.txt">The University of Illinois/NCSA</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/composable_kernel">Composable Kernel</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/composable_kernel/blob/develop/LICENSE">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/half/">half</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/half/blob/rocm/LICENSE.txt">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/HIP/">HIP</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/HIP/blob/amd-staging/LICENSE.txt">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/clr/tree/amd-staging/hipamd">hipamd</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/clr/blob/amd-staging/hipamd/LICENSE.txt">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipBLAS/">hipBLAS</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipBLAS/blob/develop/LICENSE.md">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipBLASLt/">hipBLASLt</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipBLASLt/blob/develop/LICENSE.md">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/llvm-project/tree/amd-staging/amd/hipcc">HIPCC</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/llvm-project/blob/amd-staging/amd/hipcc/LICENSE.txt">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipCUB/">hipCUB</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipCUB/blob/develop/LICENSE.txt">Custom</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipFFT/">hipFFT</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipFFT/blob/develop/LICENSE.md">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipfort/">hipfort</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipfort/blob/develop/LICENSE">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/HIPIFY/">HIPIFY</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/HIPIFY/blob/amd-staging/LICENSE.txt">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipRAND/">hipRAND</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipRAND/blob/develop/LICENSE.txt">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipSOLVER/">hipSOLVER</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipSOLVER/blob/develop/LICENSE.md">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipSPARSE/">hipSPARSE</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipSPARSE/blob/develop/LICENSE.md">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipSPARSELt/">hipSPARSELt</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipSPARSELt/blob/develop/LICENSE.md">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipTensor">hipTensor</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/hipTensor/blob/develop/LICENSE">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>hsa-amd-aqlprofile</p></td>
<td class="text-left"><p><a class="reference external" href="https://www.amd.com/en/legal/eula/amd-software-eula.html">AMD Software EULA</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/llvm-project/">llvm-project</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/llvm-project/blob/amd-staging/LICENSE.TXT">Apache</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/llvm-project/tree/amd-staging/flang">llvm-project/flang</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/llvm-project/blob/amd-staging/flang/LICENSE.TXT">Apache 2.0</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/AMDMIGraphX/">MIGraphX</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/AMDMIGraphX/blob/develop/LICENSE">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/MIOpen/">MIOpen</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/MIOpen/blob/develop/LICENSE.txt">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/MIVisionX/">MIVisionX</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/MIVisionX/blob/develop/LICENSE.txt">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocAL">rocAL</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocAL/blob/develop/LICENSE.txt">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocALUTION/">rocALUTION</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocALUTION/blob/develop/LICENSE.md">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocBLAS/">rocBLAS</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocBLAS/blob/develop/LICENSE.md">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/ROCdbgapi/">ROCdbgapi</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/ROCdbgapi/blob/amd-staging/LICENSE.txt">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocDecode">rocDecode</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocDecode/blob/develop/LICENSE">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocFFT/">rocFFT</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocFFT/blob/develop/LICENSE.md">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/ROCgdb/">ROCgdb</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/ROCgdb/blob/amd-staging/COPYING3">GNU General Public License v3.0</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocJPEG/">rocJPEG</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocJPEG/blob/develop/LICENSE">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/ROCK-Kernel-Driver/">ROCK-Kernel-Driver</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/ROCK-Kernel-Driver/blob/master/COPYING">GPL 2.0 WITH Linux-syscall-note</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocminfo/">rocminfo</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocminfo/blob/amd-staging/License.txt">The University of Illinois/NCSA</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocm_bandwidth_test/">ROCm Bandwidth Test</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocm_bandwidth_test/blob/master/LICENSE.txt">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocm-cmake/">ROCm CMake</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocm-cmake/blob/develop/LICENSE">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rccl/">ROCm Communication Collectives Library (RCCL)</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rccl/blob/develop/LICENSE.txt">Custom</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocm-core">ROCm-Core</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocm-core/blob/master/copyright">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocprofiler-compute">ROCm Compute Profiler</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocprofiler-compute/blob/amd-staging/LICENSE">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rdc/">ROCm Data Center (RDC)</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rdc/blob/amd-staging/LICENSE">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/llvm-project/tree/amd-staging/amd/device-libs">ROCm-Device-Libs</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/llvm-project/blob/amd-staging/amd/device-libs/LICENSE.TXT">The University of Illinois/NCSA</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/clr/tree/amd-staging/opencl">ROCm-OpenCL-Runtime</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/clr/blob/amd-staging/opencl/LICENSE.txt">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rpp">ROCm Performance Primitives (RPP)</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rpp/blob/develop/LICENSE">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocm_smi_lib/">ROCm SMI Lib</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocm_smi_lib/blob/amd-staging/License.txt">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocprofiler-systems">ROCm Systems Profiler</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocprofiler-systems/blob/amd-staging/LICENSE">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/ROCmValidationSuite/">ROCm Validation Suite</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/ROCmValidationSuite/blob/master/LICENSE">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocPRIM/">rocPRIM</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocPRIM/blob/develop/LICENSE.txt">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocprofiler/">ROCProfiler</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocprofiler/blob/amd-staging/LICENSE">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocprofiler-sdk">ROCprofiler-SDK</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocprofiler-sdk/blob/amd-mainline/LICENSE">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocPyDecode">rocPyDecode</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocPyDecode/blob/develop/LICENSE.txt">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocRAND/">rocRAND</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocRAND/blob/develop/LICENSE.txt">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocr_debug_agent/">ROCr Debug Agent</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocr_debug_agent/blob/amd-staging/LICENSE.txt">The University of Illinois/NCSA</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/ROCR-Runtime/">ROCR-Runtime</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/ROCR-Runtime/blob/amd-staging/LICENSE.txt">The University of Illinois/NCSA</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocSHMEM/">rocSHMEM</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocSHMEM/blob/develop/LICENSE.md">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocSOLVER/">rocSOLVER</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocSOLVER/blob/develop/LICENSE.md">BSD-2-Clause</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocSPARSE/">rocSPARSE</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocSPARSE/blob/develop/LICENSE.md">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocThrust/">rocThrust</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocThrust/blob/develop/LICENSE">Apache 2.0</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/roctracer/">ROCTracer</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/roctracer/blob/amd-master/LICENSE">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocWMMA/">rocWMMA</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/rocWMMA/blob/develop/LICENSE.md">MIT</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/Tensile/">Tensile</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/Tensile/blob/develop/LICENSE.md">MIT</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/TransferBench">TransferBench</a></p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/ROCm/TransferBench/blob/develop/LICENSE.md">MIT</a></p></td>
</tr>
</tbody>
</table>
</div>
<p>Open sourced ROCm components are released via public GitHub
repositories, packages on <a class="reference external" href="https://repo.radeon.com">https://repo.radeon.com</a> and other distribution channels.
Proprietary products are only available on <a class="reference external" href="https://repo.radeon.com">https://repo.radeon.com</a>.
Proprietary components are organized in a proprietary subdirectory in the package
repositories to distinguish from open sourced packages.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The following additional terms and conditions apply to your use of ROCm technical documentation.</p>
</div>
<p>©2023 - 2025 Advanced Micro Devices, Inc. All rights reserved.</p>
<p>The information presented in this document is for informational purposes only
and may contain technical inaccuracies, omissions, and typographical errors. The
information contained herein is subject to change and may be rendered inaccurate
for many reasons, including but not limited to product and roadmap changes,
component and motherboard version changes, new model and/or product releases,
product differences between differing manufacturers, software changes, BIOS
flashes, firmware upgrades, or the like. Any computer system has risks of
security vulnerabilities that cannot be completely prevented or mitigated. AMD
assumes no obligation to update or otherwise correct or revise this information.
However, AMD reserves the right to revise this information and to make changes
from time to time to the content hereof without obligation of AMD to notify any
person of such revisions or changes.</p>
<p>THIS INFORMATION IS PROVIDED “AS IS.” AMD MAKES NO REPRESENTATIONS OR WARRANTIES
WITH RESPECT TO THE CONTENTS HEREOF AND ASSUMES NO RESPONSIBILITY FOR ANY
INACCURACIES, ERRORS, OR OMISSIONS THAT MAY APPEAR IN THIS INFORMATION. AMD
SPECIFICALLY DISCLAIMS ANY IMPLIED WARRANTIES OF NON-INFRINGEMENT,
MERCHANTABILITY, OR FITNESS FOR ANY PARTICULAR PURPOSE. IN NO EVENT WILL AMD BE
LIABLE TO ANY PERSON FOR ANY RELIANCE, DIRECT, INDIRECT, SPECIAL, OR OTHER
CONSEQUENTIAL DAMAGES ARISING FROM THE USE OF ANY INFORMATION CONTAINED HEREIN,
EVEN IF AMD IS EXPRESSLY ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p>
<p>AMD, the AMD Arrow logo, ROCm, and combinations thereof are trademarks of
Advanced Micro Devices, Inc. Other product names used in this publication are
for identification purposes only and may be trademarks of their respective
companies.</p>
<section id="package-licensing">
<h4>Package licensing<a class="headerlink" href="#package-licensing" title="Link to this heading">#</a></h4>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>AQL Profiler and AOCC CPU optimization are both provided in binary form, each
subject to the license agreement enclosed in the directory for the binary available
in <code class="docutils literal notranslate"><span class="pre">/opt/rocm/share/doc/hsa-amd-aqlprofile/EULA</span></code>. By using, installing,
copying or distributing AQL Profiler and/or AOCC CPU Optimizations, you agree to
the terms and conditions of this license agreement. If you do not agree to the
terms of this agreement, do not install, copy or use the AQL Profiler and/or the
AOCC CPU Optimizations.</p>
</div>
<p>For the rest of the ROCm packages, you can find the licensing information at the
following location: <code class="docutils literal notranslate"><span class="pre">/opt/rocm/share/doc/&lt;component-name&gt;/</span></code> or in the locations
specified in the preceding table.</p>
<p>For example, you can fetch the licensing information of the <code class="docutils literal notranslate"><span class="pre">amd_comgr</span></code>
component (Code Object Manager) from the <code class="docutils literal notranslate"><span class="pre">/opt/rocm/share/doc/amd_comgr/LICENSE.txt</span></code> file.</p>
</section>
</section>
</section>
</div>
</section>
</article>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
</div>
</footer>
</div>
<dialog id="pst-secondary-sidebar-modal"></dialog>
<div class="bd-sidebar-secondary bd-toc" id="pst-secondary-sidebar"><div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage">
<i class="fa-solid fa-list"></i> Contents
  </div>
<nav class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-what-is-rocm">What is ROCm?</a><ul class="visible nav section-nav flex-column">
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-components">ROCm components</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#libraries">Libraries</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-computer-vision">Machine Learning &amp; Computer Vision</a></li>
<li class="toctree-l4 toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#communication">Communication</a></li>
<li class="toctree-l4 toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#math">Math</a></li>
<li class="toctree-l4 toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#primitives">Primitives</a></li>
</ul>
</li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tools">Tools</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#system-management">System Management</a></li>
<li class="toctree-l4 toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#performance">Performance</a></li>
<li class="toctree-l4 toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#development">Development</a></li>
</ul>
</li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compilers">Compilers</a></li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#runtimes">Runtimes</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-about/release-notes">Release notes</a><ul class="visible nav section-nav flex-column">
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-highlights">Release highlights</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#amdgpu-driver-updates">AMDGPU driver updates</a></li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-smi-update">ROCm SMI update</a></li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-documentation-updates">ROCm documentation updates</a></li>
</ul>
</li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#operating-system-and-hardware-support-changes">Operating system and hardware support changes</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-components">ROCm components</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#detailed-component-changes">Detailed component changes</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-smi-7-7-0"><strong>ROCm SMI</strong> (7.7.0)</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#added">Added</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-known-issues">ROCm known issues</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-upcoming-changes">ROCm upcoming changes</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#amd-smi-migration-to-amdgpu-driver-repository">AMD SMI migration to AMDGPU driver repository</a></li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-smi-deprecation">ROCm SMI deprecation</a></li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#roctracer-rocprofiler-rocprof-and-rocprofv2-deprecation">ROCTracer, ROCProfiler, rocprof, and rocprofv2 deprecation</a></li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#amdgpu-wavefront-size-compiler-macro-deprecation">AMDGPU wavefront size compiler macro deprecation</a></li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hipcc-perl-scripts-deprecation">HIPCC Perl scripts deprecation</a></li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#changes-to-rocm-object-tooling">Changes to ROCm Object Tooling</a></li>
<li class="toctree-l3 toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hip-runtime-api-changes">HIP runtime API changes</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-compatibility/compatibility-matrix">Compatibility matrix</a><ul class="visible nav section-nav flex-column">
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#operating-systems-kernel-and-glibc-versions">Operating systems, kernel and Glibc versions</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#past-versions-of-rocm-compatibility-matrix">Past versions of ROCm compatibility matrix</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference external nav-link" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html">Linux system requirements</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference external nav-link" href="https://rocm.docs.amd.com/projects/install-on-windows/en/latest/reference/system-requirements.html">Windows system requirements</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Install</span></p>
<ul class="nav section-nav flex-column">
<li class="toctree-l1 nav-item toc-entry"><a class="reference external nav-link" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/">ROCm on Linux</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference external nav-link" href="https://rocm.docs.amd.com/projects/install-on-windows/en/latest/">HIP SDK on Windows</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference external nav-link" href="https://rocm.docs.amd.com/projects/radeon/en/latest/index.html">ROCm on Radeon GPUs</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/deep-learning-rocm">Deep learning frameworks</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/build-rocm">Build ROCm from source</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">How to</span></p>
<ul class="nav section-nav flex-column">
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/index">Use ROCm for AI</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/install">Installation</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-frameworks">Machine learning frameworks</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">Next steps</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/system-health-check">System health benchmarks</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-validation-suite-rvs-tests">ROCm Validation Suite (RVS) tests</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#install-rocm-validation-suite">Install ROCm Validation Suite</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmark-stress-and-qualification-tests">Benchmark, stress, and qualification tests</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#babelstream-test">BabelStream test</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#rccl-tests">RCCL tests</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#transferbench-test">TransferBench test</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/training/index">Training</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/training/benchmark-docker/megatron-lm">Train a model with Megatron-LM</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-models">Supported models</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-measurements">Performance measurements</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#system-validation">System validation</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-setup">Environment setup</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#download-the-docker-image">Download the Docker image</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#configuration">Configuration</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#network-interface">Network interface</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenizer">Tokenizer</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-options">Dataset options</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#download-the-dataset">Download the dataset</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-configuration">Multi-node configuration</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#run-training">Run training</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#single-node-training">Single node training</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-training-examples">Multi-node training examples</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#key-options">Key options</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#previous-versions">Previous versions</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/training/benchmark-docker/pytorch-training">Train a model with PyTorch</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-models">Supported models</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-measurements">Performance measurements</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#system-validation">System validation</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking">Benchmarking</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#previous-versions">Previous versions</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/training/benchmark-docker/jax-maxtext">Train a model with JAX MaxText</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-features-and-models">Supported features and models</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#unsupported-features">Unsupported features</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#system-validation">System validation</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-setup">Environment setup</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-setup">Multi-node setup</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#pull-the-docker-image">Pull the Docker image</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-started">Getting started</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#single-node-training-benchmarking-examples">Single node training benchmarking examples</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-training-benchmarking-examples">Multi-node training benchmarking examples</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#previous-versions">Previous versions</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/training/benchmark-docker/mpt-llm-foundry">Train a model with LLM Foundry</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#system-validation">System validation</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-started">Getting started</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-output">Interpreting the output</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/training/scale-model-training">Scale model training</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-distributed">PyTorch distributed</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-fsdp">PyTorch FSDP</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#deepspeed">DeepSpeed</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-mixed-precision-amp">Automatic mixed precision (AMP)</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-your-model">Fine-tuning your model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/fine-tuning/index">Fine-tuning LLMs</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/fine-tuning/overview">Conceptual overview</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-challenge-of-fine-tuning-models">The challenge of fine-tuning models</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizations-for-model-fine-tuning">Optimizations for model fine-tuning</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#walkthrough">Walkthrough</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/fine-tuning/fine-tuning-and-inference">Fine-tuning</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/fine-tuning/single-gpu-fine-tuning-and-inference">Use a single accelerator</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-setup">Environment setup</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-the-base-implementation-environment">Setting up the base implementation environment</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#download-the-base-model-and-fine-tuning-dataset">Download the base model and fine-tuning dataset</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#configure-fine-tuning-parameters">Configure fine-tuning parameters</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning">Fine-tuning</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-adapters-or-fully-fine-tuned-models">Saving adapters or fully fine-tuned models</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-model-inference">Basic model inference</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/fine-tuning/multi-gpu-fine-tuning-and-inference">Use multiple accelerators</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-setup">Environment setup</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-the-base-implementation-environment">Setting up the base implementation environment</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#hugging-face-accelerate-for-fine-tuning-and-inference">Hugging Face Accelerate for fine-tuning and inference</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#torchtune-for-fine-tuning-and-inference">torchtune for fine-tuning and inference</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference/index">Inference</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference/hugging-face-models">Run models from Hugging Face</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-hugging-face-transformers">Using Hugging Face Transformers</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-hugging-face-with-optimum-amd">Using Hugging Face with Optimum-AMD</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#installation">Installation</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#flash-attention">Flash Attention</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#gptq">GPTQ</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#onnx">ONNX</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference/llm-inference-frameworks">LLM inference frameworks</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#vllm-inference">vLLM inference</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-vllm">Installing vLLM</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-llms-tgi">Hugging Face TGI</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#install-tgi">Install TGI</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference/benchmark-docker/vllm">vLLM inference performance testing</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#what-s-new">What’s new</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-models">Supported models</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-measurements">Performance measurements</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#system-validation">System validation</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#pull-the-docker-image">Pull the Docker image</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking">Benchmarking</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-usage">Advanced usage</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#reproducing-the-docker-image">Reproducing the Docker image</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#known-issues-and-workarounds">Known issues and workarounds</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#previous-versions">Previous versions</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference/benchmark-docker/pytorch-inference">PyTorch inference performance testing</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-models">Supported models</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#system-validation">System validation</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#pull-the-docker-image">Pull the Docker image</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking">Benchmarking</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference/benchmark-docker/sglang">SGLang inference performance testing</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#system-validation">System validation</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#pull-the-docker-image">Pull the Docker image</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking">Benchmarking</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#previous-versions">Previous versions</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference/deploy-your-model">Deploy your model</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#serving-using-vllm">Serving using vLLM</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#vllm-installation">vLLM installation</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#vllm-walkthrough">vLLM walkthrough</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#validating-vllm-performance">Validating vLLM performance</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#serving-using-hugging-face-tgi">Serving using Hugging Face TGI</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#tgi-installation">TGI installation</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#tgi-walkthrough">TGI walkthrough</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference-optimization/index">Inference optimization</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference-optimization/model-quantization">Model quantization techniques</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#amd-quark">AMD Quark</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-quark">Installing Quark</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#using-quark-for-quantization">Using Quark for quantization</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-the-quantized-model-with-vllm">Evaluating the quantized model with vLLM</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#gptq">GPTQ</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-autogptq">Installing AutoGPTQ</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#using-gptq-with-autogptq">Using GPTQ with AutoGPTQ</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#using-gptq-with-hugging-face-transformers">Using GPTQ with Hugging Face Transformers</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#exllama-v2-support">ExLlama-v2 support</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#bitsandbytes">bitsandbytes</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-bitsandbytes">Installing bitsandbytes</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#using-bitsandbytes-primitives">Using bitsandbytes primitives</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#using-bitsandbytes-with-hugging-face-transformers">Using bitsandbytes with Hugging Face Transformers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference-optimization/model-acceleration-libraries">Model acceleration libraries</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#flash-attention-2">Flash Attention 2</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-flash-attention-2">Installing Flash Attention 2</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#xformers">xFormers</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-ck-xformers">Installing CK xFormers</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-built-in-acceleration">PyTorch built-in acceleration</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-compilation">PyTorch compilation</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-tunableop">PyTorch TunableOp</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#fbgemm-and-fbgemm-gpu">FBGEMM and FBGEMM_GPU</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-fbgemm-gpu">Installing FBGEMM_GPU</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#set-up-the-miniconda-environment">Set up the Miniconda environment</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#install-the-rocm-components">Install the ROCm components</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#install-pytorch">Install PyTorch</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#perform-the-prebuild-and-build">Perform the prebuild and build</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#post-build-validation">Post-build validation</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-fbgemm">Testing FBGEMM</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference-optimization/optimizing-with-composable-kernel">Optimize with Composable Kernel</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#high-level-overview-a-ck-gemm-instance">High-level overview: a CK GEMM instance</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#template-parameter-definition">Template parameter definition</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-data-precision">Matrix data precision</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-data-layout">Matrix data layout</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-element-operation">Matrix element operation</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#tunable-parameters">Tunable parameters</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#instantiating-and-running-the-templated-kernel">Instantiating and running the templated kernel</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#developing-fused-int8-kernels-for-smoothquant-models">Developing fused INT8 kernels for SmoothQuant models</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#operation-flow-analysis">Operation flow analysis</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#developing-the-complete-function">Developing the complete function</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#binding-to-python">Binding to Python</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#int8-model-inference-and-performance">INT8 model inference and performance</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference-optimization/optimizing-triton-kernel">Optimize Triton kernels</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference-optimization/profiling-and-debugging">Profile and debug</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-ai/inference-optimization/workload">Workload optimization</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#workload-tuning-strategy">Workload tuning strategy</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#measure-the-current-workload">Measure the current workload</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#mi300x-profiling-start">Identify tuning requirements</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#high-level-profiling-tools">High-level profiling tools</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-level-profiling-tools">Kernel-level profiling tools</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#analyze-and-tune">Analyze and tune</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#optimize-model-inference-with-vllm">Optimize model inference with vLLM</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-tunable-configurations">Auto-tunable configurations</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-tuning">Manual tuning</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#iterate-and-validate">Iterate and validate</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#profiling-tools">Profiling tools</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-profiler">PyTorch Profiler</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-profiling-tools">ROCm profiling tools</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#rocprofiler">ROCProfiler</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-compute-profiler">ROCm Compute Profiler</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-systems-profiler">ROCm Systems Profiler</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#vllm-performance-optimization">vLLM performance optimization</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-environment-variables">Performance environment variables</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-tuning-using-pytorch-tunableop">Auto-tuning using PyTorch TunableOp</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-tuning-based-on-vllm-engine-configurations">Performance tuning based on vLLM engine configurations</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-performance-by-throughput-measurement">Evaluating performance by throughput measurement</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#maximizing-vllm-instances-on-a-single-node">Maximizing vLLM instances on a single node</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#configure-the-gpu-memory-utilization-parameter">Configure the gpu_memory_utilization parameter</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#run-vllm-on-multiple-gpus">Run vLLM on multiple GPUs</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#choose-an-attention-backend">Choose an attention backend</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#vllm-engine-arguments">vLLM engine arguments</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#configure-the-max-num-seqs-parameter">Configure the max-num-seqs parameter</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#use-the-float16-dtype">Use the float16 dtype</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-step-scheduling">Multi-step scheduling</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-executor-backend">Distributed executor backend</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-mode-max-seq-len-to-capture">Graph mode max-seq-len-to-capture</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#whether-to-enable-chunked-prefill">Whether to enable chunked prefill</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-support">Quantization support</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#fp8-quantization">FP8 quantization</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#awq-quantization">AWQ quantization</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#fp8-kv-cached-dtype">fp8 kv-cached-dtype</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-tunableop">PyTorch TunableOp</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#workflow">Workflow</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#offline-tuning">Offline tuning</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-inductor-max-autotune-tuning-knobs">PyTorch inductor max-autotune tuning knobs</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#triton-backend">Triton backend</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#composable-kernel-backend">Composable Kernel backend</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-library-tuning">ROCm library tuning</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#gemm-general-matrix-multiplication">GEMM (general matrix multiplication)</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#hipblaslt-benchmarking">hipBLASLt benchmarking</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#hipblaslt-auto-tuning-using-hipblaslt-bench">hipBLASLt auto-tuning using hipblaslt-bench</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#hipblaslt-backend-assembly-generator-tuning">hipBLASLt backend assembly generator tuning</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#tensilelite-tuning-flow">TensileLite tuning flow</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-initial-solution-parameters">Step 1: Initial solution parameters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-benchmark-common-parameters">Step 2: Benchmark common parameters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-fork-parameters">Step 3: Fork parameters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-benchmark-fork-parameters">Step 4: Benchmark fork parameters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-join-parameters">Step 5: Join parameters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-benchmark-join-parameters">Step 6: Benchmark join parameters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#step-7-benchmark-final-parameters">Step 7: Benchmark final parameters</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#update-logic-yaml-files">Update logic YAML files</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#tensile-optimization-and-performance-tuning-tips">Tensile optimization and performance tuning tips</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-composable-kernel-gemm-kernels">Optimizing Composable Kernel GEMM kernels</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#miopen">MIOpen</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#convolution">Convolution</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#tuning-in-miopen">Tuning in MIOpen</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-fastest-kernel">Finding the fastest kernel</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#rccl">RCCL</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#use-all-eight-gpus">Use all eight GPUs</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#disable-numa-auto-balancing">Disable NUMA auto-balancing</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#disable-acs-for-multi-node-rccl">Disable ACS for multi-node RCCL</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#run-rccl-unittests">Run RCCL-Unittests</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#npkit-profiler">NPKit profiler</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#rccl-tests">RCCL-tests</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#use-one-process-per-gpu-mode">Use one-process-per-GPU mode</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#rccl-in-e2e-workloads">RCCL in E2E workloads</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#triton-kernel-performance-optimization">Triton kernel performance optimization</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-tunable-kernel-configurations">Auto-tunable kernel configurations</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-gpu-resource-utilization">Overall GPU resource utilization</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#mlir-analysis">MLIR analysis</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#isa-assembly-analysis">ISA assembly analysis</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#hip-performance-optimization">HIP performance optimization</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#parallel-execution-and-gpu-hardware-utilization">Parallel execution and GPU hardware utilization</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-usage-optimization">Memory usage optimization</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnostic-and-performance-analysis">Diagnostic and performance analysis</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#debug-memory-access-faults">Debug memory access faults</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-the-occupancy-of-a-kernel">Compute the occupancy of a kernel</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#special-considerations">Special considerations</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-gpu-communications">Multi-GPU communications</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-fsdp-and-rccl-settings">Multi-node FSDP and RCCL settings</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference external nav-link" href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/">AI tutorials</a></li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/rocm-for-hpc/index">Use ROCm for HPC</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/system-optimization/index">System optimization</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/gpu-performance/mi300x">AMD Instinct MI300X performance guides</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/system-debugging">System debugging</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-language-and-system-level-debug-flags-and-environment-variables">ROCm language and system-level debug, flags, and environment variables</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocr-error-code">ROCr error code</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#command-to-dump-firmware-version-and-get-linux-kernel-version">Command to dump firmware version and get Linux kernel version</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#debug-flags">Debug flags</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocr-level-environment-variables-for-debug">ROCr level environment variables for debug</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#turn-off-page-retry-on-gfx9-vega-devices">Turn off page retry on GFX9/Vega devices</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#hip-environment-variables-3-x">HIP environment variables 3.x</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#opencl-debug-flags">OpenCL debug flags</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#pcie-debug">PCIe-debug</a></li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/compiler-topics">Use advanced compiler features</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference external nav-link" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/index.html">ROCm compiler infrastructure</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference external nav-link" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/conceptual/using-gpu-sanitizer.html">Use AddressSanitizer</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference external nav-link" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/conceptual/openmp.html">OpenMP support</a></li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/setting-cus">Set the number of CUs</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-how-to/Bar-Memory">Troubleshoot BAR access limitation</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-physical-address-limitation">Handling physical address limitation</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#bar-configuration-for-amd-gpus">BAR configuration for AMD GPUs</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-bar-usage-on-amd-gpus">Example of BAR usage on AMD GPUs</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference external nav-link" href="https://github.com/amd/rocm-examples">ROCm examples</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Conceptual</span></p>
<ul class="nav section-nav flex-column">
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/gpu-arch">GPU architecture overview</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/gpu-arch/mi300">MI300 microarchitecture</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#node-level-architecture">Node-level architecture</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference external nav-link" href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-mi300-cdna3-instruction-set-architecture.pdf">AMD Instinct MI300/CDNA3 ISA</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference external nav-link" href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-3-white-paper.pdf">White paper</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/gpu-arch/mi300-mi200-performance-counters">MI300 and MI200 Performance counter</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#mi300-and-mi200-series-performance-counters">MI300 and MI200 series performance counters</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#command-processor-counters">Command processor counters</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#command-processor-fetcher-counters">Command processor-fetcher counters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#command-processor-compute-counters">Command processor-compute counters</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#graphics-register-bus-manager-counters">Graphics register bus manager counters</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#shader-processor-input-counters">Shader processor input counters</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-unit-counters">Compute unit counters</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#instruction-mix">Instruction mix</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-fused-multiply-add-operation-counters">Matrix fused multiply-add operation counters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#level-counters">Level counters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#wavefront-counters">Wavefront counters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#wavefront-cycle-counters">Wavefront cycle counters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#lds-counters">LDS counters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#miscellaneous-counters">Miscellaneous counters</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#l1-instruction-cache-l1i-and-scalar-l1-data-cache-l1d-counters">L1 instruction cache (L1i) and scalar L1 data cache (L1d) counters</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-l1-cache-subsystem-counters">Vector L1 cache subsystem counters</a><ul class="nav section-nav flex-column">
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#texture-addressing-unit-counters">Texture addressing unit counters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#texture-data-unit-counters">Texture data unit counters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#texture-cache-per-pipe-counters">Texture cache per pipe counters</a></li>
<li class="toctree-l6 nav-item toc-entry"><a class="reference internal nav-link" href="#texture-cache-arbiter-counters">Texture cache arbiter counters</a></li>
</ul>
</li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#l2-cache-access-counters">L2 cache access counters</a></li>
</ul>
</li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#mi300-and-mi200-series-derived-metrics-list">MI300 and MI200 series derived metrics list</a><ul class="nav section-nav flex-column">
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#hardware-counters-by-and-over-all-texture-addressing-unit-instances">Hardware counters by and over all texture addressing unit instances</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#hardware-counters-over-all-texture-cache-per-channel-instances">Hardware counters over all texture cache per channel instances</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#hardware-counters-by-for-or-over-all-texture-cache-per-pipe-instances">Hardware counters by, for, or over all texture cache per pipe instances</a></li>
<li class="toctree-l5 nav-item toc-entry"><a class="reference internal nav-link" href="#hardware-counter-over-all-texture-data-unit-instances">Hardware counter over all texture data unit instances</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/gpu-arch/mi250">MI250 microarchitecture</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#node-level-architecture">Node-level architecture</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference external nav-link" href="https://www.amd.com/system/files/TechDocs/instinct-mi200-cdna2-instruction-set-architecture.pdf">AMD Instinct MI200/CDNA2 ISA</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference external nav-link" href="https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna2-white-paper.pdf">White paper</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/gpu-arch/mi100">MI100 microarchitecture</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#microarchitecture">Microarchitecture</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference external nav-link" href="https://www.amd.com/system/files/TechDocs/instinct-mi100-cdna1-shader-instruction-set-architecture%C2%A0.pdf">AMD Instinct MI100/CDNA1 ISA</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference external nav-link" href="https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna-white-paper.pdf">White paper</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/file-reorg">File structure (Linux FHS)</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#adopting-the-fhs">Adopting the FHS</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#changes-from-earlier-rocm-versions">Changes from earlier ROCm versions</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-fhs-reorganization-backward-compatibility">ROCm FHS reorganization: backward compatibility</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#wrapper-header-files">Wrapper header files</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#executable-files">Executable files</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#library-files">Library files</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#cmake-config-files">CMake config files</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#changes-required-in-applications-using-rocm">Changes required in applications using ROCm</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#changes-in-versioning-specifications">Changes in versioning specifications</a></li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/gpu-isolation">GPU isolation techniques</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-variables">Environment variables</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#rocr-visible-devices"><code class="docutils literal notranslate"><span class="pre">ROCR_VISIBLE_DEVICES</span></code></a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-device-ordinal"><code class="docutils literal notranslate"><span class="pre">GPU_DEVICE_ORDINAL</span></code></a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#hip-visible-devices"><code class="docutils literal notranslate"><span class="pre">HIP_VISIBLE_DEVICES</span></code></a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#cuda-visible-devices"><code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code></a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#omp-default-device"><code class="docutils literal notranslate"><span class="pre">OMP_DEFAULT_DEVICE</span></code></a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#docker">Docker</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-passthrough-to-virtual-machines">GPU passthrough to virtual machines</a></li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/cmake-packages">Using CMake</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-dependencies">Finding dependencies</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-hip-in-cmake">Using HIP in CMake</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-hip-single-source-programming-model">Using the HIP single-source programming model</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#consuming-rocm-c-c-libraries">Consuming ROCm C/C++ libraries</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#consuming-the-hip-api-in-c-code">Consuming the HIP API in C++ code</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#compiling-device-code-in-c-language-mode">Compiling device code in C++ language mode</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-cmake-packages">ROCm CMake packages</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-cmake-presets">Using CMake presets</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-hip-with-presets">Using HIP with presets</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-conceptual/ai-pytorch-inception">Inception v3 with PyTorch</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-training">Deep learning training</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-phases">Training phases</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#case-studies">Case studies</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#inception-v3-with-pytorch">Inception V3 with PyTorch</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-a-pre-trained-model">Evaluating a pre-trained model</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#training-inception-v3">Training Inception V3</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-model-with-cifar-10-on-pytorch">Custom model with CIFAR-10 on PyTorch</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#case-study-tensorflow-with-fashion-mnist">Case study: TensorFlow with Fashion-MNIST</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#case-study-tensorflow-with-text-classification">Case study: TensorFlow with text classification</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav section-nav flex-column">
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-reference/api-libraries">ROCm libraries</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-reference/rocm-tools">ROCm tools, compilers, and runtimes</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-reference/gpu-arch-specs">Accelerator and GPU hardware specifications</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#glossary">Glossary</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-reference/gpu-atomics-operation">Hardware atomics operation support</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#support-summary">Support summary</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#amd-instinct-accelerators">AMD Instinct™ accelerators</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#amd-gfx-generic-targets">AMD gfx generic targets</a></li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#gpus-atomics-support">GPUs atomics support</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#integer-atomics-operations">Integer atomics operations</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">AMD Instinct accelerators</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">AMD gfx generic targets</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#bitwise-atomics-operations">Bitwise atomics operations</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">AMD Instinct accelerators</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">AMD gfx generic targets</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#float-atomics-operations">Float atomics operations</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">AMD Instinct accelerators</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">AMD gfx generic targets</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-reference/precision-support">Precision support</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#integral-types">Integral types</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#floating-point-types">Floating-point types</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#level-of-support-definitions">Level of support definitions</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-type-support-by-hardware-architecture">Data type support by hardware architecture</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#hip-c-type-implementation-support">HIP C++ type implementation support</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-units-support">Compute units support</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-core-support">Matrix core support</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#atomic-operations-support">Atomic operations support</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-type-support-in-rocm-libraries">Data type support in ROCm libraries</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#libraries-input-output-type-support">Libraries input/output type support</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#hipdatatype-enumeration">hipDataType enumeration</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-reference/graph-safe-support">Graph safe support</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Contribute</span></p>
<ul class="nav section-nav flex-column">
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-contribute/contributing">Contributing to the ROCm documentation</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-rocm-repositories">The ROCm repositories</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#editing-and-adding-to-the-documentation">Editing and adding to the documentation</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-contribute/toolchain">ROCm documentation toolchain</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-docs-core">rocm-docs-core</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#sphinx">Sphinx</a><ul class="nav section-nav flex-column">
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#sphinx-external-toc">Sphinx External ToC</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#sphinx-book-theme">Sphinx-book-theme</a></li>
<li class="toctree-l4 nav-item toc-entry"><a class="reference internal nav-link" href="#sphinx-design">Sphinx Design</a></li>
</ul>
</li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#doxygen">Doxygen</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#breathe">Breathe</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#read-the-docs">Read the Docs</a></li>
</ul>
</li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-contribute/building">Building documentation</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#github">GitHub</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#command-line">Command line</a></li>
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#visual-studio-code">Visual Studio Code</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-contribute/feedback">Providing feedback about the ROCm documentation</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#participating-in-discussions-through-github-discussions">Participating in discussions through GitHub Discussions</a></li>
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#submitting-issues-through-github-issues">Submitting issues through GitHub Issues</a></li>
</ul>
</li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="#document-about/license">ROCm licenses</a><ul class="nav section-nav flex-column">
<li class="toctree-l2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-component-licenses">ROCm component licenses</a><ul class="nav section-nav flex-column">
<li class="toctree-l3 nav-item toc-entry"><a class="reference internal nav-link" href="#package-licensing">Package licensing</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav></div>
</div></div>
</div>
<footer class="bd-footer-content">
<p>
</p>
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script defer="" src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer="" src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>
<footer class="rocm-footer">
<div class="container-lg">
<section class="bottom-menu menu py-45">
<div class="row d-flex align-items-center">
<div class="col-12 text-center">
<ul>
<li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
<li><a href="https://rocm.docs.amd.com/en/latest/about/license.html">ROCm Licenses and Disclaimers</a></li>
<li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
<li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
<li><a href="https://www.amd.com/content/dam/amd/en/documents/corporate/cr/supply-chain-transparency.pdf" target="_blank">Supply Chain Transparency</a></li>
<li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
<li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
<li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
<!-- OneTrust Cookies Settings button start -->
<li><a class="ot-sdk-show-settings" href="#cookie-settings" id="ot-sdk-btn">Cookie Settings</a></li>
<!-- OneTrust Cookies Settings button end -->
</ul>
</div>
</div>
<div class="row d-flex align-items-center">
<div class="col-12 text-center">
<div>
<span class="copyright">© 2025 Advanced Micro Devices, Inc</span>
</div>
</div>
</div>
</section>
</div>

<!-- Chatbot widget script -->
<script async
src="https://yqmxexojfmlktmgfsncmmcld.agents.do-ai.run/static/chatbot/widget.js"
data-agent-id="17d75c8d-6be1-11f0-bf8f-4e013e2ddde4"
data-chatbot-id="c2NdnHrih0B7HD9srWUv3dvDLMtD6bz7"
data-name="rocm-doc Chatbot"
data-primary-color="#031B4E"
data-secondary-color="#E5E8ED"
data-button-background-color="#0061EB"
data-starting-message="Hello! How can I help you today?"
data-logo="/static/chatbot/icons/default-agent.svg">
</script>

</footer>
<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
</body>
</html>